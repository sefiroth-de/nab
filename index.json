[{"authors":["admin"],"categories":null,"content":"Norman Markgraf ist freiberuflicher Dozent für Mathematik, Statistik, Data Science und Informatik, sowie freiberuflicher Programmierer. Als Lehrbeauftragter hat er an der Hochschule Bochum, Hochschule Rhein-Waal (am Campus Kleve) und der FOM Hochschule für Oekonomie und Management an den Standorten Aachen, Bonn, Dortmund, Duisburg, Düsseldorf, Gütersloh (Bertelsmann), Köln, Münster, Neuss und Wuppertal verschiedene Lehrveranstaltungen abgehalten.\n","date":1486684800,"expirydate":-62135596800,"kind":"term","lang":"de","lastmod":1486684800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Norman Markgraf ist freiberuflicher Dozent für Mathematik, Statistik, Data Science und Informatik, sowie freiberuflicher Programmierer. Als Lehrbeauftragter hat er an der Hochschule Bochum, Hochschule Rhein-Waal (am Campus Kleve) und der FOM Hochschule für Oekonomie und Management an den Standorten Aachen, Bonn, Dortmund, Duisburg, Düsseldorf, Gütersloh (Bertelsmann), Köln, Münster, Neuss und Wuppertal verschiedene Lehrveranstaltungen abgehalten.","tags":null,"title":"Norman Markgraf","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://sefiroth.net/nab/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/nab/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":[],"categories":["Statistisches"],"content":"  Ab und zu bekomme ich die Frage, wie man einen Fragebogen mit Likert-Scalen-Items auswerten kann.\nDazu kann etwas gezieltes Datenjudo helfen. Wir schauen uns das folgende generierte Mini-Beispiel an:\nlibrary(mosaic) # Basis Paket library(tibble) # Eine modernere Variante der data.frames! set.seed(2009) # Reproduzierbarkeit N \u0026lt;- 25 # Anzahl der Testzeileneinträge in den \u0026quot;testdaten\u0026quot;! # Wir wollen eine Likert-Scale minLikert \u0026lt;- 1 # bis maxLikert \u0026lt;- 6 # erstellen. # Für den Zufallszahlengenerator: maxRnd \u0026lt;- maxLikert + 0.99 # Zum späteren Umrechnen der inversen Items: maxInvItem \u0026lt;- maxLikert + 1 # Wir bauen uns eine Testumfrage mit zwei Itemserien # (AS1-AS6 und BS1-BS6) und N Beobachtungen. # Die Items AS3, AS4 und BS1 und BS5 sind dabei # inverse Items, welche später umgerechnet werden: testdaten \u0026lt;- tibble( ID = 1:N, # AS1-AS6 bilden ein Itemset: AS1 = trunc(runif(N, min = minLikert, max = maxLikert)), AS2 = trunc(runif(N, min = minLikert, max = maxLikert)), AS3 = trunc(runif(N, min = minLikert, max = maxLikert)), AS4 = trunc(runif(N, min = minLikert, max = maxLikert)), AS5 = trunc(runif(N, min = minLikert, max = maxLikert)), AS6 = trunc(runif(N, min = minLikert, max = maxLikert)), # BS1-BS5 bilden ein Itemset: BS1 = trunc(runif(N, min = minLikert, max = maxLikert)), BS2 = trunc(runif(N, min = minLikert, max = maxLikert)), BS3 = trunc(runif(N, min = minLikert, max = maxLikert)), BS4 = trunc(runif(N, min = minLikert, max = maxLikert)), BS5 = trunc(runif(N, min = minLikert, max = maxLikert)), # Geschlecht als sex mit (1 für Frauen und 2 für Männer) sex = trunc(runif(N, min = 1, max = 2.99)) ) # Orinal testdaten einmal ausgeben: head(testdaten) #\u0026gt; # A tibble: 6 x 13 #\u0026gt; ID AS1 AS2 AS3 AS4 AS5 AS6 BS1 BS2 BS3 BS4 BS5 sex #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 1 1 1 5 4 2 1 1 5 5 1 3 1 #\u0026gt; 2 2 4 4 1 5 2 2 1 2 2 3 1 2 #\u0026gt; 3 3 4 1 5 2 4 2 1 1 3 2 4 2 #\u0026gt; 4 4 1 3 3 4 4 2 4 1 5 1 2 1 #\u0026gt; 5 5 3 2 2 2 1 5 1 3 4 5 4 2 #\u0026gt; 6 6 1 1 1 3 3 5 3 4 3 1 2 1 Die Spalten AS3, AS4 und BS1, BS5 waren inverse Items, die wir noch umrechnen müssen:\n# Inverse Item umrechnen: testdaten %\u0026gt;% mutate( AS3 = maxInvItem - AS3, AS4 = maxInvItem - AS4, BS1 = maxInvItem - BS1, BS5 = maxInvItem - BS5 ) -\u0026gt; testdaten_korrigiert # Die Daten mit den umgerechnetern inversen Items: head(testdaten_korrigiert) #\u0026gt; # A tibble: 6 x 13 #\u0026gt; ID AS1 AS2 AS3 AS4 AS5 AS6 BS1 BS2 BS3 BS4 BS5 sex #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 1 1 1 2 3 2 1 6 5 5 1 4 1 #\u0026gt; 2 2 4 4 6 2 2 2 6 2 2 3 6 2 #\u0026gt; 3 3 4 1 2 5 4 2 6 1 3 2 3 2 #\u0026gt; 4 4 1 3 4 3 4 2 3 1 5 1 5 1 #\u0026gt; 5 5 3 2 5 5 1 5 6 3 4 5 3 2 #\u0026gt; 6 6 1 1 6 4 3 5 4 4 3 1 5 1 Die jeweiligen Itemsets werden nun zur einem Wert (Gesamtscore) zusammengefasst, in dem wir jeweils den Mittelwert von AS1-AS6 und BS1-BS5 bildenund in AS bzw. BS speichern:\n# Wir fassen nun die AS1-AS6 und die BS1-BS5 zusammen # und bilden die jeweiligen Mittelwerte: testdaten_korrigiert %\u0026gt;% group_by(ID, sex) %\u0026gt;% # Damit wird für jede Zeile die Zusammenfassung gemacht! summarise( AS = mean(c(AS1, AS2, AS3, AS4, AS5, AS6)), BS = mean(c(BS1, BS2, BS3, BS4, BS5)) ) -\u0026gt; testdaten_sum # Ausgabe der Mittelwerte der AS und BS head(testdaten_sum) #\u0026gt; # A tibble: 6 x 4 #\u0026gt; # Groups: ID [6] #\u0026gt; ID sex AS BS #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 1 1 1.67 4.2 #\u0026gt; 2 2 2 3.33 3.8 #\u0026gt; 3 3 2 3 3 #\u0026gt; 4 4 1 2.83 3 #\u0026gt; 5 5 2 3.5 4.2 #\u0026gt; 6 6 1 3.33 3.4 Die Datentabelle testdaten_sum enthält nun die Spalten AS und BS mit den entsprechenden Mittelwerten der einzelnen Items AS1-AS6 sowieso BS1- BS5.\nWir wollen nun die Ergebnisse als Boxplots anzeigen lassen. Dafür benennen wir die Geschlechter von 1,2 auf “Frau”, “Mann” um:\ntestdaten_sum %\u0026gt;% mutate(sex = factor(sex, levels = c(1, 2), labels = c(\u0026quot;Frau\u0026quot;, \u0026quot;Mann\u0026quot;)) ) -\u0026gt; testdaten_sex  Nun können wir die Boxplots erstellen:\n# Darstellung der Ergebnisse als Boxplot AS ~ sex: gf_boxplot(AS ~ sex, data = testdaten_sex) %\u0026gt;% gf_labs( title = \u0026quot;Boxplot von AS nach Geschlechtern\u0026quot;, x = \u0026quot;Geschlechter\u0026quot;, y = \u0026quot;Item AS\u0026quot; ) %\u0026gt;% gf_refine( scale_y_continuous( breaks = 1:6, label = 1:6, limits = c(2.5, 4.5) # Gibt den Bereich von 2.5 bis 4.5 aus! ) ) # Darstellung der Ergebnisse als Boxplot BS ~ sex: gf_boxplot(BS ~ sex, data = testdaten_sex) %\u0026gt;% gf_labs( title = \u0026quot;Boxplot von BS nach Geschlechtern\u0026quot;, x = \u0026quot;Geschlechter\u0026quot;, y = \u0026quot;Item BS\u0026quot; ) %\u0026gt;% gf_refine( scale_y_continuous( breaks = 1:6, label = 1:6, limits = c(1, 6) # GIbt den ganzen Bereich von 1 bis 6 aus! ) ) Die Kennzahlen dazu erhalten wir mit favstats:\nfavstats(AS ~ sex, data = testdaten_sex)[1:6] #\u0026gt; sex min Q1 median Q3 max #\u0026gt; 1 Frau 1.666667 2.625 3.333333 3.666667 4.000000 #\u0026gt; 2 Mann 3.000000 3.000 3.333333 3.500000 3.833333 favstats(BS ~ sex, data = testdaten_sex)[1:6] #\u0026gt; sex min Q1 median Q3 max #\u0026gt; 1 Frau 2.6 3.2 3.6 4.0 4.2 #\u0026gt; 2 Mann 2.2 2.8 3.0 3.8 4.2 Voilà!\n","date":1624752000,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1624811275,"objectID":"2dcc37bccd90de88686d883b5cee67e0","permalink":"https://sefiroth.net/nab/post/datenjudo-fur-fragebogen/","publishdate":"2021-06-27T00:00:00Z","relpermalink":"/nab/post/datenjudo-fur-fragebogen/","section":"post","summary":"Ab und zu bekomme ich die Frage, wie man einen Fragebogen mit Likert-Scalen-Items auswerten kann.\nDazu kann etwas gezieltes Datenjudo helfen. Wir schauen uns das folgende generierte Mini-Beispiel an:","tags":["R","Datenjudo"],"title":"Datenjudo für Fragebögen ","type":"post"},{"authors":[],"categories":["Statistisches"],"content":"  Wir wollen den Fall unterschen bei dem wir mit zwei statistischen Variabeln (\\(X\\) und \\(Y\\)) eine dritte Variable (\\(Z\\)) mittels einer multiplen linearen Regression modellieren.\nEs seien die Datenpunkte \\((x_1, y_1, z_1), \\dots, (x_n, y_n, z_n)\\) gegeben und wir wollen eine lineare Funktion \\(g(x,y)\\) finden, so dass\n\\[ z_i = g(x_i,y_i)+ \\epsilon_i =\\beta_0 + \\beta_1 \\cdot x_i + \\beta_2 \\cdot y_i + \\epsilon_i \\]\ngilt und der Abweichungsterm \\(\\epsilon_i\\) möglichst klein ist.\nAuf Grundlage unserer Datenpunkt wollen wir die Koeffizienten so schätzen, dass die Summe der quadratische Abweichungen minimal ist. \\[ QS = QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) = \\sum\\limits_{i=1}^n (z_i - \\hat\\beta_0 - \\hat\\beta_1 \\cdot x_i - \\hat\\beta_2 \\cdot y_i )^2 \\]\nDas führt zu der folgenden, notwendigen Bedingen (für stationäre Punkte):\n\\[ \\nabla QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) = \\begin{pmatrix} 0\\\\ 0\\\\ 0\\end{pmatrix} \\]\nIm einzelnen heißt das:\n\\[ \\begin{aligned} \\frac{\\partial}{\\partial \\hat\\beta_0} QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) \u0026amp;= -2 \\cdot\\sum\\limits_{i=1}^n \\left(z_i - \\hat\\beta_0 - \\hat\\beta_1 \\cdot x_i - \\hat\\beta_2 \\cdot y_i \\right) \\\\ \u0026amp;= -2 \\cdot n \\cdot \\left(\\bar{z} - \\hat\\beta_0 - \\hat\\beta_1 \\cdot\\bar{x} - \\hat\\beta_2 \\cdot\\bar{y} \\right) \\\\ \\\\ \\frac{\\partial}{\\partial \\hat\\beta_1} QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) \u0026amp;= -2 \\cdot \\sum\\limits_{i=1}^n \\left( z_i\\cdot x_i - \\hat\\beta_0 \\cdot x_i - \\hat\\beta_1 \\cdot x_i\\cdot x_i - \\hat\\beta_2 \\cdot y_i\\cdot x_i \\right) \\\\ \u0026amp;= -2 \\cdot \\left(\\sum\\limits_{i=1}^n z_i\\cdot x_i - \\hat\\beta_0 \\cdot n \\cdot \\bar{x}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i^2 - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i\\cdot x_i \\right) \\\\ \\\\ \\frac{\\partial}{\\partial \\hat\\beta_2} QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) \u0026amp;= -2 \\cdot \\sum\\limits_{i=1}^n \\left( z_i\\cdot y_i - \\hat\\beta_0 \\cdot y_i - \\hat\\beta_1 \\cdot x_i\\cdot y_i - \\hat\\beta_2 \\cdot y_i\\cdot y_i \\right) \\\\ \u0026amp;= -2 \\cdot \\left(\\sum\\limits_{i=1}^n z_i\\cdot y_i - \\hat\\beta_0 \\cdot n \\cdot \\bar{y}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i\\cdot y_i - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i^2 \\right) \\\\ \\end{aligned} \\]\nWir setzen die 1. Gleichung gleich Null und stellen nach \\(\\hat\\beta_0\\) um:\n\\[ \\hat\\beta_0 = \\bar{z} - \\hat\\beta_1 \\cdot \\bar{x} - \\hat\\beta_2 \\cdot \\bar{y} \\] Nun ersetzen wir \\(\\hat\\beta_0\\) in den verbleibenden Gleichungen durch \\(z_i - \\hat\\beta_1 \\cdot x_i - \\hat\\beta_2 \\cdot y_i\\) und nutzen den Verschiebesatz: \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\hat\\beta_1} QS \u0026amp;= -2 \\cdot \\left(\\sum\\limits_{i=1}^n z_i\\cdot x_i - (\\bar{z} - \\hat\\beta_1 \\cdot \\bar{x} - \\hat\\beta_2 \\cdot \\bar{y}) \\cdot n \\cdot \\bar{x}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i^2 - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i\\cdot x_i \\right) \\\\ \u0026amp;= -2 \\cdot \\left(\\sum\\limits_{i=1}^n (z_i-\\bar{z})(x_i - \\bar{x}) - \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n (x_i -\\bar{x})^2 - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x}) \\right) \\\\ \\\\ \\frac{\\partial}{\\partial \\hat\\beta_2} QS \u0026amp;= -2 \\cdot \\left(\\sum\\limits_{i=1}^n z_i\\cdot y_i - (\\bar{z} - \\hat\\beta_1 \\cdot \\bar{x} - \\hat\\beta_2 \\cdot \\bar{y}) \\cdot n \\cdot \\bar{y}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i\\cdot y_i - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i^2 \\right) \\\\ \u0026amp;= -2 \\cdot \\left(\\sum\\limits_{i=1}^n (z_i-\\bar{z})(y_i - \\bar{y}) - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n (y_i -\\bar{y})^2 - \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x}) \\right) \\\\ \\end{aligned} \\]\nWir setzen die beiden Gleichungen nun gleich Null und formen nach \\(\\hat\\beta_1\\) und \\(\\hat\\beta_2\\) um:\n\\[ \\begin{aligned} \\hat\\beta_1 \u0026amp;= \\frac{\\sum\\limits_{i=1}^n (z_i-\\bar{z})(x_i - \\bar{x}) - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x})}{\\sum\\limits_{i=1}^n (x_i -\\bar{x})^2} \\\\ \\\\ \\hat\\beta_2 \u0026amp;= \\frac{\\sum\\limits_{i=1}^n (z_i-\\bar{z})(y_i - \\bar{y}) - \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x})}{\\sum\\limits_{i=1}^n (y_i -\\bar{y})^2} \\end{aligned} \\]\nDurch Erweiterung von Zähler nun Nenner mit \\(\\frac{1}{n-1}\\) erhalten wir:\n\\[ \\begin{aligned} \\hat\\beta_1 \u0026amp;= \\frac{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (z_i-\\bar{z})(x_i - \\bar{x}) - \\hat\\beta_2 \\cdot \\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x})}{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (x_i -\\bar{x})^2} \\\\ \u0026amp;= \\frac{s_{x,z}-\\hat\\beta_2\\cdot s_{x,y}}{s^2_{x}} = \\frac{s_{x,z}}{s^2_x}-\\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}} \\\\ \\\\ \\hat\\beta_2 \u0026amp;= \\frac{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (z_i-\\bar{z})(y_i - \\bar{y}) - \\hat\\beta_1 \\cdot \\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i - \\bar{x})}{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (y_i -\\bar{y})^2} \\\\ \u0026amp;= \\frac{s_{y,z}-\\hat\\beta_1\\cdot s_{x,y}}{s^2_{y}} = \\frac{s_{y,z}}{s^2_y}-\\hat\\beta_1 \\frac{s_{x,y}}{s^2_{y}} \\\\ \\end{aligned} \\]\nwir setzen nun die erste in die zweite Gleichung ein und erhalten:\n\\[ \\begin{aligned} \\hat\\beta_2 \u0026amp;= \\frac{s_{y,z}}{s^2_y} - \\left(\\frac{s_{x,z}}{s^2_x}-\\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}}\\right) \\frac{s_{x,y}}{s^2_{y}} \\\\ \u0026amp;= \\frac{s_{y,z}}{s^2_y} - \\frac{s_{x,z}}{s^2_x}\\frac{s_{x,y}}{s^2_{y}} + \\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}}\\frac{s_{x,y}}{s^2_{y}} \\\\ \u0026amp;= \\frac{\\frac{s_{y,z}}{s^2_y} - \\frac{s_{x,z}}{s^2_x}\\frac{s_{x,y}}{s^2_{y}}}{1-\\frac{s_{x,y}}{s^2_{x}}\\frac{s_{x,y}}{s^2_{y}}} \\\\ \u0026amp;= \\frac{\\frac{s_{y,z}\\cdot s^2_x - s_{x,z} s_{x,y}}{s^2_x\\cdot s^2_y}}{\\frac{s^2_x s^2_y-(s_{x,y})^2}{s^2_x \\cdot s^2_y}} = \\frac{s_{y,z}\\cdot s^2_x - s_{x,z} s_{x,y}}{s^2_x s^2_y-(s_{x,y})^2} \\end{aligned} \\]\nUnd damit weiter:\n\\[ \\begin{aligned} \\hat\\beta_1 \u0026amp;= \\frac{s_{x,z}}{s^2_x}-\\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}} \\\\ \u0026amp;= \\frac{s_{x,z}}{s^2_x} - \\frac{s_{y,z}\\cdot s^2_x - s_{x,z} s_{x,y}}{s^2_x s^2_y-(s_{x,y})^2} \\frac{s_{x,y}}{s^2_{x}} \\\\ \u0026amp;= \\frac{s_{x,z} (s^2_x s^2_y - (s_{x,y})^2) - s_{y,z}s_{x,y}s^2_x + s_{x,z}s_{x,y}s_{x,y}}{s^2_x (s^2_x s^2_y - (s_{x,y})^2)} \\\\ \u0026amp;= \\frac{s_{x,z}s^2_x s^2_y - s_{x,z}(s_{x,y})^2 - s_{y,z}s_{x,y}s^2_x + s_{x,z}(s_{x,y})^2}{s^2_x s^2_x s^2_y- s^2_x(s_{x,y})^2} \\\\ \\end{aligned} \\]\nlibrary(mosaic) mtcars %\u0026gt;% select(mpg, hp, wt) -\u0026gt; dt # Von R berechnete Koeffizienten: coef(lm(mpg ~ hp + wt, data = dt)) #\u0026gt; (Intercept) hp wt #\u0026gt; 37.22727012 -0.03177295 -3.87783074 mean_x = mean( ~ hp, data = dt) mean_y = mean( ~ wt, data = dt) mean_z = mean( ~ mpg, data = dt) s_xy \u0026lt;- cov(hp ~ wt, data = dt) s_xz \u0026lt;- cov(hp ~ mpg, data = dt) s_yz \u0026lt;- cov(wt ~ mpg, data = dt) var_x \u0026lt;- var(~ hp, data = dt) var_y \u0026lt;- var(~ wt, data = dt) b1 \u0026lt;- (s_xz*var_x*var_y - s_xz*(s_xy)**2 - s_yz*s_xy*var_x + s_xz*s_xy**2) / (var_x*var_x*var_y - var_x*s_xy**2) b2 \u0026lt;- (s_yz*var_x - s_xz*s_xy) / (var_x * var_y- s_xy*s_xy) b0 \u0026lt;- mean_z - b1 * mean_x - b2 * mean_y # Koeffizienten zur Ausgabe aufbereiten: my_coef \u0026lt;- c(b0, b1, b2) names(my_coef) \u0026lt;- c(\u0026quot;(Intercept)\u0026quot;, \u0026quot;hp\u0026quot;, \u0026quot;wt\u0026quot;) # Von Hand berechnete Koeffizienten: my_coef #\u0026gt; (Intercept) hp wt #\u0026gt; 37.22727012 -0.03177295 -3.87783074 Was passiert, wenn wir alle Datenpunkte studentisieren? Wir rechnen um in:\n\\[ x_i^{\\text{stud}} = \\frac{x_i-\\bar{x}}{s_x}; \\quad y_i^{\\text{stud}} = \\frac{y_i-\\bar{y}}{s_y}; \\quad z_i^{\\text{stud}} = \\frac{z_i-\\bar{z}}{s_z} \\] Damit ist \\[ \\bar{x_i}^\\text{stud} = 0; \\quad \\bar{y_i}^\\text{stud} = 0;\\quad \\bar{z_i}^\\text{stud} = 0 \\] und\n\\[ s_{{x_i}^\\text{stud}} = 1; \\quad s_{{y_i}^\\text{stud}} = 1;\\quad s_{{z_i}^\\text{stud}} = 1 \\] Zur Vereinfachung lassen wir die Kennzeichnung “stud” weg. Damit ist dann:\n\\[ \\begin{aligned} \\hat\\beta_0 \u0026amp;= 0\\\\ \\\\ \\hat\\beta_1 \u0026amp;= \\frac{s_{x,z} \\cdot s^2_x \\cdot s^2_y - s_{x,z} \\cdot (s_{x,y})^2 - s_{y,z} \\cdot s_{x,y}s^2_x + s_{x,z} \\cdot (s_{x,y})^2}{s^2_x \\cdot s^2_x s^2_y- s^2_x \\cdot (s_{x,y})^2} \\\\ \u0026amp;= \\frac{s_{x,z} \\cdot 1 \\cdot 1 - s_{x,z} \\cdot (s_{x,y})^2 - s_{y,z} \\cdot s_{x,y} \\cdot 1 + s_{x,z} \\cdot (s_{x,y})^2}{1 \\cdot 1 \\cdot 1 - 1 \\cdot (s_{x,y})^2}\\\\ \u0026amp;= \\frac{s_{x,z} - s_{x,z} \\cdot (s_{x,y})^2 - s_{y,z} \\cdot s_{x,y} + s_{x,z} \\cdot (s_{x,y})^2}{1 - (s_{x,y})^2} \\\\ \u0026amp;= \\frac{s_{x,z} - s_{y,z} \\cdot s_{x,y} }{1 - (s_{x,y})^2} \\\\ \\\\ \\hat\\beta_2 \u0026amp;= \\frac{s_{y,z} \\cdot s^2_x - s_{x,z} \\cdot s_{x,y}}{s^2_x \\cdot s^2_y - (s_{x,y})^2} \\\\ \u0026amp;= \\frac{s_{y,z} \\cdot 1 - s_{x,z} \\cdot s_{x,y}}{1 \\cdot 1 - (s_{x,y})^2} \\\\ \u0026amp;= \\frac{s_{y,z} - s_{x,z} \\cdot s_{x,y}}{1 - (s_{x,y})^2} \\\\ \\end{aligned} \\]\nWir schauen uns ein paar Fälle genauer an:\nFall: \\(X\\) und \\(Y\\) sind unabhängig. Dann ist \\(s_{x,y}=0\\) und wir erhalten \\(\\hat\\beta_1=s_{x,z}\\in[-1;1]\\) und \\(\\hat\\beta_2=s_{y,z}\\in[-1;1]\\).\n Fall: \\(X\\) und \\(Y\\) sind abhängig. Dann ist \\(|s_{x,y}|=1\\) und es gibt keine Lösung für \\(\\hat\\beta_1\\) und \\(\\hat\\beta_2\\).\n Fall: \\(0 \u0026lt; |s_{x,y}| \u0026lt; 1\\). …\n   ","date":1624492800,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1624514951,"objectID":"e16556a36136101f3c8fc2b5afa3e84e","permalink":"https://sefiroth.net/nab/post/dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/","publishdate":"2021-06-24T00:00:00Z","relpermalink":"/nab/post/dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/","section":"post","summary":"Wir wollen den Fall unterschen bei dem wir mit zwei statistischen Variabeln (\\(X\\) und \\(Y\\)) eine dritte Variable (\\(Z\\)) mittels einer multiplen linearen Regression modellieren.\nEs seien die Datenpunkte \\((x_1, y_1, z_1), \\dots, (x_n, y_n, z_n)\\) gegeben und wir wollen eine lineare Funktion \\(g(x,y)\\) finden, so dass","tags":["Data Science","Lineare Regression","Statistik"],"title":"Dinge die man in zwei Dimensionen machen kann - Multiple lineare Regression","type":"post"},{"authors":[],"categories":"Statistisches","content":"  Einleitung Bei einer multiplen linearen Regression kann man den Einfluss einer unabhägigen Variable auf das Verhalten einer anderen unabhägigen Variable in Bezug auf die abhägige Variable mit modellieren.\nWir wollen das einmal an dem Beispiel der folgenden Datentabelle Impact of Beauty on Instructor’s Teaching Ratings und der Fragestellung in wie weit das Alter und das Geschlecht einen Einfluss auf das Evaluationsergebnis haben.\nDazu stellen laden wir die Daten aus dem Internet:\nlibrary(mosaic) url \u0026lt;- paste0(\u0026quot;https://vincentarelbundock.github.io/Rdatasets/csv/AER/\u0026quot;, \u0026quot;TeachingRatings.csv\u0026quot;) teacherratings \u0026lt;- read.csv(url) und betrachten das Streudiagramm:\ngf_point(eval ~ age, color = ~gender, data = teacherratings)  Ein lineares Modell Ein klassisches lineares Modell sieht wie folgt aus:\nerglm \u0026lt;- lm(eval ~ age + gender + age:gender, data = teacherratings) coef(erglm) #\u0026gt; (Intercept) age gendermale age:gendermale #\u0026gt; 4.49018892 -0.01306572 -0.32104348 0.01109285 Doch was bedeuten diese Werte konkret:\n (Intercept) = 4.4901889: Gibt das (theoretische) Evaluationsergebnis für einer Frau im Alter von 0 Jahren an.\n age = -0.0130657: Gibt an, um wie viele Punkte im Schnitt sich eine Frau pro Lebensjahr mehr verändert. (Da der Wert negativ ist, also verschlechtert.)\n gendermale = -0.3210435: Gibt an, um wie viel sich das Startwert bei 0 Jahren verändert, wenn es ein Mann gewesen wäre. Wir kommen damit auf einen Startwert bei 0 Jahren für Männer von 4.1691454\n age:gendermale = 0.0110928: Gibt an um wie viel sich die Steigung ändert, wenn statt einer Frau ein Mann betrachtet wird. Statt einer Änderung um -0.0130657 bei Frauen beträgt sie bei Männern \\(-0.0130657-0.0110928 = -0.0019729\\).\n  coef_female = c(coef(erglm)[1], coef(erglm)[2]) coef_male = c( coef(erglm)[1] + coef(erglm)[3], coef(erglm)[2] + coef(erglm)[4] ) gf_point(eval ~ age, color = ~gender, data = teacherratings) %\u0026gt;% gf_coefline(coef = coef_female, color = ~\u0026quot;female\u0026quot;) %\u0026gt;% gf_coefline(coef = coef_male, color = ~\u0026quot;male\u0026quot;)  Wir können so die folgenden Modellgleichungen aufstellen:\nFür Frauen: \\[ \\begin{aligned} \\widehat{eval}_{\\text{female}} \u0026amp; = 4.4901889 - 0.0130657 \\cdot age \\\\ \u0026amp;\\approx 4.49 - 0.013 \\cdot age \\end{aligned} \\]\n Für Männer: \\[ \\begin{aligned} \\widehat{eval}_{\\text{male}} \u0026amp;= 4.1691454 - 0.0019729 \\cdot age\\\\ \u0026amp;\\approx 4.169 - 0.002 \\cdot age \\end{aligned} \\]\n   Besserer Blick durch gute Transformation der Daten Spannender wäre es aber, wenn die y-Achenabschnitte nicht so weit ausserhalb unseres Betrachungsbereichs (29; 73) liegen würde.\nWir zentrieren daher einmal unsere Altersangaben mit der Transformation:\n\\[age_i^\\text{center} = age_i - \\overline{age}\\]\nIn R:\n# Mittelwert bestimmen und speichern: mean_age = mean( ~ age, data = teacherratings) # Transformation durchführen: teacherratings %\u0026gt;% mutate( age_center = age - mean_age ) -\u0026gt; teacherratings # Das Ergebnis kurz zusammenfassen: df_stats(~ age + age_center, min, mean, sd, max, data = teacherratings) #\u0026gt; response min mean sd max #\u0026gt; 1 age 29.00000 4.836501e+01 9.802742 73.00000 #\u0026gt; 2 age_center -19.36501 3.514033e-15 9.802742 24.63499 Das der Mittelwert bei den zentrierten Daten nicht exakt Null ist liegt an den numerischen Besonderheiten des Rechners. Kurz: Computer können gar nicht richitg rechnen und haben daher hier einen kleinen Rundungsfehler!\nBetrachten wir die gerundeten Werte, so ergibt sich das folgende, etwas übersichtlichere Bild:\n# Wir bauen uns gerundete Funktionen: round_digits \u0026lt;- 3 # Anzahl der Nachkommastellen mean_r \u0026lt;- function(x) round(mean(x), round_digits) sd_r \u0026lt;- function(x) round(sd(x), round_digits) min_r \u0026lt;- function(x) round(min(x), round_digits) max_r \u0026lt;- function(x) round(max(x), round_digits) # Wir benutzen nun die gerundeten Werte: df_stats(~ age + age_center, min_r, mean_r, sd_r, max_r, data = teacherratings) #\u0026gt; response min_r mean_r sd_r max_r #\u0026gt; 1 age 29.000 48.365 9.803 73.000 #\u0026gt; 2 age_center -19.365 0.000 9.803 24.635 Im Mittel sind unsere Lehrer:innen also \\(48.365\\) alt, die Jüngsten mit 29 etwa \\(19.365\\) jünger und die Ältesten mit 73 etwa \\(24.635\\) älter als der Altersdurchschnitt.\nEin Blick auf die Koeffizenten des linearen Modells bzgl. der zentrierten Daten:\nerglm_c \u0026lt;- lm(eval ~ age_center + gender + age_center:gender, data = teacherratings) coef(erglm_c) #\u0026gt; (Intercept) age_center gendermale #\u0026gt; 3.85826543 -0.01306572 0.21546232 #\u0026gt; age_center:gendermale #\u0026gt; 0.01109285 Das dazu passende Streudiagramm mit den Regressionsgeraden:\ncoef_c_female = c(coef(erglm_c)[1], coef(erglm_c)[2]) coef_c_male = c( coef(erglm_c)[1] + coef(erglm_c)[3], coef(erglm_c)[2] + coef(erglm_c)[4] ) gf_point(eval ~ age_center, color = ~gender, data = teacherratings) %\u0026gt;% gf_coefline(coef = coef_c_female, color = ~\u0026quot;female\u0026quot;) %\u0026gt;% gf_coefline(coef = coef_c_male, color = ~\u0026quot;male\u0026quot;)  Was bedeuten nun diese Werte konkret:\n (Intercept) = 3.8582654: Gibt das Evaluationsergebnis für einer Frau mit Durchschnittsalter (48) an.\n age = -0.0130657: Gibt an, um wie viele Punkte im Schnitt sich eine Frau pro Lebensjahr mehr verändert.\n gendermale = -0.3210435: Gibt an, um wie viel sich das Evaluationsergebnis eines Mannes im Durchschnittsalter ändert gegenüber dem einer Frau. Für das Durchschnittalter liegen Männer im Schnitt bei 4.0737278\n age:gendermale = 0.0110928: Gibt an, um wie viel sich die Steigung ändert, wenn statt einer Frau ein Mann betrachtet wird. Statt einer Änderung um -0.0130657 bei Frauen beträgt sie bei Männern \\(-0.0130657-0.0110928 = -0.0019729\\).\n  Wir können daher die folgenden Modellgleichungen aufstellen:\nFür Frauen: \\[ \\begin{aligned} \\widehat{eval}_{\\text{female}} \u0026amp; = 3.8582654 - 0.0130657 \\cdot (age - 48.3650108) \\\\ \u0026amp;\\approx 3.858 - 0.013 \\cdot (age - 48.365) \\end{aligned} \\]\n Für Männer: \\[ \\begin{aligned} \\widehat{eval}_{\\text{male}} \u0026amp;= 4.0737278 - 0.0019729 \\cdot (age - 48.3650108) \\\\ \u0026amp;\\approx 4.074 - 0.002 \\cdot (age - 48.365) \\end{aligned} \\]\n   Zur Interpretation Im durchschnittlichen Alter ist das erwartete Evaluationsergebnis bei Frauen (\\(3.8582654\\)) um rund \\(0.215\\) Punkte schlechter als bei Männern (\\(4.0737278\\)). Mit jedem Lebensjahr sinkt dabei in beiden Fällen, also sowohl bei Frauen als auch bei Männern, das Evaluationsergbnis. Bei den Frauen aber mit ca. \\(-0.013\\) deutlich stärker als mit ca. \\(-0.002\\) bei den Männern .\n Fazit Eine gute Transformation einiger Daten kann, dank der angepassten Modellgleichungen, die Interpretation der Ergebnisse deutlich vereinfachen!\n Nachtrag und Danksagung Die Idee zu diesem Blog-Post verdanke ich dem Blog von Prof. Dr. Sebastian Sauer. Hier der Link zum Orginal-Blog: https://data-se.netlify.app/2021/06/17/beispiel-zur-interpretation-des-interaktionseffekts/\nDanke auch für die kritische Durchsicht und die hilfreichen Anmerkungen.\n Reproduzierbarkeitsinformationen #\u0026gt; R version 4.1.0 (2021-05-18) #\u0026gt; Platform: x86_64-apple-darwin17.0 (64-bit) #\u0026gt; Running under: macOS Catalina 10.15.7 #\u0026gt; #\u0026gt; Locale: de_DE.UTF-8 / de_DE.UTF-8 / de_DE.UTF-8 / C / de_DE.UTF-8 / de_DE.UTF-8 #\u0026gt; #\u0026gt; Package version: #\u0026gt; mosaic_1.8.3 xfun_0.24  ","date":1624406400,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1624436668,"objectID":"eb2caecaefbc83c2e438a399b89addc9","permalink":"https://sefiroth.net/nab/post/interaktionseffekte-leichter-interpretieren-durch-transformationen/","publishdate":"2021-06-23T00:00:00Z","relpermalink":"/nab/post/interaktionseffekte-leichter-interpretieren-durch-transformationen/","section":"post","summary":"Einleitung Bei einer multiplen linearen Regression kann man den Einfluss einer unabhägigen Variable auf das Verhalten einer anderen unabhägigen Variable in Bezug auf die abhägige Variable mit modellieren.","tags":["Lineare Regression","Interaktionseffekte","Data Science","Mediator","Moderator","Mediatorenanalyse","Moderatorenanalyse","R","Statistik"],"title":"Interaktionseffekte leichter interpretieren durch Transformationen","type":"post"},{"authors":[],"categories":"Statistisches","content":"  Bei einer einfachen linearen Regression versuchen wir zu vorgegebenen Datenpunkten \\((x_1, y_1), \\cdots (x_n, y_n)\\) die Parameter einer möglichst passenden Gerade \\(g(x)=\\beta_0 + \\beta_1 \\cdot x\\) zu schätzen.\nDie Schätzung des y-Achsenabschnitts \\(\\hat\\beta_0\\) und der Steigung \\(\\hat\\beta_1\\) erfolgt dabei algebraisch exakt mittels:\n\\[\\hat\\beta_0 = \\bar{y} - \\hat\\beta_1 \\cdot \\bar{x} \\quad\\text{und}\\quad \\hat\\beta_1 = \\frac{s_x}{s_y}\\cdot r_{x,y}\\]\nDabei sind \\(\\bar{x}\\) bzw. \\(\\bar{y}\\) die Mittelwerte und \\(s_x\\) bzw. \\(s_y\\) die Standardabweichungen der Datenpunkte \\(x_i\\) bzw. \\(y_i\\); darüberhinaus ist \\(r_{x,y}\\) der Korrelationskoeffizient der Datenpunkte.\nBeim studentisieren werden die Datenpunkte bzgl. des Mittelwertes zentriert und bzgl der Standardabweichung normiert:\n\\[x_i^{\\text{stud}} = \\frac{x_i-\\bar{x}}{s_x} \\quad\\text{bzw.}\\quad y_i^{\\text{stud}} = \\frac{y_i-\\bar{y}}{s_y}\\]\nWas passiert nun durch eine solche Studentisierung (oft auch z-Transformation genannt) mit den geschätzen Parametern?\nDie Mittelwerte \\(\\bar{x}^{stud}\\) und \\(\\bar{y}^{stud}\\) werden zu Null. Die Standardabweichungen \\(s_{x^{stud}}\\) und \\(s_{y^stud}\\) werden zur Eins:\n\\[\\bar{x}^{stud}=0=\\bar{y}^{stud} \\qquad s_{x^{stud}}= 1 = s_{y^{stud}}\\]\nDer y-Achsenabschnitt wird nun durch\n\\[\\hat\\beta_0^{stud} = \\bar{y}^{stud} - \\hat\\beta_1^{stud} \\cdot \\bar{x}^{stud} = 0 - \\hat\\beta_1^{stud} \\cdot 0 = 0\\]\nund die Steigung durch\n\\[ \\hat\\beta_1^{stud} = \\frac{s_{x^{stud}}}{s_{y^{stud}}}\\cdot r_{x^{stud},y^{stud}} = \\frac{1}{1}\\cdot r_{x^{stud},y^{stud}} = r_{x^{stud},y^{stud}} \\]\ngeschätzt.\nFür den Korrelationskoeffienten gilt nun \\[ r_{x^{stud},y^{stud}} = \\frac{s_{x^{stud},y^{stud}}}{s_{x^{stud}}\\cdot_{y^{stud}}} = \\frac{s_{x^{stud},y^{stud}}}{1 \\cdot 1} = s_{x^{stud},y^{stud}}. \\]\nDamit Schätzen wir unsere Steigung \\(\\hat\\beta_1^{stud}\\) direkt aus der Kovarianz \\(s_{x^{stud},y^{stud}}\\).\nDamit gilt:\n\\[\\hat\\beta_1^{stud} = r_{x^{stud},y^{stud}} = s_{x^{stud},y^{stud}} \\in [-1, 1]\\]\nIn Worten zusammengefasst: Im studentisierten Fall ist\n der y-Achsenabschnitt immer 0 und die Steigung immer ein Wert zwischen -1 und 1  Beispiel: mtcars- Daten Auf Grundlage der Datentabelle mtcars wollen wir den linearer Zusammenhang zwischen dem Verbrauch (in Meilen pro Gallone mpg) und der Leistung (Pferdestärke hp) modellieren.1\nlibrary(mosaic) # Wir nehmen die Datentabelle \u0026#39;mtcars\u0026#39;: mtcars %\u0026gt;% select(hp, mpg) -\u0026gt; dt # Ein kurzer Blick aus die Daten: df_stats( ~ hp + mpg, mean, sd, data = dt) #\u0026gt; response mean sd #\u0026gt; 1 hp 146.68750 68.562868 #\u0026gt; 2 mpg 20.09062 6.026948 # Wir vergleichen den Verbrauch (mpg, miles per gallon) # mit den Pferdestärken (hp) mit Hilfe eines Streudiagramms. # Dazu berechnen wir vorab die Mittelwerte mean_hp \u0026lt;- mean(~ hp, data = dt) mean_mpg \u0026lt;- mean(~ mpg, data = dt) # und berechnen nun die Schätzwerte für die Regressionsgerade beta_1 \u0026lt;- cov(mpg ~ hp, data = dt) / var(~ hp, data = dt) beta_0 \u0026lt;- mean_mpg - beta_1 * mean_hp # schliesslich zeichnen alles in das Streudiagramm ein: gf_point(mpg ~ hp, data = dt) %\u0026gt;% gf_hline(yintercept = ~ mean_mpg, color = \u0026quot;grey60\u0026quot;, linetype = \u0026quot;dashed\u0026quot;) %\u0026gt;% gf_vline(xintercept = ~ mean_hp, color = \u0026quot;grey60\u0026quot;, linetype = \u0026quot;dashed\u0026quot;) %\u0026gt;% gf_point(mean_mpg ~ mean_hp, color = \u0026quot;red\u0026quot;, size = 5, alpha = 0.2) %\u0026gt;% gf_abline(slope = ~ beta_1, intercept = ~beta_0, color = \u0026quot;dodgerblue\u0026quot;) %\u0026gt;% gf_lims(y = c(5,35)) Die Funktionsvorschrift für die (blaue) Regressionsgerade lautet:\n\\[\\begin{aligned} \\hat{y} \u0026amp;= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\ \u0026amp;\\approx 30.0988605 -0.0682283 \\cdot x \\\\ \u0026amp;\\approx 30.099 -0.068 \\cdot x \\end{aligned}\\]\nStudentisieren wir nun die mpg und hp Werte. In R können wir das mit der Funktion ‘zscore()’2 wie folgt machen:\ndt %\u0026gt;% mutate( hp_stud = zscore(hp), mpg_stud = zscore(mpg) ) -\u0026gt; dt # Ein kurzer Blick aus die Daten: df_stats( ~ hp_stud + mpg_stud, mean, sd, data = dt) #\u0026gt; response mean sd #\u0026gt; 1 hp_stud 1.040834e-17 1 #\u0026gt; 2 mpg_stud 7.112366e-17 1 Der Grund für die kleinen Abweichungen von der Null bei den Mittelwerten sind unumgängliche Rundungsfehler, die der Computer macht!\n# Wir \u0026quot;berechnen\u0026quot; die Mittelwerte: mean_hp_stud \u0026lt;- 0 # = mean(~ hp_stud, data = dt) mean_mpg_stud \u0026lt;- 0 # = mean(~ mpg_stud, data = dt) # Berechnen wir nun die Schätzwerte für die Regressionsgerade: beta_1_stud \u0026lt;- cov(mpg_stud ~ hp_stud, data = dt) beta_0_stud \u0026lt;- 0 # = mean_mpg_stud - beta_1_stud * mean_hp_stud # und zeichnen diese in unser Streudiagramm ein: gf_point(mpg_stud ~ hp_stud, data = dt) %\u0026gt;% gf_hline(yintercept = ~ mean_mpg_stud, color = \u0026quot;grey60\u0026quot;, linetype = \u0026quot;dashed\u0026quot;) %\u0026gt;% gf_vline(xintercept = ~ mean_hp_stud, color = \u0026quot;grey60\u0026quot;, linetype = \u0026quot;dashed\u0026quot;) %\u0026gt;% gf_point(mean_mpg_stud ~ mean_hp_stud, color = \u0026quot;red\u0026quot;, size = 5, alpha = 0.2) %\u0026gt;% gf_abline(slope = ~ beta_1_stud, intercept = ~beta_0_stud, color = \u0026quot;dodgerblue\u0026quot;) %\u0026gt;% gf_lims(y = c(-2,2)) Die Regressionsgerade im studentisierten Problem lautet nun:\n\\[ \\begin{aligned} \\hat{y}^{stud} \u0026amp;= \\hat\\beta^{stud}_0 + \\hat\\beta_1^{stud} \\cdot x^{stud} \\\\ \u0026amp;\\approx 0 - 0.7761684 \\cdot x^{stud} \\\\ \u0026amp;\\approx 0 -0.776 \\cdot x^{stud} \\end{aligned} \\]\n Direkt mit ‘R’ Wir erhalten unsere Ergebnisse natürlich auch direkt in R, ohne selber die Werte auszurechnen:\n# Ursprüngliches Modell: erglm \u0026lt;- lm(mpg ~ hp, data = dt) coef(erglm) #\u0026gt; (Intercept) hp #\u0026gt; 30.09886054 -0.06822828 # Studentisiertes Modell: erglm_stud \u0026lt;- lm(mpg_stud ~ hp_stud, data = dt) coef(erglm_stud) #\u0026gt; (Intercept) hp_stud #\u0026gt; -3.149357e-17 -7.761684e-01  Zurückrechnen der studentisierten Werte in das ursprüngliche Problem Aus dem Ergebnis des studentisierten Modells können wir die Koeffizenten des ursprünglichen Modells wie folgt berechnen:\n\\[\\hat\\beta_1 = \\hat\\beta_1^{stud} \\cdot \\frac{s_y}{s_x}\\] und\n\\[\\hat\\beta_0 = \\bar{y} - \\hat\\beta_1 \\cdot \\bar{x}\\]\nIn R geht das wie folgt:\nmean_mpg \u0026lt;- mean( ~ mpg, data = dt) sd_mpg \u0026lt;- sd( ~ mpg, data = dt) mean_hp \u0026lt;- mean( ~ hp, data = dt) sd_hp \u0026lt;- sd( ~ hp, data = dt) (beta_1 \u0026lt;- beta_1_stud * sd_mpg / sd_hp) #\u0026gt; [1] -0.06822828 (beta_0 \u0026lt;- mean_mpg - beta_1 * mean_hp) #\u0026gt; [1] 30.09886  ### Fazit ... ## Reproduzierbarkeitsinformationen  #\u0026gt; R version 4.1.0 (2021-05-18) #\u0026gt; Platform: x86_64-apple-darwin17.0 (64-bit) #\u0026gt; Running under: macOS Catalina 10.15.7 #\u0026gt; #\u0026gt; Locale: de_DE.UTF-8 / de_DE.UTF-8 / de_DE.UTF-8 / C / de_DE.UTF-8 / de_DE.UTF-8 #\u0026gt; #\u0026gt; Package version: #\u0026gt; mosaic_1.8.3 tidyr_1.1.3 xfun_0.24 ```\n  Das “Cookbook” zur Datentabelle können Sie mit Hilfe von help(\"mtcars\") aufrufen!↩︎\n Sie können hier auch die Funktion scale() verwenden!↩︎\n   ","date":1624406400,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1624427567,"objectID":"b9327c70be78d0706db581aed8836439","permalink":"https://sefiroth.net/nab/post/regression-mit-studentisierten-daten/","publishdate":"2021-06-23T00:00:00Z","relpermalink":"/nab/post/regression-mit-studentisierten-daten/","section":"post","summary":"Bei einer einfachen linearen Regression versuchen wir zu vorgegebenen Datenpunkten \\((x_1, y_1), \\cdots (x_n, y_n)\\) die Parameter einer möglichst passenden Gerade \\(g(x)=\\beta_0 + \\beta_1 \\cdot x\\) zu schätzen.","tags":["Allgemein","Data Science","Lineare Regression","Korrelationskoeffizient","R"],"title":"Regression mit studentisierten Daten","type":"post"},{"authors":[],"categories":"Statistisches","content":"  Bei einer einfachen Regression versuchen wir zu gegebenen Datenpunkten \\((x_1, y_1), ..., (x_n, y_n)\\) eine möglichst passende Funktion \\(g(x)\\) zu finden, so dass \\[y_i = g(x_i) + e_i\\] gilt. Dabei tolerieren wir eine (kleine) Abweichung \\(e_i\\).\nBei einer einfachen linearen Regression gehen wir davon aus, dass die Datenpunkte (im wesentlichen) auf einer Geraden liegen. Mit \\(g(x)=\\beta_0 + \\beta1 \\cdot x\\) ergibt sich dann für die Datenpunkte die Gleichung:\n\\[y_i = \\beta_0 + \\beta_1 \\cdot x_i + e_i\\]\nUnsere Aufgabe besteht nun darin die Parameter \\(\\beta_0\\) (y-Achsenabschnitt) und \\(\\beta_1\\) (Steigung) an Hand der \\(n\\) Datenpunkte zu schätzen. Alle unsere Schätzungen kennzeichnen wir mit einem Dach (\\(\\hat{.}\\)), um sie von den (in der Regel unbekannten) Parametern besser zu unterscheiden.\nWir suchen somit nach \\(\\hat\\beta= \\left(\\hat\\beta_0,\\, \\hat\\beta_1\\right)\\), so dass die Gerade \\(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x\\) zu gegebenem \\(x_i\\) eine möglichst gute Schätzung von \\(y_i\\) (genannt \\(\\hat{y}_i\\)) hat:\n\\[ \\hat{y_i} = \\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i \\]\nDie Abweichung \\(\\hat{e_i}\\) unserer Schätzung \\(\\hat{y}_i\\) von dem gegebenen Wert \\(y_i\\) lässt sich schreiben als:\n\\[ \\hat{e_i} = \\hat{y_i} - y_i = \\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\]\nWenn wir diese Abweichung über alle \\(i\\) minimieren, finden wir unser \\(\\hat\\beta\\).\nDoch das wirft eine Frage auf: Wie genau messen wir die möglichst kleinste Abweichung der \\(\\hat{e_i}\\) konkret?\nWir betrachten zunächst drei einfache Ideen:\nIdee: Betrag der Summe der Abweichungen\n Idee: Summe der absoluten Abweichungen\n Idee: Summe der quadratischen Abweichungen\n  Gewöhnlich nutzen wir die quadratischen Abweichungen, weshalb wir die drei Ideen ebenso in umgekehrter Reihenfolge betrachten wollen:\n3. Idee: Summe der quadratischen Abweichungen Wir bezeichnen mit\n\\[\\begin{aligned} QS \u0026amp;= QS(\\hat\\beta) = QS(\\hat\\beta_0, \\hat\\beta_1) \\\\ \u0026amp;= \\sum\\limits_{i=1}^n \\hat{e_i}^2 = \\sum\\limits_{i=1}^n \\left(\\hat{y_i} - y_i \\right)^2 \\\\ \u0026amp;= \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right)^2 \\end{aligned}\\]\ndie Quadrat-Summe der Abweichungen.\nGesucht wird \\(\\hat\\beta=\\left(\\hat\\beta_0,\\,\\hat\\beta_1\\right)\\), so das \\(QS\\) minimiert wird.\nDies ist ein Minimierungsproblem, bei dem wir zu mindestens eine (exakte) mathematisch-algebraisch Lösung in Form eines stationären Punktes finden können. Dazu berechnen wir die Nullstelle der ersten partiellen Ableitung von \\(QS\\) nach \\(\\hat\\beta_0\\) bzw. \\(\\hat\\beta_1\\).\nVorbemerkungen Wegen \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=1}^n x_i\\) ist \\(n \\cdot \\bar{x} =\\sum\\limits_{i=1}^n x_i\\) und analog \\(n \\cdot \\bar{y} =\\sum\\limits_{i=1}^n y_i\\)\n Schätzen des y-Achenabschnitts \\(\\hat\\beta_0\\) Es ist:\n\\[\\begin{aligned} \\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS \u0026amp;= 2 \\cdot \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right) \\cdot 1 \\\\ \u0026amp;= 2 \\cdot \\left(\\sum\\limits_{i=1}^n \\hat\\beta_0 + \\sum\\limits_{i=1}^n\\hat\\beta_1 \\cdot x_i - \\sum\\limits_{i=1}^n y_i\\right) \\\\ \u0026amp;= 2 \\cdot \\left( n \\cdot \\hat\\beta_0 + \\hat\\beta_1\\cdot\\sum\\limits_{i=1}^n x_i - \\sum\\limits_{i=1}^n y_i \\right) \\\\ \u0026amp;= 2 \\cdot \\left( n \\cdot \\hat\\beta_0 + \\hat\\beta_1\\cdot n \\cdot \\bar{x} - n \\cdot\\bar{y} \\right) \\\\ \u0026amp;= 2 \\cdot n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y} \\right) \\end{aligned}\\]\nUm stationäre Punkte zu ermitteln, müssen wir den Ausdruck nun gleich Null setzen und erhalten:\n\\[\\begin{aligned} 0 \u0026amp;= \\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS \\\\ \u0026amp;= 2 \\cdot n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y} \\right) \\qquad | : (2 \\cdot n) \\\\ \u0026amp;= \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y} \\end{aligned}\\]\nStellen wir nach \\(\\hat\\beta_0\\) um, erhalten wir:\n\\[\\begin{aligned} \\hat\\beta_0 \u0026amp;= - \\hat\\beta_1\\cdot\\bar{x} + \\bar{y} \\\\ \\hat\\beta_0 \u0026amp;= \\bar{y} - \\hat\\beta_1\\cdot\\bar{x} \\end{aligned}\\]\nUm \\(\\hat\\beta_0\\) zu bestimmen, benötigen wir \\(\\hat\\beta_1\\).\n Schätzen der Steigung \\(\\hat\\beta_1\\) Es ist:\n\\[\\begin{aligned} \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS \u0026amp;= 2 \\cdot \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right) \\cdot x_i \\\\ \u0026amp;= 2 \\cdot \\left(\\sum\\limits_{i=1}^n \\hat\\beta_0 \\cdot x_i + \\sum\\limits_{i=1}^n \\hat\\beta_1 \\cdot x_i\\cdot x_i- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\ \u0026amp;= 2 \\cdot \\left(\\hat\\beta_0 \\cdot \\sum\\limits_{i=1}^n x_i + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\ \u0026amp;= 2 \\cdot \\left(\\hat\\beta_0 \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\end{aligned}\\]\nWir ersetzen nun \\(\\hat\\beta_0\\) durch \\(\\bar{y} - \\hat\\beta_1\\cdot \\bar{x}\\) und erhalten:\n\\[\\begin{aligned} \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS \u0026amp;= 2 \\cdot \\left(\\hat\\beta_0 \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\ \u0026amp;= 2 \\cdot \\left(\\left(\\bar{y} - \\hat\\beta_1\\cdot \\bar{x}\\right) \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\ \u0026amp;= 2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - n \\cdot \\hat\\beta_1 \\cdot \\bar{x}^2 + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\ \u0026amp;= 2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - \\sum\\limits_{i=1}^n y_i \\cdot x_i + \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n x_i^2- n \\cdot \\bar{x}^2\\right)\\right) \\\\ \\end{aligned}\\]\nMit Hilfe des Verschiebesatzes von Steiner (zweimal angewendet) erhalten wir:\n\\[\\begin{aligned} \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS \u0026amp;=2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - \\sum\\limits_{i=1}^n y_i \\cdot x_i + \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n x_i^2- n \\cdot \\bar{x}^2\\right)\\right) \\\\ \u0026amp;=2 \\cdot \\left(- \\left(\\sum\\limits_{i=1}^n y_i \\cdot x_i - n \\cdot \\bar{y} \\cdot \\bar{x} \\right)+ \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n x_i^2- n \\cdot \\bar{x}^2\\right)\\right) \\\\ \u0026amp;=2 \\cdot \\left(\\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n x_i^2- n \\cdot \\bar{x}^2\\right)- \\left(\\sum\\limits_{i=1}^n y_i \\cdot x_i - n \\cdot \\bar{y} \\cdot \\bar{x} \\right)\\right) \\\\ \u0026amp;= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right) \\end{aligned}\\]\nWir setzen nun wieder den Ausdruck gleich Null:\n\\[\\begin{aligned} 0 \u0026amp;= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right) \\qquad | : 2\\\\ \u0026amp;= \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y}) \\end{aligned}\\]\nUnd stellen dann nach \\(\\hat\\beta_1\\) um:\n\\[\\begin{aligned} \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 \u0026amp;= \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y}) \\\\ \\hat\\beta_1 \u0026amp;= \\frac{\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\sum\\limits_{i=1}^n(x_i-\\bar{x})^2} \\end{aligned}\\]\nWir können nun Zähler und Nenner der rechten Seite mit \\(\\frac{1}{n}\\) erweitern und erhalten so:\n\\[\\begin{aligned} \\hat\\beta_1 \u0026amp;= \\frac{\\frac{1}{n} \\cdot\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\frac{1}{n} \\cdot\\sum\\limits_{i=1}^n (x_i-\\bar{x})^2} \\\\ \u0026amp;= \\frac{\\sigma_{x,y}}{\\sigma^2_x} \\\\ \\end{aligned}\\]\nOder aber wir erweitern mit \\(\\frac{1}{n-1}\\) und erhalten:\n\\[\\begin{aligned} \\hat\\beta_1 \u0026amp;= \\frac{\\frac{1}{n-1} \\cdot\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\frac{1}{n-1} \\cdot\\sum\\limits_{i=1}^n (x_i-\\bar{x})^2} \\\\ \u0026amp;= \\frac{s_{x,y}}{s^2_{x}} \\end{aligned}\\]\nDamit können wir zur Berechnung sowohl die Kovarianz der Grundgesamtheit \\(\\sigma_{x,y}\\) und die Varianz \\(\\sigma^2_x\\) von \\(x\\), als auch deren Schätzer \\(s_{x,y}\\) und \\(s^2_x\\) verwendet werden!\nDiese Methode nennt sich Methode der kleinsten Quadrate (engl. ordenary least square method) und wir sprechen dann auch von den Kleinste-Quadrate-Schätzern (oder kurz KQ-Schätzer bzw. OLS-Schätzer) \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\).\nErweitern wir den Ausdruck mit Standardabweichung \\(\\sigma_y\\) bzw. \\(s_y\\), so erhalten wir:\n\\[\\begin{aligned} \\hat\\beta_1 \u0026amp;= \\frac{\\sigma_{x,y}}{\\sigma^2_x} \\cdot \\frac{\\sigma_y}{\\sigma_y} = \\frac{\\sigma_{x,y}}{\\sigma_x \\cdot \\sigma_x} \\cdot \\frac{\\sigma_y}{\\sigma_y} = \\frac{\\sigma_{x,y}}{\\sigma_x \\cdot \\sigma_y} \\cdot \\frac{\\sigma_y}{\\sigma_x} \\\\ \u0026amp;= \\rho_{x,y} \\cdot \\frac{\\sigma_y}{\\sigma_x} \\\\ \\end{aligned}\\]\nund analog für die Schätzer:\n\\[\\begin{aligned} \\hat\\beta_1 \u0026amp;= \\frac{s_{x,y}}{s^2_x} \\cdot \\frac{s_y}{s_y} = \\frac{s_{x,y}}{s_x \\cdot s_x} \\cdot \\frac{s_y}{s_y} = \\frac{s_{x,y}}{s_x \\cdot s_y} \\cdot \\frac{s_y}{s_x} \\\\ \u0026amp;= r_{x,y} \\cdot \\frac{s_y}{s_x} \\\\ \\end{aligned}\\]\nDie Steigung \\(\\hat\\beta_1\\) hat somit eine direkte Beziehung mit dem Korrelationskoeffizenten \\(\\rho\\) (der Grundgesamtheit) bzw. \\(r\\) (der Stichprobe).\nFür eine Berechnung in R heißt dies: wir können die Regressionskoeffizienten \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\) direkt algebraisch ausrechnen, wenn wir\ndie Standardabweichungen von \\(x\\) und \\(y\\) und den Korrelationskoeffizienten oder\n die Varianz von \\(x\\) und Kovarianz von \\(x\\) und \\(y\\)\n  haben.\n Ein Beispiel in R: Auf Grundlage der Datentabelle mtcars wollen wir Prüfen wie ein linearer Zusammenhang zwischen dem Verbrauch (in Meilen pro Gallone mpg) und der Leistung (Pferdestärke hp) modelliert werden kann.1\nlibrary(mosaic) # Wir nehmen die Datentabelle \u0026#39;mtcars\u0026#39;: mtcars %\u0026gt;% select(hp, mpg) -\u0026gt; dt # Ein kurzer Blick auf die Daten: favstats(~ hp, data=dt)[c(\u0026quot;mean\u0026quot;,\u0026quot;sd\u0026quot;)] #\u0026gt; mean sd #\u0026gt; 146.6875 68.56287 favstats(~ mpg, data=dt)[c(\u0026quot;mean\u0026quot;,\u0026quot;sd\u0026quot;)] #\u0026gt; mean sd #\u0026gt; 20.09062 6.026948 # Wir vergleichen den Verbrauch (mpg, miles per gallon) # mit den Pferdestärken (hp) mit Hilfe eines Streudiagramms: gf_point(mpg ~ hp, data = dt) %\u0026gt;% gf_lims(y = c(5,35)) Berechnen wir zunächst die Mittelwerte von \\(x\\) (also ‘hp’) und \\(y\\) (also ‘mpg’)\n(mean_hp \u0026lt;- mean(~ hp, data = dt)) #\u0026gt; [1] 146.6875 (mean_mpg \u0026lt;- mean(~ mpg, data = dt)) #\u0026gt; [1] 20.09062 und zeichnen die Punkt \\((\\bar{x}, \\bar{y}) = (146.69, 20.09)\\) in unser Streudiagramm ein:\ngf_point(mpg ~ hp, data = dt) %\u0026gt;% gf_hline(yintercept = ~ mean_mpg, color = \u0026quot;grey60\u0026quot;, linetype = \u0026quot;dashed\u0026quot;) %\u0026gt;% gf_vline(xintercept = ~ mean_hp, color = \u0026quot;grey60\u0026quot;, linetype = \u0026quot;dashed\u0026quot;) %\u0026gt;% gf_point(mean_mpg ~ mean_hp, color = \u0026quot;red\u0026quot;, size = 5, alpha = 0.2) %\u0026gt;% gf_lims(y = c(5,35)) Berechnen wir nun die Schätzwerte für die Regressionsgerade\n(beta_1 \u0026lt;- cov(mpg ~ hp, data = dt) / var(~ hp, data = dt)) #\u0026gt; [1] -0.06822828 (beta_0 \u0026lt;- mean_mpg - beta_1 * mean_hp) #\u0026gt; [1] 30.09886 und zeichnen diese in unser Streudiagramm ein:\ngf_point(mpg ~ hp, data = dt) %\u0026gt;% gf_hline(yintercept = ~ mean_mpg, color = \u0026quot;grey60\u0026quot;, linetype = \u0026quot;dashed\u0026quot;) %\u0026gt;% gf_vline(xintercept = ~ mean_hp, color = \u0026quot;grey60\u0026quot;, linetype = \u0026quot;dashed\u0026quot;) %\u0026gt;% gf_point(mean_mpg ~ mean_hp, color = \u0026quot;red\u0026quot;, size = 5, alpha = 0.2) %\u0026gt;% gf_abline(slope = ~ beta_1, intercept = ~beta_0, color = \u0026quot;dodgerblue\u0026quot;) %\u0026gt;% gf_lims(y = c(5,35)) Die Funktionsvorschrift für die (blaue) Regressionsgerade lautet:\n\\[\\begin{aligned} \\hat{y} \u0026amp;= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\ \u0026amp;\\approx 30.0988605 -0.0682283 \\cdot x \\\\ \u0026amp;\\approx 30.099 -0.068 \\cdot x \\end{aligned}\\]\n Studentisieren – einmal hin und einmal zurück Was passiert eigentlich, wenn wir unsere \\(x\\) und \\(y\\) Werte studentisieren (aka standardisieren oder z-transformieren)?\nZur Erinnerung, studentisieren geht so: \\[x^{stud} = \\frac{x - \\bar{x}}{s_x}\\]\nIn R können wir das mit der Funktion ‘zscore’ wie folgt machen:\ndt %\u0026gt;% mutate( hp_stud = zscore(hp), mpg_stud = zscore(mpg) ) -\u0026gt; dt Natürlich sind die Mittelwerte nun Null und die Standardabweichungen Eins:\nfavstats(~ hp_stud, data=dt)[c(\u0026quot;mean\u0026quot;,\u0026quot;sd\u0026quot;)] #\u0026gt; mean sd #\u0026gt; 1.040834e-17 1 favstats(~ mpg_stud, data=dt)[c(\u0026quot;mean\u0026quot;,\u0026quot;sd\u0026quot;)] #\u0026gt; mean sd #\u0026gt; 7.112366e-17 1 Der Grund für die kleinen Abweichungen von der Null bei den Mittelwerten sind unumgängliche Rundungsfehler, die der Computer macht!\nSchauen wir uns nun das Streudiagramm an, zusammen mit dem Mittelpunkt \\((0,0)\\)\ngf_point(mpg_stud ~ hp_stud, data = dt) %\u0026gt;% gf_point(0 ~ 0, color = \u0026quot;red\u0026quot;, size = 5, alpha = 0.2) %\u0026gt;% gf_lims(y = c(-2, 2)) Auch wenn die Skalierungen sich geändert haben, die Diagramme sind sehr ähnlich.\nBestimmen wir die Koeffizienten der Regressionsgerade\n(beta_stud_1 \u0026lt;- cov(mpg_stud ~ hp_stud, data = dt)) #\u0026gt; [1] -0.7761684 (beta_stud_0 \u0026lt;- 0 - beta_stud_1 * 0) #\u0026gt; [1] 0 und setzen sie in das Streudiagramm ein:\nWir können das studentisierte Problem auch wieder auf unser ursprüngliches zurück rechnen.\nDie Regressionsgerade im studentisierten Problem lautet:\n\\[\\begin{aligned} \\hat{y}^{stud} \u0026amp;= \\hat\\beta^{stud}_0 + \\hat\\beta_1^{stud} \\cdot x^{stud} \\\\ \u0026amp;\\approx 0 -0.7761684 \\cdot x^{stud} \\\\ \u0026amp;\\approx 0 -0.776 \\cdot x^{stud} \\end{aligned}\\]\nRechnen wir nun mittels der Formel \\[\\hat\\beta_1 = \\hat\\beta_1^{stud} \\cdot \\frac{s_y}{s_x}\\]\ndie Steigung um, so erhalten wir:\n(b1 \u0026lt;- beta_stud_1 * sd(dt$mpg) / sd(dt$hp)) #\u0026gt; [1] -0.06822828 Und setzen wir das in unsere Gleichung zur Bestimmung von \\(\\hat\\beta_0\\) ein:\n(b0 \u0026lt;- mean(dt$mpg) - b1 * mean(dt$hp)) #\u0026gt; [1] 30.09886 so erhalten wir die Schätzwerte des ursprünglichen Problem.\n Ein anderer Weg um die Regressionskoeffizenten zu bestimmen… Gehen wir das Problem noch einmal neu an. Wir suchen \\(\\hat\\beta=(\\hat\\beta_0, \\hat\\beta_1)\\) welches \\(QS(\\hat\\beta) = QS(\\hat\\beta_0, \\hat\\beta_1) = \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right)^2\\) minimiert.\nStatt es direkt, wie oben durch Null setzen der partiellen Ableitungen, zu bestimmen, wählen wir nun einen mathematisch-numerischen Ansatz und wollen \\(\\hat\\beta \\in \\mathbf{R}^2\\) als Optimierungsproblem mit Hilfe des Gradientenverfahrens lösen.\nBeim Gradientenverfahren wird versucht, ausgehend von einem Startwert \\(\\hat\\beta^0 \\in \\mathbf{R}^2\\), gemäß der Iterationsvorschrift\n\\[ \\hat\\beta^{k+1} = \\hat\\beta^{k} + \\alpha^k \\cdot d^k \\]\nfür alle \\(k=0,1, ...\\) eine Näherungslösung für \\(\\hat\\beta\\) zu finden. Dabei ist \\(\\alpha^k \u0026gt; 0\\) eine positive Schrittweite und \\(d^k\\in\\mathbf{R}^n\\) eine Abstiegsrichtung, welche wir in jedem Iterationsschritt \\(k\\) so bestimmen, dass die Folge \\(\\hat\\beta^k\\) zu einem stationären Punkt, unserer Näherungslösung, konvergiert.\nIm einfachsten Fall, dem Verfahren des steilsten Abstieges, wird der Abstiegsvektor \\(d^k\\) aus dem Gradienten \\(\\nabla QS\\) wie folgt bestimmt:\n\\[d^k = -\\nabla QS\\left(\\hat\\beta^k\\right)\\]\nWegen \\[ \\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS = 2 \\cdot n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1\\cdot\\bar{x} - \\bar{y} \\right) \\]\nund\n\\[ \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS = 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y}) \\right) \\]\ngilt:\n\\[\\begin{aligned} \\nabla QS(\\hat\\beta) \u0026amp;= \\nabla QS(\\hat\\beta_0, \\hat\\beta_1) \\\\ \u0026amp;= 2 \\cdot \\begin{pmatrix} n \\cdot(\\hat\\beta_0 + \\hat\\beta_1\\cdot\\bar{x} - \\bar{y}) \\\\ \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y}) \\end{pmatrix} \\end{aligned}\\]\nWir wollen hier von Anfang an mit den studentisierten Werten arbeiten, weil diese numerisch viele Vorteile haben. Darum vereinfachen sich die beiden partiellen Ableitungen noch einmal zu:\n\\[ \\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS = 2 \\cdot v \\]\nund\n\\[\\begin{aligned} \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS \u0026amp;= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right) \\\\ \u0026amp;= 2 \\cdot (n-1) \\left(\\hat\\beta_1 \\cdot s^2_{x} - s_{x,y}\\right) \\end{aligned}\\]\nSomit gilt:\n\\[\\begin{aligned} \\nabla QS(\\hat\\beta) \u0026amp;= \\nabla QS(\\hat\\beta_0, \\hat\\beta_1) \\\\ \u0026amp;= 2 \\cdot \\begin{pmatrix} n \\cdot \\hat\\beta_0 \\\\ (n-1) \\left(\\hat\\beta_1 \\cdot s^2_{x} - s_{x,y}\\right) \\end{pmatrix} \\end{aligned}\\]\nUm die Varianz und die Kovarianz nicht jedesmal neu zu berechnen, speichern wir die Ergebnisse vorab. Ebenso, damit der Quellcode kürzer wird, speichern wir in \\(x\\) und \\(y\\) die studentisierten Werte von \\(hp\\) und \\(mpg\\):\n# Vorbereitungen var_x \u0026lt;- var(~ hp_stud, data = dt) cov_xy \u0026lt;- cov(mpg_stud ~ hp_stud, data = dt) n \u0026lt;- length(dt$hp_stud) x \u0026lt;- dt$hp_stud y \u0026lt;- dt$mpg_stud Nun erstellen wir die \\(QS\\) und \\(\\nabla QS\\) Funktionen: Wir definieren diese Funktion wie folgt in R:\nqs \u0026lt;- function(b_0, b_1) { sum((b_1 * x - y)**2) } nabla_qs \u0026lt;- function(b_0, b_1) { c(2 * n * b_0, 2 * (n - 1) * (b_1 * var_x - cov_xy) ) } Die Schrittweite \\(alpha\\) bestimmen wir mit Hilfe der Armijo-Bedingung und der Backtracking Liniensuche: Diese formalisiert das Konzept “genügend” in der geforderten Verringerung des Funktionswertes. Die Bedingung \\(f(x^k + \\alpha d^k) \u0026lt; f(x^k)\\) wird modifiziert zu \\[f(x^k + \\alpha d^k) \\leq f(x^k) + \\sigma \\alpha \\left(\\nabla f(x^k)\\right)^T d^k,\\] mit \\(\\sigma\\in (0,1)\\). Die Armijo-Bedingung umgeht Konvergenzprobleme der einfachen Bedingung, indem sie fordert, dass die Verringerung zumindest proportional zur Schrittweite und zur Richtungsableitung \\(\\left(\\nabla f(x^k)\\right)^T d^k\\) ist, mit Hilfe der Proportionalitätskonstante \\(\\sigma\\). In der Praxis werden oft sehr kleine Werte verwendet, z.B. \\(\\sigma=0.0001\\).\nDie Backtracking-Liniensuche verringert die Schrittweite wiederholt um den Faktor \\(\\rho\\) (rho) , bis die Armijo-Bedingung erfüllt ist. Sie terminiert garantiert nach einer endlichen Anzahl von Schritten. Weshalb wir sie hier einsetzen:\nalpha_k \u0026lt;- function(b_0, b_1, d_k, alpha = 1, sigma = 0.0001, rho = 0.5) { d_0 \u0026lt;- d_k[1] d_1 \u0026lt;- d_k[2] nabla \u0026lt;- nabla_qs(b_0, b_1) n_0 \u0026lt;- nabla[1] n_1 \u0026lt;- nabla[2] lhs \u0026lt;- qs(b_0 + alpha*d_0, b_1 + alpha*d_1) rhs \u0026lt;- qs(b_0, b_1) + sigma*alpha*(n_0*d_0 + n_1*d_1) while (lhs \u0026gt; rhs) { alpha \u0026lt;- rho * alpha lhs \u0026lt;- qs(b_0 + alpha*d_0, b_1 + alpha*d_1) rhs \u0026lt;- qs(b_0, b_1) + sigma*alpha*(n_0*d_0 + n_1*d_1) } return(alpha) } Ein paar Einstellungen vorab:\n# maximale Anzahl an Iterationen max_iter \u0026lt;- 1000 iter \u0026lt;- 0 # Genauigkeit eps \u0026lt;- 10**-6 # Startwerte b_0 \u0026lt;- 0 b_1 \u0026lt;- -1  Für eine vorgegebene Genauigkeit \\(eps=10^{-6}\\), den Startwerten \\(\\hat\\beta_0^0 = 0\\) und \\(\\hat\\beta_1^0 = -1\\) können wir somit das Verfahren starten:\nwhile (TRUE) { iter \u0026lt;- iter + 1 d_k \u0026lt;- -nabla_qs(b_0, b_1) ad_ \u0026lt;- alpha_k(b_0, b_1, d_k) * d_k x0 \u0026lt;- b_0 + ad_[1] x1 \u0026lt;- b_1 + ad_[2] if ((abs(b_0 - x0) \u0026lt; eps) \u0026amp; (abs(b_1 - x1) \u0026lt; eps) | (iter \u0026gt; max_iter)) { break } b_0 \u0026lt;- x0 b_1 \u0026lt;- x1 } Wir haben in \\(203\\) Iterationsschritten das folgende Ergebnis für die Regressionskoeffizienten:\n\\[\\hat\\beta_0^{stud} = 0 \\qquad \\hat\\beta_1^{stud} = -0.7761689\\]\nBetrachten wir die daraus erstellte Regressionsgerade:\nUm die Regressionskoeffizienten für unser ursprüngliches Problem zu erhalten müssen wir wie folgt zurück rechnen:\n(b1 \u0026lt;- b_1 * sd(dt$mpg) / sd(dt$hp)) #\u0026gt; [1] -0.06822832 (b0 \u0026lt;- mean(dt$mpg) - b1 * mean(dt$hp)) #\u0026gt; [1] 30.09887 Die Geradengleichung für das ursprüngliches Problem lautet somit:\n\\[\\begin{aligned} \\hat{y} \u0026amp;= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\ \u0026amp;\\approx 30.0988668 -0.0682283 \\cdot x \\\\ \u0026amp;\\approx 30.099 -0.068 \\cdot x \\end{aligned}\\]\n Die R Funktion optim In R gibt es bessere Optimierungsmethoden, als die hier verwendete. Zum Beispiel können wir die Funktion optim verwenden. Die Funktion optim benötigt die zu optimierende \\(f(x)\\) und ggf. die Gradientenfunktion \\(gf(x)\\) sowie einen Startpunkt \\(x^0\\):\nf \u0026lt;- function(beta) { qs(beta[1], beta[2]) } grf \u0026lt;- function(beta) { nabla_qs(beta[1], beta[2]) } # Der eigentliche Aufruf von optim: ergb \u0026lt;- optim(c(0,-0.5),f ,grf, method = \u0026quot;CG\u0026quot;) # Auslesen der Schätzer aus dem Ergebnis: (optim_beta_0 \u0026lt;- ergb$par[1]) #\u0026gt; [1] 0 (optim_beta_1 \u0026lt;- ergb$par[2]) #\u0026gt; [1] -0.7761683 Wir erhalten somit für das studentisierte Problem die Gerade:\n\\[\\begin{aligned} \\hat{y}^{stud} \u0026amp;= \\hat\\beta_0^{stud} + \\hat\\beta_1^{stud} \\cdot x^{stud} \\\\ \u0026amp;\\approx 0 -0.7761683 \\cdot x^{stud} \\\\ \u0026amp;\\approx 0 -0.776 \\cdot x^{stud} \\end{aligned}\\]\nFür das ursprüngliche Problem rechnen wir mittels\noptim_b1 \u0026lt;- optim_beta_1 * sd(dt$mpg) / sd(dt$hp) optim_b0 \u0026lt;- mean(dt$mpg) - optim_b1 * mean(dt$hp) um und erhalten:\n\\[\\begin{aligned} \\hat{y} \u0026amp;= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\ \u0026amp;\\approx 30.0988601 -0.0682283 \\cdot x \\\\ \u0026amp;\\approx 30.099 -0.068 \\cdot x \\end{aligned}\\]\n  2. Idee: Summe der absoluten Abweichungen Wir ändern nun die Abweichungsmessfunktion von der Quadrat-Summe hin zu den Absolut-Summen:\n\\[AS = AS(\\hat\\beta) = AS(\\hat\\beta_0, \\hat\\beta_1) = \\sum_{i=1}^n |\\hat{y}_i - y_i|\\]\nAuch hier wollen wir mit den studentisierten Daten arbeiten und stellen die Funktion der Absolut-Summen auf:\n# Absolute Abweichungssummen as \u0026lt;- function(b_0, b_1) { return(sum(abs(b_0 + b_1 * x - y))) } Danach konstruieren wir die zu optimierende Funktion \\(f\\):\n# Zu optimierende Funktion f \u0026lt;- function(beta) { as(beta[1], beta[2]) } Diesmal nutzen wir optim ohne eine Gradientenfunktion:\nergb \u0026lt;- optim(c(0,-1), f) # Schätzer auslesen (opti_as_beta_0 \u0026lt;- ergb$par[1]) #\u0026gt; [1] -0.1304518 (opti_as_beta_1 \u0026lt;- ergb$par[2]) #\u0026gt; [1] -0.6844911 Schauen wir uns nun die so erhaltene Gerade im Vergleich mit der ‘normalen’ Regressionsgerade an:\nIn grün und gestrichelt sehen wir die Gerade aus der Idee der quadratischen Abweichungssummen, in blau die aus der Idee der absoluten Abweichungssummen.\nFür unser ursprüngliches Problem rechnen wir um:\n# Umrechnen in die ursprüngliche Fragestellung (as_b1 \u0026lt;- opti_as_beta_1 * sd(dt$mpg) / sd(dt$hp)) #\u0026gt; [1] -0.06016948 (as_b0 \u0026lt;- (mean(dt$mpg) - as_b1 * mean(dt$hp)) + opti_as_beta_0 * sd(dt$mpg)) #\u0026gt; [1] 28.13051 Und die dazu gehörige Darstellung:\nDie Funktionsvorschrift für die (blaue) Regressionsgerade lautet:\n\\[\\begin{aligned} \\hat{y} \u0026amp;= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\ \u0026amp;\\approx 28.1305094 -0.0601695 \\cdot x \\\\ \u0026amp;\\approx 28.131 -0.06 \\cdot x \\end{aligned}\\]\nDiese Methode nennt sich Median-Regression und ein ein Spezialfall der Quantilsregression, die sich u.a. mit dem R-Paket quantreg unmittelbar umsetzen lässt:\nlibrary(quantreg) ergmedianreg \u0026lt;- rq(mpg ~ hp, data = dt) coef(ergmedianreg) #\u0026gt; (Intercept) hp #\u0026gt; 28.13050847 -0.06016949  1. Idee: Betrag der Summe der Abweichungen Wenn wir die Summe der Abweichungen \\(\\sum\\limits_{i=1}^n \\hat{e}_i\\) minimieren wollen, dann ist es sinnvoll den Betrag davon zu minimieren. Wir suchen also die Schätzer \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\), so dass der Ausdruck\n\\[ \\left| \\sum_{i=1}^n \\hat{e}_i \\right| = \\left| \\sum_{i=1}^n (\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i) \\right| \\]\nminimal ist.\nWegen:\n\\[\\begin{aligned} \\sum_{i=1}^n (\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i) \u0026amp;= \\sum_{i=1}^n \\hat\\beta_0 + \\sum_{i=1}^n \\hat\\beta_1 \\cdot x_i - \\sum_{i=1}^n y_i \\\\ \u0026amp;= n \\cdot \\hat\\beta_0 + \\hat\\beta_1 \\cdot \\sum_{i=1}^n x_i - \\sum_{i=1}^n y_i \\\\ \u0026amp;= n \\cdot \\hat\\beta_0 + \\hat\\beta_1 \\cdot n \\cdot \\bar{x} - n \\cdot \\bar{y} \\\\ \u0026amp;= n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1 \\cdot \\bar{x} - \\bar{y} \\right) \\\\ \u0026amp;= n \\cdot \\left( \\hat\\beta_0 - \\bar{y} + \\hat\\beta_1 \\cdot \\bar{x} \\right) \\end{aligned}\\]\nkönnen wir das absolute Minimum bei \\(\\hat\\beta_0 - \\bar{y} =0\\) und \\(\\hat\\beta_1 \\cdot \\bar{x}=0\\) erreichen, was zur Lösung \\(\\hat\\beta_0 =\\bar{y}\\) und \\(\\hat\\beta_1 = 0\\) führt. Dies ist unser Nullmodel in dem die \\(x_i\\) keinen Einfluss auf die \\(y_i\\) haben und wir daher pauschal die \\(y_i\\) mit \\(\\hat{y}_i=\\bar{y}\\), also dem Mittelwert der \\(y_i\\) abschätzen.\n Zusammenfassung Als Vergleich können wir uns die Quadratsumme \\(QS\\) und Absolutsumme \\(AS\\) der drei Modelle einmal ansehen:\n# Quadratische Abweichungssummen qs \u0026lt;- function(b_0, b_1) { sum(((b_0 + b_1 * dt$hp) - dt$mpg )**2) } # Absolute Abweichungssummen as \u0026lt;- function(b_0, b_1) { sum(abs((b_0 + b_1 * dt$hp) - dt$mpg)) } # Quadratsummen: quad_sum \u0026lt;- c(qs(b0, b1), qs(as_b0, as_b1), qs(mean_mpg, 0)) # Absolutsummen: abs_sum \u0026lt;- c(as(b0, b1), as(as_b0, as_b1), as(mean_mpg, 0)) tab \u0026lt;- tibble( sums = c(quad_sum, abs_sum), sum_type = rep(c(\u0026quot;quad\u0026quot;, \u0026quot;abs\u0026quot;), each = 3), methode = rep(c(\u0026quot;Idee 3\u0026quot;, \u0026quot;Idee 2\u0026quot;, \u0026quot;Idee 1\u0026quot;), 2) ) pivot_wider(tab, names_from=sum_type, values_from=sums, names_sort=T) #\u0026gt; # A tibble: 3 x 3 #\u0026gt; methode abs quad #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Idee 3 93.0 448. #\u0026gt; 2 Idee 2 87.3 477. #\u0026gt; 3 Idee 1 151. 1126.  Reproduzierbarkeitsinformationen #\u0026gt; R version 4.1.0 (2021-05-18) #\u0026gt; Platform: x86_64-apple-darwin17.0 (64-bit) #\u0026gt; Running under: macOS Catalina 10.15.7 #\u0026gt; #\u0026gt; Locale: de_DE.UTF-8 / de_DE.UTF-8 / de_DE.UTF-8 / C / de_DE.UTF-8 / de_DE.UTF-8 #\u0026gt; #\u0026gt; Package version: #\u0026gt; mosaic_1.8.3 quantreg_5.86 tidyr_1.1.3 xfun_0.24   Das “Cookbook” zur Datentabelle können Sie mit Hilfe von help(\"mtcars\") aufrufen!↩︎\n   ","date":1623196800,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1623236741,"objectID":"6691179654657ef4b22cf67e2d790d00","permalink":"https://sefiroth.net/nab/post/ueber-die-koeffizienten-einer-linearen-regression/","publishdate":"2021-06-09T00:00:00Z","relpermalink":"/nab/post/ueber-die-koeffizienten-einer-linearen-regression/","section":"post","summary":"Bei einer einfachen Regression versuchen wir zu gegebenen Datenpunkten \\((x_1, y_1), ..., (x_n, y_n)\\) eine möglichst passende Funktion \\(g(x)\\) zu finden, so dass \\[y_i = g(x_i) + e_i\\] gilt.","tags":["Allgemein","Data Science","Lineare Regression","Korrelationskoeffizient","R"],"title":"Über die Koeffizienten einer linearen Regression","type":"post"},{"authors":[],"categories":["Programmieren"],"content":"  Ein kleiner Speed-Test für Python3, pypy und GraalPython Heute habe ich mit Pyenv einen kleinen Speed-Test für CPython3, PyPy und GraalPython gemacht:\nGrundlage war das n-Damen Problem mit dem Quellcode.\nDie Ergebnisse:\n CPython (3.9.4): Calculation took 316.97 seconds\n PyPy (3.7-7.3.3): Calculation took 10.14 seconds\n GraalPython (21.0.0): Calculation took 15.75 seconds\n   Test vom 8.6.2021 Die Ergebnisse für n = 11: (python -m timeit -r 5 -n 1 ‘import nqueens; nqueens.main()’)\n CPython (3.9.4): 1 loops, best of 5: 7.03 sec per loop\n PyPy (3.7-7.3.4):\n GraalPython (21.1.0): 1 loops, best of 5: 7.01 sec per loop\n   ","date":1617840000,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1617895497,"objectID":"5ef0712fa77f51eb7bc613ff82d75def","permalink":"https://sefiroth.net/nab/post/2021-04-08-graalvm-21-0-0-pypy-3-7-7-3-3-und-python3-9-2-im-kurzen-test/","publishdate":"2021-04-08T00:00:00Z","relpermalink":"/nab/post/2021-04-08-graalvm-21-0-0-pypy-3-7-7-3-3-und-python3-9-2-im-kurzen-test/","section":"post","summary":"Ein kleiner Speed-Test für Python3, pypy und GraalPython Heute habe ich mit Pyenv einen kleinen Speed-Test für CPython3, PyPy und GraalPython gemacht:\nGrundlage war das n-Damen Problem mit dem Quellcode.","tags":["Python","GraalVM","GraalPython","PyPy"],"title":"GraalVM 21.0.0, PyPy 3.7-7.3.3 und Python 3.9.4 im kurzen Test","type":"post"},{"authors":[],"categories":"Statistisches","content":"  Vorbereitungen für R Für die graphischen Ausgaben nutzen wir R und das Paket mosaic:\nlibrary(mosaic)  Vorbemerkungen und Notationen Da alle t-Verteilungen symmetrisch sind, betrachten wir im wesendlichen nur den positiven Teil.\nZwei reelle Funktionen \\(f\\), \\(g\\) sind genau dann, im Sinne von de Bruijn1 (§1.4), asymptotisch äquivalent \\(f \\sim g\\), wenn\n\\[ \\lim\\limits_{x \\to \\infty} \\frac{f(x)}{g(x)} = 1 \\]\ngilt.\nIst \\(f \\sim g\\), so können wir auch\n\\[ f(x) = g(x)\\cdot(1+o(1)) \\]\ndafür schreiben. Dabei ist \\(h(x) = o(\\phi(x))\\) für \\(x \\to \\infty\\), falls \\(\\lim\\limits_{x \\to \\infty} \\frac{h(x)}{\\phi(x)} = 0\\) gilt. Aus der asymptotischen Äquivalenz von \\(f\\) und \\(g\\) folgt nun direkt:\n\\[ \\lim\\limits_{x \\to \\infty}\\frac{f(x)}{g(x)}-1 =\\frac{f(x)-g(x)}{g(x)} = 0 \\]\nMit \\(h(x) = \\frac{f(x)-g(x)}{g(x)}\\) ist \\(h(x) = o(1)\\) und daher \\(f(x)-g(x) = g(x)o(1)\\) und schliesslich \\(f(x) = g(x)+g(x)o(1)\\).\nEin wichtiges Korrolar sagt:\nIst \\(f \\sim g\\), so ist auch \\(\\log(f) \\sim \\log(g)\\).\n Die t-Verteilung im Allgemeinen Die Dichtefunktion der t-Verteilung lauten im Allgemeinen:\n\\[ f_n(x) = \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)} {\\sqrt{n\\pi}~\\Gamma\\left(\\frac{n}{2}\\right)}\\left(1+\\frac{x^{2}}{n}\\right)^{-\\frac{n+1}{2}}\\quad \\mathrm{für}\\quad -\\infty \u0026lt; x \u0026lt; +\\infty \\]\nwobei wir mit \\(\\Gamma(x)\\) die Gammafunktion\n\\[ \\Gamma(x)=\\int\\limits_{0}^{+\\infty}t^{x-1}e^{-t}\\operatorname{d}t \\]\nbezeichnen. Für einige \\(x\\) nimmt die Gammafunktion leicht zu berechnende Werte an:\nSo ist für alle \\(n\\in\\mathbf{N_0}\\):\n\\(\\Gamma(n+1) = n!\\) und \\(\\Gamma\\left(n + \\frac{1}{2}\\right) = \\frac{(2n)!}{n!4^n}\\sqrt{\\pi}\\)\nmit der gewöhnlichen Fakultät \\(n! = \\prod_{i=0}^n i\\), wobei per Definition \\(0!=1\\) ist.\n Die t-Verteilung mit einem Freiheitsgrad Für \\(f_1(x)\\) ergibt sich somit:\n\\[ \\begin{align*} f_1(x) \u0026amp;= \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)} {\\sqrt{n\\pi}~\\Gamma\\left(\\frac{n}{2}\\right)}\\left(1+\\frac{x^{2}}{n}\\right)^{-\\frac{n+1}{2}} \\\\ \u0026amp;= \\frac{\\Gamma\\left(\\frac{2}{2}\\right)} {\\sqrt{\\pi}~\\Gamma\\left(\\frac{1}{2}\\right)}\\left(1+x^{2}\\right)^{-\\frac{2}{2}} \\\\ \u0026amp;= \\frac{\\Gamma\\left(1\\right)} {\\sqrt{\\pi}~\\Gamma\\left(\\frac{1}{2}\\right)}\\left(1+x^{2}\\right)^{-1} \\\\ \\end{align*} \\]\nWegen \\(\\Gamma(1) = 0! = 1\\) und \\(\\Gamma\\left(\\frac{1}{2}\\right)=\\sqrt{\\pi}\\) ergibt sich nun:\n\\[ \\begin{align*} f_1(x) \u0026amp;= \\frac{1} {\\sqrt{\\pi} \\cdot \\sqrt{\\pi}} \\cdot \\left(1+x^{2}\\right)^{-1} \\\\ \u0026amp;= \\frac{1}{\\pi} \\cdot \\frac{1}{1+x^{2}} \\end{align*} \\]\nDas ist die Dichtefunktion der standardisierten Cauchy-Verteilung\n\\[ f_{(\\mu,\\lambda)}(x) = \\frac{1}{\\pi} \\cdot \\frac{\\lambda}{\\lambda^2+(x-\\mu)^2} \\]\nmit (\\(\\mu = 0\\) und \\(\\lambda=1\\)), welche – bekanntermaßen – keinen Erwartungwert hat.\nWegen\n\\[ \\begin{align*} \\lim_{x \\to +\\infty} \\frac{f_1(k \\cdot x)}{f_1(x)} \u0026amp;= \\lim_{x \\to +\\infty} \\frac{\\frac{1}{\\pi} \\cdot \\frac{1}{1+(kx)^{2}}}{\\frac{1}{\\pi} \\cdot \\frac{1}{1+x^{2}}} = \\lim_{x \\to +\\infty} \\frac{1+x^2}{1+k^2x^2} \\\\ \u0026amp;=\\lim_{x \\to +\\infty} \\frac{\\frac{1}{x^2}+\\frac{x^2}{x^2}}{\\frac{1}{x^2}+k^2\\frac{x^2}{x^2}} =\\frac{1}{k^2}=k^{-2} \\end{align*} \\]\nfür alle reellen \\(k\u0026gt;0\\) ist \\(f_1(x)\\) eine regulär variierende Funktion mit Variationsindex \\(\\rho = -2\\).\nDie Überlebensfunktion zur t-Verteilung mit einem Freiheitsgrad lautet nun:\n\\[ \\overline{F}_1(x) = \\int_x^\\infty f_1(t) \\operatorname{d}t = \\frac{1}{\\pi} \\cdot \\int_x^\\infty \\frac{1}{1+x^{2}} \\operatorname{d}t = \\frac{\\arctan(x)}{\\pi} \\]\nda wir das optionale \\(+C\\) mit \\(C=0\\) annehmen dürfen.\nEs gilt nun:\n\\[ \\arctan`(x)= \\frac{1}{1+x^2} \\to \\frac{1}{x^2} \\text{ für } x\\to \\infty \\]\nGenauer gilt wegen\n\\[ \\lim\\limits_{x \\to \\infty} \\frac{\\frac{1}{1+x^2}}{\\frac{1}{x^2}} = \\lim\\limits_{x \\to \\infty} \\frac{x^2}{1+x^2} =1, \\] dass \\(\\frac{1}{1+x^2} \\sim \\frac{1}{x^2}\\), also asymptotisch äquivalent sind und somit auch \\(\\log\\left(\\frac{1}{1+x^2}\\right) \\sim \\log\\left(\\frac{1}{x^2}\\right)\\).\nZusammen gefasst gilt somit: \\[ \\log\\left(\\frac{1}{\\pi} \\cdot \\frac{1}{1+x^{2}}\\right) \\to -2\\log(x) - \\log(\\pi) \\text{ für } x \\to \\infty \\]\nSei \\(f_1^*(x) = C \\cdot x^{-\\alpha}\\) mit \\(\\alpha = 2\\) und \\(C=\\frac{1}{\\pi} \\approx0.3183\\).\nSchauen wir uns das einmal als Grafik an:\nlower_bound \u0026lt;- 2 upper_bound \u0026lt;- 100 dfree \u0026lt;- 1 f_star \u0026lt;- function(x) { alpha \u0026lt;- 2 C \u0026lt;- 1/pi C * x**(-alpha) } x \u0026lt;- seq(lower_bound, upper_bound, 0.1) gf_dist(\u0026quot;t\u0026quot;, df = dfree, xlim = c(lower_bound, upper_bound), color = \u0026quot;darkred\u0026quot;) %\u0026gt;% gf_line(f_star(x) ~ x, color = \u0026quot;darkgreen\u0026quot;) Hier eine doppelt-logarithmische Darstellung:\ngf_dist(\u0026quot;t\u0026quot;, df = dfree, xlim = c(lower_bound, upper_bound), color = \u0026quot;darkred\u0026quot;) %\u0026gt;% gf_line( f_star(x) ~ x, color = \u0026quot;darkgreen\u0026quot;) %\u0026gt;% gf_refine( scale_x_log10(), scale_y_log10() ) Wie groß ist nun der (absolute) Fehler zwischen \\(f_1^*\\) und \\(f_1\\)?\nEine Grafik von \\(f_1^*-f_1\\) zeigt:\nx \u0026lt;- seq(1,1000,1) gf_line(x**-2 - 1/(1+x**2) ~ x, color = \u0026quot;darkgreen\u0026quot;) %\u0026gt;% gf_refine( scale_x_log10(), scale_y_log10() ) Genauer gilt:\n\\[ f_1^*(x) - f_1(x) = \\frac{1}{x^2+x^4} \\]\nWir können also für ein hinreichend großes \\(x \u0026gt;\u0026gt; 1\\) statt \\(f_1\\) auch \\(f_1^*\\) verwenden und erhalten somit:\n\\[ \\begin{align*} \\overline{F}_1(x) \u0026amp;\\approx \\int_x^\\infty f_1^*(t) \\operatorname{d}t = \\int_x^\\infty C \\cdot t^{-\\alpha} \\operatorname{d}t \\\\ \u0026amp;= \\frac{1}{\\pi} \\cdot \\int_x^\\infty t^{-2} \\operatorname{d}t = \\frac{1}{\\pi}\\left[\\lim\\limits_{\\epsilon \\to \\infty} \\left(-\\epsilon^{-1}\\right) -\\left(-x^{-1}\\right)\\right]\\\\ \u0026amp;= \\frac{1}{\\pi}\\cdot\\left[0 + \\frac{1}{x}\\right] = \\frac{1}{\\pi \\cdot x} \\end{align*} \\]\nWie hinreichend ist hier hinreichend groß?\nTaleb schreibt an dieser Stelle gerne, dass man jenseits des Karamata-Punktes die Karamata-Konstante anwenden kann. Beides Begriffe, die keine echte Definition haben und ausserhalb der Sphäre von Taleb auch kaum Verwendung finden.\nDie Karamata-Konstante ist \\(\\rho = -\\alpha\\).\nDer Karamata-Punkt bleibt nebulös. Vermutlich könnte man hier so argumentieren:\nWenn die Fehler zwischen \\(f\\) und \\(f^*\\) hinreichend klein ist.\nHierfür könnte man einen absoluten Fehler oder einen relativen Fehler als Maßstab ansehen.\nFür einen relativen Fehler vielleicht \\(\\frac{f^*-f}{x} \u0026lt; k\\)?\nOder man betrachtet hier gleich \\(\\frac{f^*-f}{\\log(x)} \u0026lt; k^*\\)?\n t-Verteilung mit zwei Freiheitsgeraden Für \\(f_2(x)\\) ergibt sich somit:\n\\[ \\begin{align*} f_2(x) \u0026amp;= \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)} {\\sqrt{n\\pi}~\\Gamma\\left(\\frac{n}{2}\\right)}\\left(1+\\frac{x^{2}}{n}\\right)^{-\\frac{n+1}{2}} \\\\ \u0026amp;= \\frac{\\Gamma\\left(\\frac{3}{2}\\right)} {\\sqrt{2\\pi}~\\Gamma\\left(\\frac{2}{2}\\right)}\\left(1+\\frac{x^{2}}{2}\\right)^{-\\frac{3}{2}} \\\\ \u0026amp;= \\frac{\\Gamma\\left(\\frac{3}{2}\\right)} {\\sqrt{2\\pi}~\\Gamma\\left(1\\right)}\\left(1+\\frac{x^{2}}{2}\\right)^{-\\frac{3}{2}} \\\\ \\end{align*} \\]\nWegen \\(\\Gamma(1) = 0! = 1\\) und \\(\\Gamma\\left(\\frac{3}{2}\\right)=\\frac{\\sqrt{\\pi}}{2}\\) ergibt sich nun:\n\\[ \\begin{align*} f_2(x) \u0026amp;= \\frac{1}{2\\sqrt{2}} \\cdot \\left(1+\\frac{x^{2}}{2}\\right)^{-\\frac{3}{2}} \\\\ \u0026amp;= \\frac{1}{\\sqrt[2]{2^3} \\cdot \\sqrt[2]{\\left(1+\\frac{x^{2}}{2}\\right)^3}} \\\\ \u0026amp;= \\frac{1}{(x^2+2)^{\\frac{3}{2}}} \\\\ \u0026amp;= \\frac{1}{\\sqrt{(x^2+2)^3}} \\end{align*} \\]\nWegen\n\\[ \\begin{align*} \\lim_{x \\to +\\infty} \\frac{f_2(k \\cdot x)}{f_2(x)} \u0026amp;= \\lim_{x \\to +\\infty} \\frac{\\frac{1}{\\sqrt{((k\\cdot x)^2+2)^3}}}{\\frac{1}{\\sqrt{(x^2+2)^3}}} = \\lim_{x \\to +\\infty} \\frac{\\sqrt{(x^2+2)^3}}{\\sqrt{((k\\cdot x)^2+2)^3}} \\\\ \u0026amp;= \\lim_{x \\to +\\infty} \\left(\\frac{x^2+2}{k^2x^2+2}\\right)^\\frac{3}{2}=\\lim_{x \\to +\\infty} \\left(\\frac{\\frac{x^2}{x^2}+\\frac{2}{x^2}}{k^2\\frac{x^2}{x^2}+\\frac{2}{x^2}}\\right)^\\frac{3}{2} \\\\ \u0026amp;=\\left(\\frac{1}{k^2}\\right)^\\frac{3}{2}=\\frac{1}{k^3}=k^{-3} \\end{align*} \\]\nfür alle reellen \\(k\u0026gt;0\\) ist \\(f_2(x)\\) eine regulär variierende Funktion mit Variationsindex \\(\\rho = -3\\).\nDie Überlebensfunktion zur t-Verteilung mit einem Freiheitsgrad lautet nun:\n\\[ \\begin{align*} \\overline{F}_2(x) \u0026amp;= \\int_x^\\infty f_2(t) \\operatorname{d}t = \\int_x^\\infty \\frac{1}{\\sqrt{(t^2+2)^3}} \\operatorname{d}t \\\\ \u0026amp;= \\frac{x}{2 \\cdot \\sqrt{x^2+2}} \\end{align*} \\]\nda wir das optionale \\(+C\\) mit \\(C=0\\) annehmen dürfen.\nEs gilt für jedes feste \\(k\u0026gt;0\\):\n\\[ \\begin{align*} \\lim\\limits_{x \\to \\infty} \\frac{\\overline{F}_2(k x)}{\\overline{F}_2(x)} \u0026amp;= \\lim\\limits_{x \\to \\infty}k \\cdot \\sqrt{\\frac{x^2+2}{k^2x^2+2}} \\\\ \u0026amp;= k \\cdot \\lim\\limits_{x \\to \\infty} \\sqrt{\\frac{1}{k^2} \\cdot \\frac{x^2+2}{x^2+\\frac{2}{k^2}}} \\\\ \u0026amp;= \\frac{k}{k} \\cdot \\lim\\limits_{x \\to \\infty} \\sqrt{ \\frac{x^2+2}{x^2+\\frac{2}{k^2}}} = 1\\end{align*} \\]\nWegen\n\\[ \\lim_{x \\to \\infty} \\frac{\\frac{1}{\\sqrt{(x^2+2)^3}}}{\\frac{1}{x^3}} =\\lim\\limits_{x \\to \\infty} \\frac{x^3}{(\\sqrt{x^2+2})^3} = \\lim\\limits_{x \\to \\infty} \\left(\\frac{x}{\\sqrt{x^2+2}}\\right)^3= 1 \\]\nist \\(f_2 \\sim f^*_2\\) und somit auch \\(\\log(f_2) \\sim \\log(f^*_2)\\).\nAus \\(\\log\\left(\\frac{1}{x^3}\\right) = \\log(1)- 3\\cdot\\log(x)\\) können wir daher auf \\(\\alpha = 3\\) und \\(C=1\\) schliesse und schreiben:\n\\[ f_2^*(x) = C \\cdot x^{-\\alpha} = x^{-3} \\]\nSchauen wir uns das einmal als Grafik an:\nlower_bound \u0026lt;- 2 upper_bound \u0026lt;- 100 dfree \u0026lt;- 2 f_star \u0026lt;- function(x) { alpha \u0026lt;- 3 C \u0026lt;- 1 C * x**(-alpha) } x \u0026lt;- seq(lower_bound, upper_bound, 0.1) gf_dist(\u0026quot;t\u0026quot;, df = dfree, xlim = c(lower_bound, upper_bound), color = \u0026quot;darkred\u0026quot;) %\u0026gt;% gf_line(f_star(x) ~ x, color = \u0026quot;darkgreen\u0026quot;) Hier eine doppelt-logarithmische Darstellung:\ngf_dist(\u0026quot;t\u0026quot;, df = dfree, xlim = c(lower_bound, upper_bound), color = \u0026quot;darkred\u0026quot;) %\u0026gt;% gf_line( f_star(x) ~ x, color = \u0026quot;darkgreen\u0026quot;) %\u0026gt;% gf_refine( scale_x_log10(), scale_y_log10() ) Wie groß ist nun der absolute Fehler zwischen \\(f_2^*\\) und \\(f_2\\) genau?\nEine Grafik zeigt von \\(f_2^*-2_1\\) zeigt:\nx \u0026lt;- seq(1,1000,1) gf_line(f_star(x) - dt(x,df=2) ~ x, color = \u0026quot;darkgreen\u0026quot;) %\u0026gt;% gf_refine( scale_x_log10(), scale_y_log10() )  Fussnoten   de Bruijn, N. G. (1981), Asymptotic Methods in Analysis, Dover Publications, ISBN 9780486642215↩︎\n   ","date":1613520000,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1613560715,"objectID":"1cf52859f92cbad9bf06d83269a11dcd","permalink":"https://sefiroth.net/nab/post/2021-02-14-uber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/","publishdate":"2021-02-17T00:00:00Z","relpermalink":"/nab/post/2021-02-14-uber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/","section":"post","summary":"Vorbereitungen für R Für die graphischen Ausgaben nutzen wir R und das Paket mosaic:\nlibrary(mosaic)  Vorbemerkungen und Notationen Da alle t-Verteilungen symmetrisch sind, betrachten wir im wesendlichen nur den positiven Teil.","tags":["Dichtefunktion","t-Verteilung","student-t","Freiheitsgrade"],"title":"Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden","type":"post"},{"authors":[],"categories":["Statistisches"],"content":"  Reelle Funktionen, die ihren Funktionswert kaum ändern, kann man mit Fug und Recht durchaus behäbig nennen, korrekter wäre aber von langsam variierenden Funktionen zu sprechen\nIm Kontext von potenzgesetzlichen Verteilungen kommt der Begriff slowly varying function vor, der Funktionen beschreibt die nur sehr gering auf Änderungen ihres Parameters reagieren.\nDie Definition dieser behäbigen besser langsam variierenden Funktionen stammt von Jovan Karamata:\nEine positive stetige Funktion \\(L\\)1 auf den positiven reelen Zahlen ist langsam variierend (im unendlichen), falls für alle reellen \\(t\u0026gt;0\\)\n\\[\\lim_{x \\to +\\infty} \\frac{L(t\\cdot x)}{L(x)} = 1\\]\ngilt.\nBeispiele:\nJede konstante Funktionen (\\(\\neq 0\\)) ist langsam variierend.  Beweisskizze: Mit \\(L(x) = c\\) ist \\(L(x) = L(t x) = c\\) und damit \\(\\frac{L(t x)}{L(x)}= 1\\).\nJeder Funktion \\(L(x)\\) mit einem Grenzwert \\(b\u0026gt;0\\) ist langsam variierend.  Beweisskizze: Da \\(\\lim_{x \\to +\\infty} L(x) = b = \\lim_{x \\to +\\infty} L(t\\cdot x)\\) ist \\(\\lim_{x \\to +\\infty} \\frac{L(t\\cdot x)}{L(x)} = \\frac{\\lim_{x \\to +\\infty} L(t\\cdot x)}{\\lim_{x \\to +\\infty} L(x)} = \\frac{b}{b} = 1\\)\nFür jedes reellwertige \\(\\beta\\) ist \\(L(x) = log_\\beta(x)\\) langsam variierend.  Beweisskizze: Es gilt: - Für jede reelle Zahl \\(x\u0026gt;0\\) ist \\(\\log_x(x) = 1\\). - Für reelle Zahlen \\(a, b\\) gilt: \\(\\frac{log(a)}{\\log(b)} = \\log_b(a)\\) - Für reelle Zahlen \\(a, b\\) gilt. \\(\\log(a \\cdot b) = log(a) + log(b)\\) - Für jede Konstante \\(k\\) gilt \\(\\lim_{x \\to +\\infty} \\log_x (k) = 0\\) Somit gilt \\(\\frac{\\log_\\beta(k \\cdot x)}{\\log_\\beta(x)} = \\log_x(k\\cdot x) = \\log_x(k) + \\log_x(x) = \\log_x(k) +1 \\to 1\\) wenn \\(x \\to +\\infty\\)\nDie Funktion \\(x^\\beta\\) ist für alle \\(\\beta \\neq 0\\) nicht langsam variierend. Beweisskizze: Für \\(t \\neq 1\\) gilt: \\[\\lim_{x \\to +\\infty} \\frac{(tx)^\\beta}{x^\\beta} = t^\\beta \\neq 1\\] Damit sind die Funktionen zwar (s.u.) regulär variierend, aber nicht langsam variierend.  Eine regulär variierende Funktion \\(L:(0,+\\infty) \\to (0,+\\infty)\\) ist eine Funktion für die der Term\n\\[\\lim_{x \\to +\\infty} \\frac{L(t\\cdot x)}{L(x)} = g(t)\\]\nmit \\(g(t)\\) für alle \\(t\u0026gt;0\\) einen endlichen aber nicht verschwindenen Wert (m.a.W.: \\(g(t) \\neq 0\\)) hat .\nKaramata hat die regulär variierenden Funktionen nun wie folgt charaterisiert:\nCharakterisierungssatz von Karamata Jede regulär variierende Funktion \\(f:(0,+\\infty) \\to (0,+\\infty)\\) ist von der Form\n\\[f(x) = x^\\beta \\cdot L(x),\\]\nwobei \\(\\beta \\in \\mathbf{R}\\) eine reelle Zahl und \\(L(x)\\) eine langsam variiernde Funktion ist.\nEine Konsequenz aus dem Charakterisierungssatz von Karamata ist, das die Funktion \\(g(t)\\) aus der Definition der regulär variierenden Funktionen notwendigerweise die Gestalt\n\\[g(t) = t^\\rho,\\]\nmit einem \\(\\rho \\in \\mathbf{R}\\), haben muss.\nDieser Wert \\(\\rho\\) wird Index der Varition (engl. index of variation) genannt.\nDie Katamata Theorie ist eine Theorie “erster Ordnung” für reguläre Variation. Weiterführend gibt es mit der de Haan Theorie als Theorie “zweiter Ordnung”.\nQuellen:\n Encylopedia of Math\n Bingham, N. H.; Goldie, C. M.; Teugels, J. L. (1987), Regular Variation, Encyclopedia of Mathematics and its Applications, 27, Cambridge: Cambridge University Press, ISBN 0-521-30787-2, MR 0898871, Zbl 0617.26001\n    \\(L\\) oder auch \\(l\\) wird (angeblich) hier für den Begriff lente (serb. für faul) verwendet. Behäbig ist also doch nicht so falsch. ;-)↩︎\n   ","date":1613174400,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1613215952,"objectID":"754a28a305ecbe55dc800160ca8cfb29","permalink":"https://sefiroth.net/nab/post/2021-02-13-behabige-funktionen-aka-slowly-varying-function/","publishdate":"2021-02-13T00:00:00Z","relpermalink":"/nab/post/2021-02-13-behabige-funktionen-aka-slowly-varying-function/","section":"post","summary":"Reelle Funktionen, die ihren Funktionswert kaum ändern, kann man mit Fug und Recht durchaus behäbig nennen, korrekter wäre aber von langsam variierenden Funktionen zu sprechen\nIm Kontext von potenzgesetzlichen Verteilungen kommt der Begriff slowly varying function vor, der Funktionen beschreibt die nur sehr gering auf Änderungen ihres Parameters reagieren.","tags":["Potenzgesetzliche Verteilung","power law distibution","slowly varying functions"],"title":"Behäbige Funktionen aka slowly varying function","type":"post"},{"authors":[],"categories":["Statistisches"],"content":"  Eine Funktion \\(f(x)\\) heißt potenzgesetzlich, falls\n\\[f(x) = C \\cdot x^a\\]\ngilt, für mindestens alle reellen \\(x \u0026gt; x_{min}\\).\nGewöhnlich setzt man \\(\\alpha = -a\\) und schreibt\n\\[f(x) = C \\cdot x^{-\\alpha}.\\]\nDamit ergibt sich für \\(f\u0026#39;(x)\\) die Form:\n\\[ f\u0026#39;(x) = -C \\cdot \\alpha \\cdot x^{-\\alpha -1} = C^* \\cdot x^{-(\\alpha + 1)} \\] mit \\(C^* = -C \\cdot \\alpha\\).\nEine (streng) potenzgesetzliche Verteilungen (engl. (strong) power-law probability distribution) zur ZV \\(X\\) ist eine Verteilung deren Überlebensfunktion \\(\\overline{F}_X(x)=P(X \u0026gt; x)\\) die folgende Gestalt hat: \\[\\overline{F}(x)=P(X \u0026gt; x) = C \\cdot x^{-\\alpha}\\]\nMit der Dichte \\(f_X\\) ergibt sich: \\[\\overline{F}(x)=P(X \u0026gt; x) = C \\cdot x^{-\\alpha} =\\int_x^\\infty f_X(t) \\text{d}t = C^* \\cdot \\int_x^\\infty t^{-(\\alpha+1)} \\text{d}t = C^* \\cdot \\int_x^\\infty t^{-\\alpha} \\text{d}t\\]\nAnstelle der Konstanten \\(C\\) tritt oft eine langsam variierende Funktion (engl. slowly varying funktion). Wir erhalten somit die folgende, allgemeinere Definition:\nEine potenzgesetzliche Verteilungen (engl. power-law probability distribution) (zu einer Zufallsvariable \\(X\\)) ist eine Verteilung deren Überlebensfunktion \\(\\overline{F}(x)=P(X \u0026gt; x)\\) die folgende Gestalt hat:\n\\[\\overline{F}(x)=P(X \u0026gt; x) = L(x) \\cdot x^{-\\alpha}\\]\nDabei ist \\(L(x):(x_{\\min}, +\\infty) \\to (x_{\\min}, +\\infty)\\) eine langsam variierende Funktion, also gilt für alle \\(t\u0026gt;0\\):\n\\[ \\lim_{x \\to +\\infty} \\frac{L(t \\cdot x)}{L(x)} = 1 \\]\nIst nun wieder \\(f_X\\) die Dichte, so erhalten wir: \\[\\overline{F}(x)=P(X \u0026gt; x) = L(x) \\cdot x^{-\\alpha} = \\int_x^\\infty f_X(t) dt\\]\n\\[f\u0026#39;(x) = [L(x)x^{-\\alpha}]\u0026#39;\\] \\[\\int_x^\\infty f_X(t) dt= \\int_x^\\infty [L(t)t^{-\\alpha}]\u0026#39; dt\\]\n\\(\\Delta x_0 = h = x_1 - x_0\\) \\(x_1 = x_0 + \\Delta x_0 = x_0 + h\\) \\(x_1 = c \\cdot x_0\\) \\(x_0 + h = c \\cdot x_0 \u0026lt;=\u0026gt; c = 1 + \\frac{h}{x_0}\\) \\(L(x_1) = L(x_0+h) = L(c \\cdot x_0)\\) \\(L(x_1) - L(x_0) = L(c \\cdot x_0) - L(x_0) = L(x_0 + h) - L(x_0)}\\) $\nFakten:\n Sinnvoll nur, wenn \\(\\alpha \u0026gt; 0\\). Ist \\(\\alpha \u0026lt; 3\\), dann ist die Varianz und die Schiefe (engl. skewness) (mathematisch) nicht definiert. Für \\(k \u0026gt; \\alpha-1\\) ist das k. Moment unendlich.  Logarithmiert man \\(y=f(x)=C \\cdot x^{-\\alpha}\\), so erhält mensch:\n\\[\\log(y) = \\log(C) -\\alpha \\cdot \\log(x)\\]\nIst eine Verteilung potenzgesetzlich, dann kann man \\(\\alpha\\), wie folgt abschätzen:\nSeien \\(x_0, x_1 \u0026gt; x_{min}\\) zwei reelle Zahlen, \\(y_0=f(x_0)\\) bzw. \\(y_1 = f(y_1)\\).\nDann kann mensch wegen\n\\[\\begin{align*} \\log(y_1) - \\log(y_0) \u0026amp;= \\log(C) - \\alpha \\cdot\\log(x_1) - \\log(C) + \\alpha \\cdot \\log(x_0) \\\\ \u0026amp;= \\alpha \\cdot\\left(\\log(x_0)- \\log(x_1) \\right) \\end{align*}\\]\nden Wert für \\(\\alpha\\), so kann man mittels\n\\[\\alpha = \\frac{\\log(y_1) - \\log(y_0)}{\\log(x_0)- \\log(x_1)}\\]\nden Wert für \\(\\alpha\\) bestimmen.\nMit dem so ermittelten \\(\\alpha\\), können wir \\(C\\) wegen\n\\[\\log(C) = \\log(y)+ \\alpha\\log(x)\\]\nmit Hilfe von\n\\[C = y \\cdot x_0^\\alpha = f(x_0) \\cdot x_0^\\alpha\\]\nfür ein \\(x_0 \u0026gt; x_{min}\\) abschätzen.\nBeispiel:\nNehmen wir die t-Verteilung mit \\(n=2\\) Freiheitsgeraden. Die Dichtefunktion bezeichnen wir mit \\(t_{2}(x)\\).\nDann dann können wir \\(\\alpha=3\\) und \\(C=1\\) abschätzen.\nSchauen wir uns das einmal als Grafik an:\nlower_bound \u0026lt;- 2 upper_bound \u0026lt;- 100 x \u0026lt;- seq(lower_bound, upper_bound, 0.1) gf_dist(\u0026quot;t\u0026quot;, df = dfree, xlim = c(lower_bound, upper_bound), color = \u0026quot;darkred\u0026quot;) %\u0026gt;% gf_line(C*x**(-alpha) ~ x, color = \u0026quot;darkgreen\u0026quot;) Hier eine doppelt-logarithmische Darstellung:\ngf_dist(\u0026quot;t\u0026quot;, df = dfree, xlim = c(lower_bound, upper_bound), color = \u0026quot;darkred\u0026quot;) %\u0026gt;% gf_line( C*x**(-alpha) ~ x, color = \u0026quot;darkgreen\u0026quot;) %\u0026gt;% gf_refine( scale_x_log10(), scale_y_log10() ) Noch ein Beispiel:\nNehmen wir die t-Verteilung mit \\(n=1\\) Freiheitsgeraden. Die Dichtefunktion bezeichnen wir mit \\(t_{1}(x)\\).\nDann dann können wir \\(\\alpha=2\\) und \\(C=0.32\\) abschätzen.\nSchauen wir uns das einmal als Grafik an:\nlower_bound \u0026lt;- 2 upper_bound \u0026lt;- 100 x \u0026lt;- seq(lower_bound, upper_bound, 0.1) gf_dist(\u0026quot;t\u0026quot;, df = dfree, xlim = c(lower_bound, upper_bound), color = \u0026quot;darkred\u0026quot;) %\u0026gt;% gf_line(C*x**(-alpha) ~ x, color = \u0026quot;darkgreen\u0026quot;) Hier eine doppelt-logarithmische Darstellung:\ngf_dist(\u0026quot;t\u0026quot;, df = dfree, xlim = c(lower_bound, upper_bound), color = \u0026quot;darkred\u0026quot;) %\u0026gt;% gf_line( C*x**(-alpha) ~ x, color = \u0026quot;darkgreen\u0026quot;) %\u0026gt;% gf_refine( scale_x_log10(), scale_y_log10() ) Ein ’Gegen-’Beispiel:\nBetrachten wir nun die (rechte Seite – \\(x\u0026gt;1=x_{min}\\)) einer Gauß’schen Standardnormalverteilung.\nMit den Stützstellen \\(x_0 = 1\\) und \\(x_1 = 5\\) können wir \\(\\alpha=7.46\\) und \\(C=0.24\\) abschätzen. Schauen wir uns das einmal als Grafik an:\nlower_bound \u0026lt;- 1 upper_bound \u0026lt;- 8 x \u0026lt;- seq(lower_bound, upper_bound, 0.1) gf_dist(\u0026quot;norm\u0026quot;, xlim = c(lower_bound, upper_bound), color = \u0026quot;darkred\u0026quot;) %\u0026gt;% gf_line(C*x**(-alpha) ~ x, color = \u0026quot;darkgreen\u0026quot;) Hier eine doppelt-logarithmische Darstellung:\ngf_dist(\u0026quot;norm\u0026quot;, xlim = c(lower_bound, upper_bound), color = \u0026quot;darkred\u0026quot;) %\u0026gt;% gf_line( C*x**(-alpha) ~ x, color = \u0026quot;darkgreen\u0026quot;) %\u0026gt;% gf_refine( scale_x_log10(), scale_y_log10() ) Wir erkennen, dass hier etwas nicht passt. Die Standardnormalverteilung ist (vielleicht) keine potenzgesetzich Verteilung?\nEin oft verwendetes Kriterium ist, dass sich die Funktion in der doppelt-logarithmischen Darstellung als Gerade offenbart.\nSchauen wir daher einmal nach:\nlower_bound \u0026lt;- 1 upper_bound \u0026lt;- 8 gf_dist(\u0026quot;norm\u0026quot;, xlim = c(lower_bound, upper_bound), color = \u0026quot;darkred\u0026quot;) %\u0026gt;% gf_refine( scale_x_log10(), scale_y_log10() ) ** — **\nWeiter gilt für potenzgesetzliche Verteilungen wegen\n\\[\\frac{f(x)}{f(c\\cdot x)} = \\frac{C \\cdot x^{-\\alpha}}{C \\cdot (c\\cdot x)^{-\\alpha}} = c^\\alpha\\]\n\\(f(x)\\) und \\(f(c\\cdot x)\\) für alle (beliebig aber festen) \\(c\u0026gt;0\\) proportional, was man gerne als \\(f(x) \\propto f(c \\cdot x)\\) schreibt.\n** — **\nDie Wahrscheinlichkeit für ein (mindestens) \\(8-\\sigma\\) Ereignis liegt bei einer Standardnormalverteilung bei etwa \\(6.66133814775094\\times 10^{-16}\\).\nBei einer t-Verteilung mit 2 Freiheitsgeraden bei etwa 0.00763403608266899$.\nWährend die Eintrittschance eines (mindestens) \\(8-\\sigma\\) Ereignisses bei der Standardnormalverteilung bei etwa \\(1 : round(1/(1-pnorm(8)),0)\\) liegt, ist diese der t-Verteilung mit einem Freiheitsgrad bei etwa $1 : 131\n","date":1613088000,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1613122597,"objectID":"d28effa963393c54c47604608690b314","permalink":"https://sefiroth.net/nab/post/2021-02-12-ein-paar-gedanken-uber-potenzgesetzliche-verteilungen-power-law-distributions/","publishdate":"2021-02-12T00:00:00Z","relpermalink":"/nab/post/2021-02-12-ein-paar-gedanken-uber-potenzgesetzliche-verteilungen-power-law-distributions/","section":"post","summary":"Eine Funktion \\(f(x)\\) heißt potenzgesetzlich, falls\n\\[f(x) = C \\cdot x^a\\]\ngilt, für mindestens alle reellen \\(x \u0026gt; x_{min}\\).\nGewöhnlich setzt man \\(\\alpha = -a\\) und schreibt\n\\[f(x) = C \\cdot x^{-\\alpha}.","tags":["power law distibution","Potenzgesetzliche Verteilung"],"title":"Ein paar Gedanken über potenzgesetzliche Verteilungen (power law distributions)","type":"post"},{"authors":[],"categories":["Statistisches"],"content":"  Eine kleine Liste von fundermentalen Begriffen in der Statistik. Gilt für eine reelle Funktion \\(f: \\mathbf{R} \\to \\mathbf{R}\\):\n \\(f(x)\\) ist nichtnegativ, d.h., \\(f(x) \\geq 0\\), für alle \\(x \\in \\mathbf{R}\\). \\(f(x)\\) ist integrierbar. \\(f(x)\\) ist normiert in dem Sinne, dass \\[\\int_{-\\infty}^\\infty f(x) \\text{d}x = 1\\]  Dann nennen wir \\(f(x)\\) eine Wahrscheinlichkeitsdichtefunktion (engl. probability density funktion kurz pdf) oder kurz Dichte (engl. density).\nDurch \\[P([a, b]) := \\int_a^b f(x) \\text{d} x\\] definiert \\(f\\) eine Wahrscheinlichkeitsverteilung auf den reellen Zahlen.\nIst \\(X\\) eine reelwertige Zufallsvariable (kurz ZV) und existiert eine reelle Funktion \\(f_X(x)\\) der Art, dass für alle \\(a \\in \\mathbf{R}\\)\n\\[P(X \\leq a) = \\int_{-\\infty}^a f_X(x) \\text{d}x\\]\ngilt, so nennt man \\(f\\) die Wahrscheinlichkeitsdichtefunktion von \\(X\\).\nDie Funktion\n\\[F_X(a) = P(X \\leq a)\\]\nnenen wir (Wahrscheinlichkeits-)Verteilung(-sfunktion) (engl. cumulative distribution function kurz cdf aber auch nur distribution function) von \\(X\\).\nGenau dann ist eine Funktion \\(F: \\mathbf{R} \\to [0, 1]\\) eine Verteilungsfunktion, wenngilt:\n Es ist \\(\\lim_{t \\to -\\infty} F(t)=0\\) und \\(\\lim_{t \\to +\\infty} F(t)=1\\) Die Funktion \\(\\overline{F}(t)\\) ist monoton wachsend. Die Funktion \\(\\overline{F}(t)\\) ist rechtsseitig stetig  Die Funktion\n\\[\\overline{F}_X(a) = 1 - F_X(a)\\]\nnennen wir Überlebensfunktion (engl. survival function, complementarey cumulative distribution funktion kurz ccdf, tail distribution, exceedance oder reliability function).\nEs gilt \\(\\overline{F}_X(a) + F_X(a) = 1\\).\nGenau dann ist eine Funktion \\(\\overline{F}: \\mathbf{R} \\to [0, 1]\\) eine Überlebensfunktion, wenn gilt:\n Es ist \\(\\lim_\\limits{t \\to -\\infty} \\overline{F}(t)=1\\) und \\(\\lim_{t \\to + \\infty}\\overline{F}(t)=0\\) Die Funktion \\(\\overline{F}(t)\\) ist monoton fallend. Die Funktion \\(\\overline{F}(t)\\) ist rechtsseitig stetig  Ein paar weitere Eigenschaften von Überlebensfunktionen:\n Nicht-negative stetige ZV \\(X\\) mit Erwartungswert, also \\(\\int_0^\\infty x f(x) \\text{d} x = \\mu \u0026lt; \\infty\\), erfüllen die Markov-Ungleichung  \\[\\overline{F}_X(x) \\leq \\frac{\\operatorname{E}(X)}{x}\\] - Ist \\(X\\) eine ZV und \\(\\overline{F}_X\\) die zugehörige Überlebensfunktion. Existiert \\(E(X)\\), dann gilt \\(\\lim_{t \\to +\\infty}\\overline{F}(x)=0 = o\\left(\\frac{1}{x}\\right)\\). Beweisskizze: Sei \\(f_X\\) die Dichtefunktion von \\(F_X\\) zur ZV \\(X\\). Fü jedes \\(c\u0026gt;0\\) ist dann $$\\[\\begin{align*} E(X) = \\int_0^\\infty x \\cdot f_X(x) \\text{d}x \u0026amp;= \\int_0^cx \\cdot f_X(x) \\text{d}x +\\int_c^\\infty x \\cdot f_X(x) \\text{d}x \\\\ \u0026amp;\\geq \\int_0^cx \\cdot f_X(x) \\text{d}x +\\int_c^\\infty c \\cdot f_X(x) \\text{d}x \\\\ \u0026amp;= \\int_0^cx \\cdot f_X(x) \\text{d}x + c \\cdot \\int_c^\\infty f_X(x) \\text{d}x \\\\ \u0026amp;= \\int_0^cx \\cdot f_X(x) \\text{d}x +c \\cdot \\overline{F}(c) \\end{align*}\\]$$\nDamit gilt nun: \\[0 \\leq c \\cdot \\overline{F}(c) \\leq E(X) - \\int_0^cx \\cdot f_X(x) \\text{d}x\\] Wegen \\(\\lim\\limits_{c \\to +\\infty} \\int_0^cx \\cdot f_X(x) \\text{d}x = E(X)\\) folgt: \\[0 \\leq c \\cdot \\overline{F}(c) \\leq E(X) - \\int_0^cx \\cdot f_X(x) \\text{d}x\\to 0 \\text{ wenn } c \\to \\infty\\] - Für nicht-negative ZV \\(X\\) gilt: \\[E(X) = \\int_0^\\infty \\overline{F}_X(x) \\text{d} x\\]\n ","date":1613088000,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1613119314,"objectID":"5ce9e2b598fc840485f6c41c61df0deb","permalink":"https://sefiroth.net/nab/post/2021-02-12-gedankenstutze-zu-wichtigen-funktionsbegriffen-in-der-statistik/","publishdate":"2021-02-12T00:00:00Z","relpermalink":"/nab/post/2021-02-12-gedankenstutze-zu-wichtigen-funktionsbegriffen-in-der-statistik/","section":"post","summary":"Eine kleine Liste von fundermentalen Begriffen in der Statistik. Gilt für eine reelle Funktion \\(f: \\mathbf{R} \\to \\mathbf{R}\\):\n \\(f(x)\\) ist nichtnegativ, d.h., \\(f(x) \\geq 0\\), für alle \\(x \\in \\mathbf{R}\\).","tags":["Wahrscheinlichkeitsdichte","Wahrscheinlichkeitsdichtefunktion","Dichtefunktion","Dichte","probability desity function","pdf","cumulative distributen function","cdf","complementary cumulative distributen function","ccdf","survial function","tail distribution"],"title":"Gedankenstütze zu wichtigen Funktionsbegriffen in der Statistik","type":"post"},{"authors":[],"categories":["Statistisches"],"content":"  Frage: Was macht einen Wert zum Ausreißer?\nEine mögliche Antwort wäre: Er liegt weit weg von den anderen Werten und hat einen (starken) Einfluss auf unser Modell.\nEine Möglichkeit solche Ausreißer zu finden ist der Cook Abstand (eng.: Cook’s distance). Die Idee dabei ist es zu messen welchen Einfluss ein Wert auf das Modell hat. Dazu schauen wir uns das Modell einmal mit und einmal ohne diesen Wert an und vergleicht diese Ergebnisse.\nSchauen wir uns den Cook Abstand einmal für ein (einfaches) lineares Regressionmodell konkret an:\nVorbereitungen Wir wollen mit mosaic arbeiten, also laden wir das Paket als erstes:\nlibrary(mosaic) Falls die tipping-Daten noch nicht im Verzeichnis liegen, laden wir diese aus dem Internet nach:\nif (!file.exists(\u0026quot;tips.csv\u0026quot;)) { download.file(\u0026quot;https://goo.gl/whKjnl\u0026quot;, destfile = \u0026quot;tips.csv\u0026quot;) } Nun laden wir die tipping-Daten in den Datenrahmen tips:\ntips \u0026lt;- read.csv2(\u0026quot;tips.csv\u0026quot;) Wir brauchen für unser Modell nur den Rechnungsbetrag total_bill und den Trinkgeldbetrag tip für unser Modell:\ntips %\u0026gt;% select(c(\u0026quot;total_bill\u0026quot;, \u0026quot;tip\u0026quot;)) -\u0026gt; tips  Unser Modell: Werfen wir zunächst einen Blick auf das Streudiagramm unserer Daten:\ngf_point(tip ~ total_bill, data = tips) Und erstellen dann ein lineares Modell:\nerg_lm \u0026lt;- lm(tip ~ total_bill, data = tips) summary(erg_lm) ## ## Call: ## lm(formula = tip ~ total_bill, data = tips) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1982 -0.5652 -0.0974 0.4863 3.7434 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.920270 0.159735 5.761 2.53e-08 *** ## total_bill 0.105025 0.007365 14.260 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.022 on 242 degrees of freedom ## Multiple R-squared: 0.4566, Adjusted R-squared: 0.4544 ## F-statistic: 203.4 on 1 and 242 DF, p-value: \u0026lt; 2.2e-16 Betrachten wir nun die Regressionsgerade in unseren Daten:\ngf_point(tip ~ total_bill, data = tips) %\u0026gt;% gf_coefline( model = erg_lm, color = ~ \u0026quot;Regressionsgerade\u0026quot;, show.legend = FALSE )  Für lineare Regressionsmodell können einflussreiche Ausreißer sehr hinderlich sein. Was ändert sich, wenn wir einen Wert, z.B. einen potentiellen Ausreißer, nicht betrachten?\nAls Beispiel wählen wir die folgende Beobachtung aus:\ntips %\u0026gt;% slice(173) -\u0026gt; tips_removed tips_removed ## total_bill tip ## 1 7.25 5.15 tips %\u0026gt;% slice(-173) -\u0026gt; tips_red erg_lm_red \u0026lt;- lm(tip ~ total_bill, data = tips_red) summary(erg_lm_red) ## ## Call: ## lm(formula = tip ~ total_bill, data = tips_red) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2136 -0.5351 -0.0818 0.4951 3.6869 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.86065 0.15709 5.479 1.08e-07 *** ## total_bill 0.10731 0.00723 14.843 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.9992 on 241 degrees of freedom ## Multiple R-squared: 0.4776, Adjusted R-squared: 0.4754 ## F-statistic: 220.3 on 1 and 241 DF, p-value: \u0026lt; 2.2e-16 gf_point(tip ~ total_bill, data = tips_red) %\u0026gt;% gf_coefline( model = erg_lm, color = ~ \u0026quot;Regressionsgerade\u0026quot; ) %\u0026gt;% gf_point( tip ~ total_bill, colour = ~ \u0026quot;Entfernter Punkt\u0026quot;, data = tips_removed) Um zu messen was diese Änderung bewirkt hat, schaut sich der Cook Abstand zunächst die Summe der quadrierten Differenzen der vorhergesagten Werte in beiden Modellen an:\nnew_data \u0026lt;- tibble(total_bill = tips$total_bill) prognose_lm \u0026lt;- predict(erg_lm, newdata = new_data) prognose_lm_red \u0026lt;- predict(erg_lm_red, newdata = new_data) \\[d_j = \\sum_{i=1}^n \\left(\\hat{y}_i - \\hat{y}_{i(j)}\\right)^2\\] Dabei ist \\(\\hat{y}_i\\) die Prognose des Wertes \\(y_i\\) auf Basis von \\(x_i\\) mit dem Originalmodell und \\(\\hat{y}_{i(j)}\\) die Prognose wenn man die \\(j\\)-te Beobachtung aus dem Modell gestrichen hat.\nd_j \u0026lt;- sum((prognose_lm - prognose_lm_red)^2) d_j ## [1] 0.1511406 Der Cook Abstand \\(D_j\\) wird nun noch normiert durch \\[{\\text{var}_{\\text{cook}}} = p \\cdot s_{\\epsilon_i^2}^2\\] Dabei ist \\(s_{\\epsilon_i^2}^2\\) der erwartungstreue Schätzer der Varianz der Residuen und \\(p\\) die Anzahl aller erklärenden Variablen (hier \\(1\\)) \\(+ 1\\).\nEs ist also:\n\\[D_j = \\frac{d_j}{\\text{var}_{\\text{cook}}} = \\frac{\\sum\\limits_{i=1}^n \\left(\\hat{y}_i - \\hat{y}_{i(j)}\\right)^2}{p \\cdot s_{\\epsilon_i^2}^2}\\]\n# Summary des Modells selm \u0026lt;- summary(erg_lm) # Wir finden p als rank im Modell p \u0026lt;- erg_lm$rank # Wir finden den erwatungtreuen Schätzer im Summary des Modells s_quad_eps_quad \u0026lt;- (selm$sigma)^2 var_cook = p * s_quad_eps_quad D_j = d_j / var_cook D_j ## [1] 0.07234504 Wir können den Wert aber auch viel einfacher direkt berechnen lassen und dass für alle \\(j\\) mit Hilfe von cooks.distance(..):\ncooks.distance(erg_lm)[173] ## 173 ## 0.07234504 Wann aber ist nun ein Wert ein einflussreicher Ausreißer?\nCook selber gibt dafür die Bedingung \\(D_j \u0026gt; 1\\) an. Andere Autor*innen schreiben \\(D_j \u0026gt; 4/n\\), wobei \\(n\\) die Anzahl der Beobachtung ist.\nIn unserem Beispiel liefert die Variante \\(D_j \u0026gt; 1\\)\ncooks \u0026lt;- cooks.distance(erg_lm) names(cooks) \u0026lt;- NULL n \u0026lt;- nrow(tips) any(cooks \u0026gt; 1) ## [1] FALSE keinen Ausreißer.\nWenn wir jedoch mit \\(D_j \u0026gt; 4/n\\) suchen .\nany(cooks \u0026gt; 4/n) ## [1] TRUE dann gibt es Ausreißer.\nDie Indices dieser finden wir mit:\nwhich(cooks \u0026gt; 4/n) ## [1] 24 48 57 103 142 157 171 173 179 183 184 185 188 208 211 213 215 238 Bereinigen wir nun unsere Daten um genau diese Werte:\nremove \u0026lt;- which(cooks \u0026gt; 4/n) tips %\u0026gt;% slice(-remove) -\u0026gt; tips_no_outliers tips %\u0026gt;% slice(remove) -\u0026gt; tips_removed erg_lm_no_outliers \u0026lt;- lm(tip ~ total_bill, data = tips_no_outliers) Und schauen uns das Ergebnis an:\nsummary(erg_lm_no_outliers) ## ## Call: ## lm(formula = tip ~ total_bill, data = tips_no_outliers) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.22592 -0.48166 -0.06794 0.46992 2.31414 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.773324 0.139435 5.546 8.2e-08 *** ## total_bill 0.111799 0.006958 16.069 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.7778 on 224 degrees of freedom ## Multiple R-squared: 0.5355, Adjusted R-squared: 0.5334 ## F-statistic: 258.2 on 1 and 224 DF, p-value: \u0026lt; 2.2e-16 gf_point(tip ~ total_bill, data = erg_lm_no_outliers) %\u0026gt;% gf_coefline( model = erg_lm_no_outliers, color = ~\u0026quot;Regressionsgerade\u0026quot; ) Im direkten Vergleich:\ngf_point(tip ~ total_bill, data = erg_lm) %\u0026gt;% gf_coefline( model = erg_lm, color = ~ \u0026quot;Regressionsgerade (Orginal)\u0026quot; ) %\u0026gt;% gf_coefline( model = erg_lm_no_outliers, color = ~ \u0026quot;Regressionsgerade (No Outliers)\u0026quot; ) %\u0026gt;% gf_point( tip ~ total_bill, color = ~ \u0026quot;Entfernte Punkte\u0026quot;, data = tips_removed )  Unsere beiden Modelle als Formeln Das ursprüngliche Modell: \\[\\widehat{tips}_{lm} = 0.9202696 + 0.1050245 \\cdot total\\_bill + \\epsilon\\]\nDas um pot. Ausreißer bereinigte Modell: \\[\\widehat{tips}_{lm\\_no} 0.7733236 + 0.1117985 \\cdot total\\_bill + \\epsilon\\]\n ","date":1593388800,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1623913871,"objectID":"da19fda6d3fb008252eaf4be5189936a","permalink":"https://sefiroth.net/nab/post/cook-abstand/","publishdate":"2020-06-29T00:00:00Z","relpermalink":"/nab/post/cook-abstand/","section":"post","summary":"Frage: Was macht einen Wert zum Ausreißer?\nEine mögliche Antwort wäre: Er liegt weit weg von den anderen Werten und hat einen (starken) Einfluss auf unser Modell.\nEine Möglichkeit solche Ausreißer zu finden ist der Cook Abstand (eng.","tags":["Cook Abstand","Cooks Distance","Ausreißer","Outliers"],"title":"Cook Abstand","type":"post"},{"authors":[],"categories":["Statistisches"],"content":"   Bei der Rangkorrelation werden statt der Werte die Ränge der Werte zur Berechnung herangezogen.\nBeispiel: Es seien die folgenden Datenpunkte gegeben:\n  x  0.15  2.91  4.2  3.63  4.88  7.80  8.70  4.96  6.71  10.06    y  1.38  4.30  7.9  14.87  22.20  36.72  49.94  63.77  82.76  100.12     Das Streudiagramm dieser Daten sieht dann so aus:\nDer Korrelationskoeffizient ist nun:\n\\[r = r_{BP} = 0.7944953\\]\nSchauen wir uns nun die Werte der Tabelle mit ihrem Rang an:\n  x  0.15  2.91  4.2  3.63  4.88  7.80  8.70  4.96  6.71  10.06    y  1.38  4.30  7.9  14.87  22.20  36.72  49.94  63.77  82.76  100.12    rx  1.00  2.00  4.0  3.00  5.00  8.00  9.00  6.00  7.00  10.00    ry  1.00  2.00  3.0  4.00  5.00  6.00  7.00  8.00  9.00  10.00     Das Streudiagramm der Ränge sieht nun wie folgt aus:\nUnd der Korrelationskoeffizient der Ränge ist nun:\n\\[r_{sp} = r_{ry,rx}=0.8909091\\]\nDieser Korrelationskoeffizient der Ränge wird Rangkorrelationkoeffizient genannt.\nAber Vorsicht: Die Ränge anstatt der Werte in die Formel für den Korrelationskoeffizienten einzusetzen funktioniert nur, wenn jeder Wert genau einmal vorkommt!\nUm das zu sehen, modifizieren wir unser Beispiel, so dass an zwei Stellen die Werte doppelt vorkommen. Wir erhalten damit die folgende, neue Tabelle:\n  x  0.15  2.91  3.63  3.63  4.88  7.80  8.70  4.96  6.71  10.06    y  1.38  4.30  7.90  22.20  22.20  36.72  49.94  63.77  82.76  100.12    rx  1.00  2.00  3.00  4.00  5.00  8.00  9.00  6.00  7.00  10.00    ry  1.00  2.00  3.00  4.00  5.00  6.00  7.00  8.00  9.00  10.00     Bevor wir nun einfach die Ränge so in die Formel für den Korrelationskoeffizienten (nach Pearson) einsetzen können, müssen wir noch etwas beachten, was die Definition von Rängen angeht. Denn dort steht im Kleingedruckten folgendes:\n Den ranggleichen Beobachtungen wird das arithmetische Mittel der auf sie fallenden Ränge zugeordnet!\n Darum müssen wir die Ränge noch etwas korrigieren und erhalten:\n  x  0.15  2.91  3.63  3.63  4.88  7.80  8.70  4.96  6.71  10.06    y  1.38  4.30  7.90  22.20  22.20  36.72  49.94  63.77  82.76  100.12    rx  1.00  2.00  3.50  3.50  5.00  8.00  9.00  6.00  7.00  10.00    ry  1.00  2.00  3.00  4.50  4.50  6.00  7.00  8.00  9.00  10.00     Nun können wir von den Rängen wieder “ganz normal” den Korrelationskoeffizienten berechnen und erhalten: \\[r_{sp} = r_{ry,rx} = 0.8932927\\]\n ","date":1590364800,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1590401700,"objectID":"a3a3044130b8ff04addee97ad6f2b795","permalink":"https://sefiroth.net/nab/post/ein-kleines-beispiel-zum-rangkorrelationskoeffizienten/","publishdate":"2020-05-25T00:00:00Z","relpermalink":"/nab/post/ein-kleines-beispiel-zum-rangkorrelationskoeffizienten/","section":"post","summary":"Bei der Rangkorrelation werden statt der Werte die Ränge der Werte zur Berechnung herangezogen.\nBeispiel: Es seien die folgenden Datenpunkte gegeben:\n  x  0.15  2.","tags":["Zusammemhangsanalyse","Korrelation","Rang","Rangkorrelation","Spearman","Rangkorrelationskoeffizient","Korrelationskoeffizient"],"title":"Ein kleines Beispiel zum Rangkorrelationskoeffizienten","type":"post"},{"authors":[],"categories":["Allgemeines"],"content":"  Das korrigieren von Arbeiten ist nicht gerade des Lehrenden liebste Tätigkeit. Vorallem, wenn man eine Mischung auf Multiple-Choice und Freitest Aufgaben zu korrigieren hat und leider keine gute technische Unterstützung vorfindet.\nKlar gibt es wunderschöne Lösungen mit R dazu, wie zum Beispiel http://www.r-exams.org.\nAber zum Glück kann man den Ablauf auch “mit Bordmitteln” etwas verbessern.\nWir bekommen eine Liste der Teilnehmer, in Form eine Excel- oder CSV-Datei, in die wir die Punkte eintragen können, die von der Hochschule digital verarbeitet wird. Diese Liste nehmen wir als Grundlage um den Ablauf etwas zu optimieren.\nIn den Klausuren gibt es vier Sorten von Aufgabentypen:\nMultiple Choice mit genau einer Antwortmöglichkeit Multiple Choice mit mehr als einer Antwortmöglichkeit (Ganze) Zahlen als Antwort auf eine Frage Begründungen und/oder freie Textantworten  Die ersten drei Aufgabentypen können sehr schön mit Hilfe einer Exceltabelle erfasst werden. Der vierte Aufgabentyp muss direkt bewertet werden und wird in so ebenfalls in die Exceltabelle eingegeben.\nDie erfassten Daten der Klausuren liegen zu Beginn der Auswertung in einer Exceldatei bereit, die in etwa wie folgt aussieht:\n\u0026lt; \u0026lt; \u0026lt; BILD EXCEL DATEI \u0026gt; \u0026gt; \u0026gt;\nDiese Datei wird unter dem Namen Klausurteilnehmendenliste.xslx gespeichert.\nDie Auswertung geschieht dann mittels eines kleinen R-Skripts:\nlibrary(mosaic) library(readxl) max.show \u0026lt;- 10 Aufgaben.Index \u0026lt;- 1:40 Aufgaben.Typ \u0026lt;- rep(\u0026quot;MC\u0026quot;, 40) for (idx in c(1, 9, 14, 36, 37, 38, 39, 40)){ Aufgaben.Typ[idx] \u0026lt;- \u0026quot;nonMC\u0026quot; } Aufgaben.Punkte.max \u0026lt;- c( 8, 1, 1, 1, 2, 1, 2, 2, 3, 1, 2, 1, 1, 3, 2, 1, 4, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 6, 4, 4, 6, 8) Aufgaben.MC.richtig \u0026lt;- c(NA, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;D\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, NA, \u0026quot;B\u0026quot;, \u0026quot;BD\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;A\u0026quot;, NA, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;BC\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;B\u0026quot;, NA, NA, NA, NA, NA ) if (sum(Aufgaben.Punkte.max) != 90) stop(\u0026quot;Gesamtpunktzahl ist nicht 90!\u0026quot;) for (idx in 1:40) { if (Aufgaben.Typ[idx] == \u0026quot;MC\u0026quot;) { if (is.na(Aufgaben.MC.richtig[idx])) stop(paste(\u0026quot;MC Aufgabe\u0026quot;, idx, \u0026quot;ohne Musterlösung!\u0026quot;)) } else { if (!is.na(Aufgaben.MC.richtig[idx])) stop(paste(\u0026quot;Nicht-MC Aufgabe\u0026quot;, idx, \u0026quot;mit Musterlösung!\u0026quot;)) } } Aufgaben \u0026lt;- c( 531388, NA, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, NA, \u0026quot;B\u0026quot;, \u0026quot;BD\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;A\u0026quot;, NA, \u0026quot;B\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;E\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;, NA, NA, NA, NA, NA ) sumMC \u0026lt;- function(test) { summe \u0026lt;- 0 for(idx in 1:40) { if (!is.na(test[idx+1]) \u0026amp;\u0026amp; (!(test[idx+1] == \u0026quot;/\u0026quot; ))) { if (Aufgaben.Typ[idx] == \u0026quot;MC\u0026quot;) { tmp \u0026lt;- length(Aufgaben.MC.richtig[idx]) if (tmp \u0026gt; 1) { pp \u0026lt;- Aufgaben.Punkte.max[idx]/tmp for(x in unlist(strsplit(Aufgaben.MC.richtig[idx], NULL))) { if (toupper(x) %in% toupper(test[idx+1])) { summe \u0026lt;- summe + pp } } } else { if (toupper(test[idx+1]) == toupper(Aufgaben.MC.richtig[idx])) { summe \u0026lt;- summe + Aufgaben.Punkte.max[idx] } } } else { if (!is.na(test[idx+1])) { summe \u0026lt;- summe + as.integer(test[idx+1]) } } } } return(summe) } Klausurteilnehmendenliste \u0026lt;- read_excel(\u0026quot;Klausurteilnehmendenliste.xlsx\u0026quot;) Klausurteilnehmendenliste \u0026lt;- Klausurteilnehmendenliste %\u0026gt;% select(c(-1,-2)) %\u0026gt;% mutate_all(toupper) Punkte \u0026lt;- apply(Klausurteilnehmendenliste, 1, sumMC) Klausurteilnehmendenliste \u0026lt;- cbind(Klausurteilnehmendenliste, Punkte) head(select(Klausurteilnehmendenliste, c(1, \u0026quot;Punkte\u0026quot;)), max.show) tail(select(Klausurteilnehmendenliste, c(1, \u0026quot;Punkte\u0026quot;)), max.show) Klausurteilnehmendenliste %\u0026gt;% filter(Punkte \u0026gt; 10) %\u0026gt;% gf_counts(~ Punkte, data=.) Mit den Zeilen\nAufgaben.Typ \u0026lt;- rep(\u0026quot;MC\u0026quot;, 40) for (idx in c(1, 9, 14, 36, 37, 38, 39, 40)){ Aufgaben.Typ[idx] \u0026lt;- \u0026quot;nonMC\u0026quot; } werden zunächst 40 MC-Aufgaben angelegt und danach die Aufgaben 1, 9, 14, 36, 37, 38, 39 und 40 als nicht MC Aufgaben gekennzeichnet. Hier muss später die Punktzahl direkt eingegeben werden!\nAufgaben.Punkte.max \u0026lt;- c( 8, 1, 1, 1, 2, 1, 2, 2, 3, 1, 2, 1, 1, 3, 2, 1, 4, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 6, 4, 4, 6, 8) Aufgaben.MC.richtig \u0026lt;- c(NA, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;D\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, NA, \u0026quot;B\u0026quot;, \u0026quot;BD\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;A\u0026quot;, NA, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;BC\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;B\u0026quot;, NA, NA, NA, NA, NA ) In Aufgaben.Punkte.max werden die maximal erreichbaren Punkte pro Aufgabe gespeichert. In Aufgaben.MC.richtig sind alle nicht MC-Aufgaben mit NA gekennzeichnet. Die Lösungen werden als Zeichenkette (in Großbuchstaben) hinterlegt. Sollte eine MC-Aufgabe mehrere richtige Antworten haben, so schreibt man diese einfach hintereinander. So bedeutet “BD”, dass die Lösungen “B” und “D” richtig sind.\nWird in den Lösungen für eine Aufgabe “/” eingetragen, so gilt diese Aufgabe als nicht bearbeitet und wird mit 0 Punkten bewertet.\n","date":1584489600,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1584523621,"objectID":"4ff97692747e34a12555c6cd8e10c32b","permalink":"https://sefiroth.net/nab/post/korrigieren-von-arbeiten-mit-excel-und-r/","publishdate":"2020-03-18T00:00:00Z","relpermalink":"/nab/post/korrigieren-von-arbeiten-mit-excel-und-r/","section":"post","summary":"Das korrigieren von Arbeiten ist nicht gerade des Lehrenden liebste Tätigkeit. Vorallem, wenn man eine Mischung auf Multiple-Choice und Freitest Aufgaben zu korrigieren hat und leider keine gute technische Unterstützung vorfindet.","tags":["Lehre","R"],"title":"Korrigieren von Arbeiten mit Excel und R","type":"post"},{"authors":[],"categories":"Mathematik","content":"  Das schnelle Lösen von quadratischen Gleichungen – mit (nahezu) ganzzahligen Lösungen – ist etwas für den (Wurzel-)Satz von Vieta.\nIst eine quadratischen Gleichung in Normalform, genügt sie der folgenden Darstellung:\n\\[x^2 + px + q = 0\\]\nDiese Gleichung hat, wenn überhaupt, zwei (nicht notwendig verschiedene) Lösungen \\(x_1\\) und \\(x_2\\).\nWegen\n\\[(x-x_1) \\cdot (x-x_2) = x^2 -(x_1+x_2)\\cdot x + x_1\\cdot x_2 = x^2 + px + q\\]\nmuss \\(p = -\\left(x_1+x_2\\right)\\) und gleichzeitig \\(q = x_1 \\cdot x_2\\) gelten.\nSucht man nach (ganzzahligen) \\(x_1\\) und \\(x_2\\), so nimmt man die Teilermenge \\(T_q\\) von \\(q\\) und prüft für jedes \\(x \\in T_q\\) ob es ein \\(y \\in T_q\\) gibt, so dass \\(\\pm x\\) und \\(\\pm y\\) sowohl die Gleichungen für \\(p\\) und \\(q\\) erfüllen. Ein für \\(q\\) passendes Paar zu finden ist dabei nicht das Problem. Aber die Gleichung für \\(p\\) muss auch erfüllt sein.\nEin Beispiel Hat die Gleichung \\(x^2 - 8 \\cdot x + 12 = 0\\) also nur ganzzahlige Lösungen, so reicht es die Teiler von \\(12\\) zu untersuchen.\nNun ist \\(12 = 2 \\cdot 2 \\cdot 3\\) und somit die Teilermenge von \\(12\\) die Menge \\(T_{12} = \\{1, 2, 3, 4, 6, 12\\}\\).\nWegen der \\(p=-8\\) muss \\(x_1 + x_2\\) eine positive Zahl sein. Wegen \\(q=+12\\) müssen \\(x_1\\) und \\(x_2\\) beide positiv sein.\nGesucht ist also die Lösung von \\(x_1+x_2 = 8\\) für \\(x_1, x_2 \\in T_{12}\\).\nFür \\(1\\) findet sich \\(12\\), für \\(2\\) findet sich \\(6\\) und für \\(3\\) findet sich \\(4\\). Danach drehen sich die Positionen nur noch um, es gibt aber keinen neuen Paarungen.\nWeil \\(1 + 12 = 13 \\neq 8\\) und \\(3+4=7 \\neq 8\\) ist fallen diese Paare als Lösungen aus.\nDa \\(2+6=8\\) und \\(2 \\cdot 6 = 12\\) ist, haben wir mit \\(x_1=2\\) und \\(x_2=6\\) die Lösung gefunden.\n Dr. Loh’s Methode Dr. Loh’s Methode zielt darauf ab, das Raten des Lösungspaars \\(x_1\\) und \\(x_2\\) etwas stärker in eine Berechnung zu überführen.\nWir starten mit \\(p=8\\) und wissen, dass \\(p\\) in die Summe von zwei Zahlen zerlegt werden muss. \\(p/2\\) ist der Scheitelpunktstelle der quadratischen Funktion \\(f(x) = x^2+px+q\\) und die zwei Nullstellen, sofern existent, haben von der Scheitelpunktstelle \\(p/\\) den selben Abstand, den wir \\(u\\) nennen wollen.\nDamit ist\n\\[x_1 = \\frac{p}{2} +u \\quad\\text{ und }\\quad x_2 = \\frac{p}{2} - u\\]\nIn unserem Beispiel gilt: \\[x_1 = 4+u \\quad\\text{ und }\\quad x_2 = 4 - u\\] Da nachdem Wurzelsatz von Vieta \\(q=x_1 \\cdot x_2\\) ist, gilt (mit der 3. Binomischen Formel) für unser Beispiel:\n\\[x_1 \\cdot x_2 = (4+u)(4-u)=4^2-u^2 = 16-u^2 = 12\\]\nWir stellen die Gleichung um und erhalten:\n\\[u^2 = 16-12 = 4\\] Diese Wurzelgleichung hat nun zwei Lösungen:\n\\[u = \\pm\\sqrt{4} = \\pm 2, \\text{ somit: }u_1=-2, u_2 = 2\\]\nUnd – entweder durch ausprobieren oder nachdenken – erhalten wir die Lösung \\(x_1=4+u_1=2\\) und \\(x_2=4-u_1=6\\) ergibt.\n Ein weiteres Beispiel \\[x^2 -10 \\cdot x + 24 = 0\\]\nDann ist \\(x_1=5-u\\), \\(x_2=5+u\\), \\(x_1\\cdot x_2=25-u^2 = 24\\) und somit \\(u^2=\\pm 1\\) und die Lösung \\(x_1=4\\) sowie \\(x_2=6\\).\n Und noch ein Beispiel \\[x^2 - 7 \\cdot x + 12 = 0\\]\nNun ist \\(x_1=3{,}5-u\\), \\(x_2=3{,}5+u\\), \\(x_1 \\cdot x_2 = 12{,}25-u^2=12\\) und somit \\(u^2=\\pm 0{,}25\\) und die Lösung \\(x_1=3\\) und \\(x_2=4\\).\n ","date":1582675200,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1582702569,"objectID":"b285ae6976e1445211e876e9fc759aaf","permalink":"https://sefiroth.net/nab/post/der-satz-von-vieta-und-dr-loh-s-methode/","publishdate":"2020-02-26T00:00:00Z","relpermalink":"/nab/post/der-satz-von-vieta-und-dr-loh-s-methode/","section":"post","summary":"Das schnelle Lösen von quadratischen Gleichungen – mit (nahezu) ganzzahligen Lösungen – ist etwas für den (Wurzel-)Satz von Vieta.\nIst eine quadratischen Gleichung in Normalform, genügt sie der folgenden Darstellung:","tags":["Quadratische Gleichung","Satz von Vieta","Dr. Loh's Methode"],"title":"Der Satz von Vieta und Dr. Loh's Methode","type":"post"},{"authors":[],"categories":["Information"],"content":"  Nach readr kommt vroom. In der Zwischenzeit liegt vroom in der Version 1.2.0 vor und daher habe ich mir ein paar Stunden Zeit gekommen um ein paar erste Experimente damit zu machen.\nErste Schritte mit vroom library(vroom) # URL für die Quelle von tips.csv: url \u0026lt;- \u0026quot;https://goo.gl/whKjnl\u0026quot; # Locale auf Deutsche Sprache, Dezimalkomma und Gruppierungspunkte setzen mylocale \u0026lt;- locale(\u0026quot;de\u0026quot;, decimal_mark = \u0026quot;,\u0026quot;, grouping_mark = \u0026quot;.\u0026quot;) # Spaltentypen ggf. vorgeben: mycols \u0026lt;- cols( col_number(), # total_bill col_number(), # tip col_factor(), # sex col_factor(), # smoker col_factor(), # day col_factor(), # time col_integer() # size ) # Laden mit vroom, Spaltentypen erraten, Locale auf mylocale tips.vroom \u0026lt;- vroom(url, locale = mylocale) ## Rows: 244 Columns: 7 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: \u0026quot;;\u0026quot; ## chr (4): sex, smoker, day, time ## dbl (3): total_bill, tip, size ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(tips.vroom) ## # A tibble: 6 x 7 ## total_bill tip sex smoker day time size ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 17.0 1.01 Female No Sun Dinner 2 ## 2 10.3 1.66 Male No Sun Dinner 3 ## 3 21.0 3.5 Male No Sun Dinner 3 ## 4 23.7 3.31 Male No Sun Dinner 2 ## 5 24.6 3.61 Female No Sun Dinner 4 ## 6 25.3 4.71 Male No Sun Dinner 4 str(tips.vroom) ## spec_tbl_df [244 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ total_bill: num [1:244] 17 10.3 21 23.7 24.6 ... ## $ tip : num [1:244] 1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ... ## $ sex : chr [1:244] \u0026quot;Female\u0026quot; \u0026quot;Male\u0026quot; \u0026quot;Male\u0026quot; \u0026quot;Male\u0026quot; ... ## $ smoker : chr [1:244] \u0026quot;No\u0026quot; \u0026quot;No\u0026quot; \u0026quot;No\u0026quot; \u0026quot;No\u0026quot; ... ## $ day : chr [1:244] \u0026quot;Sun\u0026quot; \u0026quot;Sun\u0026quot; \u0026quot;Sun\u0026quot; \u0026quot;Sun\u0026quot; ... ## $ time : chr [1:244] \u0026quot;Dinner\u0026quot; \u0026quot;Dinner\u0026quot; \u0026quot;Dinner\u0026quot; \u0026quot;Dinner\u0026quot; ... ## $ size : num [1:244] 2 3 3 2 4 4 2 4 2 2 ... ## - attr(*, \u0026quot;spec\u0026quot;)= ## .. cols( ## .. total_bill = col_double(), ## .. tip = col_double(), ## .. sex = col_character(), ## .. smoker = col_character(), ## .. day = col_character(), ## .. time = col_character(), ## .. size = col_double(), ## .. .delim = \u0026quot;;\u0026quot; ## .. ) ## - attr(*, \u0026quot;problems\u0026quot;)=\u0026lt;externalptr\u0026gt; object.size(tips.vroom) ## 20632 bytes # Laden mit vroom, Spaltentypen mycols, Locale auf mylocale tips.vroom2 \u0026lt;- vroom(url, col_types = mycols, locale = mylocale) head(tips.vroom2) ## # A tibble: 6 x 7 ## total_bill tip sex smoker day time size ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 17.0 1.01 Female No Sun Dinner 2 ## 2 10.3 1.66 Male No Sun Dinner 3 ## 3 21.0 3.5 Male No Sun Dinner 3 ## 4 23.7 3.31 Male No Sun Dinner 2 ## 5 24.6 3.61 Female No Sun Dinner 4 ## 6 25.3 4.71 Male No Sun Dinner 4 str(tips.vroom2) ## spec_tbl_df [244 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ total_bill: num [1:244] 17 10.3 21 23.7 24.6 ... ## $ tip : num [1:244] 1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ... ## $ sex : Factor w/ 2 levels \u0026quot;Female\u0026quot;,\u0026quot;Male\u0026quot;: 1 2 2 2 1 2 2 2 2 2 ... ## $ smoker : Factor w/ 2 levels \u0026quot;No\u0026quot;,\u0026quot;Yes\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ day : Factor w/ 4 levels \u0026quot;Sun\u0026quot;,\u0026quot;Sat\u0026quot;,\u0026quot;Thur\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ time : Factor w/ 2 levels \u0026quot;Dinner\u0026quot;,\u0026quot;Lunch\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ size : int [1:244] 2 3 3 2 4 4 2 4 2 2 ... ## - attr(*, \u0026quot;spec\u0026quot;)= ## .. cols( ## .. total_bill = col_number(), ## .. tip = col_number(), ## .. sex = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE), ## .. smoker = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE), ## .. day = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE), ## .. time = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE), ## .. size = col_integer(), ## .. .delim = \u0026quot;;\u0026quot; ## .. ) ## - attr(*, \u0026quot;problems\u0026quot;)=\u0026lt;externalptr\u0026gt; object.size(tips.vroom2) ## 19416 bytes # Readr library(readr) ## Registered S3 methods overwritten by \u0026#39;readr\u0026#39;: ## method from ## format.col_spec vroom ## print.col_spec vroom ## print.collector vroom ## print.date_names vroom ## print.locale vroom ## str.col_spec vroom ## ## Attache Paket: \u0026#39;readr\u0026#39; ## Die folgenden Objekte sind maskiert von \u0026#39;package:vroom\u0026#39;: ## ## as.col_spec, col_character, col_date, col_datetime, col_double, ## col_factor, col_guess, col_integer, col_logical, col_number, ## col_skip, col_time, cols, cols_condense, cols_only, date_names, ## date_names_lang, date_names_langs, default_locale, fwf_cols, ## fwf_empty, fwf_positions, fwf_widths, locale, output_column, ## problems, spec tips.readr \u0026lt;- readr::read_csv2(url) ## ℹ Using \u0026#39;\\\u0026#39;,\\\u0026#39;\u0026#39; as decimal and \u0026#39;\\\u0026#39;.\\\u0026#39;\u0026#39; as grouping mark. Use `read_delim()` for more control. ## ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## cols( ## total_bill = col_double(), ## tip = col_double(), ## sex = col_character(), ## smoker = col_character(), ## day = col_character(), ## time = col_character(), ## size = col_double() ## ) head(tips.readr) ## # A tibble: 6 x 7 ## total_bill tip sex smoker day time size ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 17.0 1.01 Female No Sun Dinner 2 ## 2 10.3 1.66 Male No Sun Dinner 3 ## 3 21.0 3.5 Male No Sun Dinner 3 ## 4 23.7 3.31 Male No Sun Dinner 2 ## 5 24.6 3.61 Female No Sun Dinner 4 ## 6 25.3 4.71 Male No Sun Dinner 4 str(tips.readr) ## spec_tbl_df [244 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ total_bill: num [1:244] 17 10.3 21 23.7 24.6 ... ## $ tip : num [1:244] 1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ... ## $ sex : chr [1:244] \u0026quot;Female\u0026quot; \u0026quot;Male\u0026quot; \u0026quot;Male\u0026quot; \u0026quot;Male\u0026quot; ... ## $ smoker : chr [1:244] \u0026quot;No\u0026quot; \u0026quot;No\u0026quot; \u0026quot;No\u0026quot; \u0026quot;No\u0026quot; ... ## $ day : chr [1:244] \u0026quot;Sun\u0026quot; \u0026quot;Sun\u0026quot; \u0026quot;Sun\u0026quot; \u0026quot;Sun\u0026quot; ... ## $ time : chr [1:244] \u0026quot;Dinner\u0026quot; \u0026quot;Dinner\u0026quot; \u0026quot;Dinner\u0026quot; \u0026quot;Dinner\u0026quot; ... ## $ size : num [1:244] 2 3 3 2 4 4 2 4 2 2 ... ## - attr(*, \u0026quot;spec\u0026quot;)= ## .. cols( ## .. total_bill = col_double(), ## .. tip = col_double(), ## .. sex = col_character(), ## .. smoker = col_character(), ## .. day = col_character(), ## .. time = col_character(), ## .. size = col_double() ## .. ) object.size(tips.readr) ## 20400 bytes # Readr tips.readr2 \u0026lt;- readr::read_csv2(url, col_types = list( col_double(), # total_bill col_double(), # tip col_factor(), # sex col_factor(), # smoker col_factor(), # day col_factor(), # time col_integer() # size ) ) ## ℹ Using \u0026#39;\\\u0026#39;,\\\u0026#39;\u0026#39; as decimal and \u0026#39;\\\u0026#39;.\\\u0026#39;\u0026#39; as grouping mark. Use `read_delim()` for more control. head(tips.readr2) ## # A tibble: 6 x 7 ## total_bill tip sex smoker day time size ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 17.0 1.01 Female No Sun Dinner 2 ## 2 10.3 1.66 Male No Sun Dinner 3 ## 3 21.0 3.5 Male No Sun Dinner 3 ## 4 23.7 3.31 Male No Sun Dinner 2 ## 5 24.6 3.61 Female No Sun Dinner 4 ## 6 25.3 4.71 Male No Sun Dinner 4 str(tips.readr2) ## spec_tbl_df [244 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ total_bill: num [1:244] 17 10.3 21 23.7 24.6 ... ## $ tip : num [1:244] 1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ... ## $ sex : Factor w/ 2 levels \u0026quot;Female\u0026quot;,\u0026quot;Male\u0026quot;: 1 2 2 2 1 2 2 2 2 2 ... ## $ smoker : Factor w/ 2 levels \u0026quot;No\u0026quot;,\u0026quot;Yes\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ day : Factor w/ 4 levels \u0026quot;Sun\u0026quot;,\u0026quot;Sat\u0026quot;,\u0026quot;Thur\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ time : Factor w/ 2 levels \u0026quot;Dinner\u0026quot;,\u0026quot;Lunch\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ size : int [1:244] 2 3 3 2 4 4 2 4 2 2 ... ## - attr(*, \u0026quot;spec\u0026quot;)= ## .. cols( ## .. total_bill = col_double(), ## .. tip = col_double(), ## .. sex = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE), ## .. smoker = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE), ## .. day = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE), ## .. time = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE), ## .. size = col_integer() ## .. ) object.size(tips.readr2) ## 19184 bytes # Mit Bordmitteln von R download.file(\u0026quot;https://goo.gl/whKjnl\u0026quot;, destfile = \u0026quot;tips.csv\u0026quot;) tips.csv2 \u0026lt;- read.csv2(\u0026quot;tips.csv\u0026quot;) head(tips.csv2) ## total_bill tip sex smoker day time size ## 1 16.99 1.01 Female No Sun Dinner 2 ## 2 10.34 1.66 Male No Sun Dinner 3 ## 3 21.01 3.50 Male No Sun Dinner 3 ## 4 23.68 3.31 Male No Sun Dinner 2 ## 5 24.59 3.61 Female No Sun Dinner 4 ## 6 25.29 4.71 Male No Sun Dinner 4 str(tips.csv2) ## \u0026#39;data.frame\u0026#39;: 244 obs. of 7 variables: ## $ total_bill: num 17 10.3 21 23.7 24.6 ... ## $ tip : num 1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ... ## $ sex : chr \u0026quot;Female\u0026quot; \u0026quot;Male\u0026quot; \u0026quot;Male\u0026quot; \u0026quot;Male\u0026quot; ... ## $ smoker : chr \u0026quot;No\u0026quot; \u0026quot;No\u0026quot; \u0026quot;No\u0026quot; \u0026quot;No\u0026quot; ... ## $ day : chr \u0026quot;Sun\u0026quot; \u0026quot;Sun\u0026quot; \u0026quot;Sun\u0026quot; \u0026quot;Sun\u0026quot; ... ## $ time : chr \u0026quot;Dinner\u0026quot; \u0026quot;Dinner\u0026quot; \u0026quot;Dinner\u0026quot; \u0026quot;Dinner\u0026quot; ... ## $ size : int 2 3 3 2 4 4 2 4 2 2 ... object.size(tips.csv2) ## 14720 bytes  ","date":1582243200,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1582287473,"objectID":"4ad6a6f8f0fa8d808bcda78bf944894a","permalink":"https://sefiroth.net/nab/post/erste-schritte-mit-vroom/","publishdate":"2020-02-21T00:00:00Z","relpermalink":"/nab/post/erste-schritte-mit-vroom/","section":"post","summary":"Nach readr kommt vroom. In der Zwischenzeit liegt vroom in der Version 1.2.0 vor und daher habe ich mir ein paar Stunden Zeit gekommen um ein paar erste Experimente damit zu machen.","tags":["R","vroom","readr","tidyverse"],"title":"Erste Schritte mit `vroom`","type":"post"},{"authors":[],"categories":["Statistisches"],"content":"  Vor kurzem fand ich mal wieder eine Anfrage einer Studierenden in meinem Email Postfach. Die Frage lautete in etwa wie folgt:\n Guten Tag Herr Markgraf,\nich würde gerne die Hypothese untersuchen: Die reduzierte Abhängigkeit des Iphones tagsüber liegt am schönen Wetter. Dazu habe ich eine Variable “iphones.tagsüber.unbeachtet” mit 1x, 2x und 3x täglich als Ausprägungen und eine andere Variable “wetter.ist.gut”, die als Ausprägung “Ja” und “Nein” hat. Welchen Test kann ich dazu zur Überprüfung einer Abhängigkeit nehmen?\nVielen Dank im Voraus.\nMfG Monika Mustermann\n Natürlich ist diese Frage im Prinzip einfach zu beantworten, sogar von Leuten, die Statistik an einer Hochschule gehört haben. – Aber da ich ja auch sonst nichts zu tun habe, gebe ich gerne statistische Hilfestellung für Studierende. Sicher, ich verdiene damit eigentlich mein Geld. Also ist es nur natürlich, dass ich so etwas vollkommen unentgeldlich mache. Und wieso sollten Studierende einfach mal ein Buch in die Hand nehmen und selber nachdenken? Es gibt vermutlich keine Bücher zu diesem Thema, denn es ist ganz sicher eine Geheimwissenschaft. Und wieso sollte man dann also seine Betreuungsperson zu diesem Probem fragen? Die hat ja auch so viel zu tun… – Egal.\nWas haben wir hier vorliegen? – Im einfachsten Fall sind es zwei kategoriale Variablen, und wir wollen sehen ob diese von einander (un-)abhängig sind.\nMangels tatsächlicher Daten basteln wir uns einfach mal ein Beispiel:\nWir bastlen uns ein Beispiel # Wie immer zuerst das Paket \u0026#39;mosaic\u0026#39; laden library(mosaic) # Einen beliebigen Startwert für den Zufallszahlengenerator # für die Reproduzierbarkeit set.seed(123) # Anzahl der Vorfälle insgesamt n \u0026lt;- 176 # Anzahl der Wiederholungen für die SBI-Methoden loops \u0026lt;- 10000 # Erfinden eines Beispieldatensatzes daten \u0026lt;- data.frame( iphones.tagsüber.unbeachtet = sample(rep(c(\u0026quot;1xtäglich\u0026quot;,\u0026quot;2xtäglich\u0026quot;,\u0026quot;3xtäglich\u0026quot;),n),n), wetter.ist.gut = sample(rep(c(\u0026quot;Ja\u0026quot;,\u0026quot;Nein\u0026quot;),n),n) ) # Ausgabe der ersten Zeilen des Datensatzes head(daten) ## iphones.tagsüber.unbeachtet wetter.ist.gut ## 1 1xtäglich Ja ## 2 1xtäglich Nein ## 3 2xtäglich Ja ## 4 3xtäglich Nein ## 5 1xtäglich Ja ## 6 2xtäglich Ja  Ein Blick auf Kennzahlen und Visualisierungsmöglichkeiten Man kann diese Daten als Kreuztabelle zusammenfassen und diese dann mit Hilfe eines Mosaikplots darstellen:\ntally(iphones.tagsüber.unbeachtet ~ wetter.ist.gut, data = daten) ## wetter.ist.gut ## iphones.tagsüber.unbeachtet Ja Nein ## 1xtäglich 29 33 ## 2xtäglich 34 26 ## 3xtäglich 27 27 mosaicplot(wetter.ist.gut ~ iphones.tagsüber.unbeachtet, data = daten) # Für später speichern wir die Kreuztabelle in obs.tab obs.tab \u0026lt;- tally(iphones.tagsüber.unbeachtet ~ wetter.ist.gut, data = daten)  Von der Forschungsthese zur Hypothese Um nun zwischen abhängig und unabhängig statistisch zu unterscheiden, sollte man sich die Null- und Alternativhypothese genau überlegen und operationalisieren.\nEin Blick auf die (orginale) Forschungsthese: “Die reduzierte Abhängigkeit des Iphones tagsüber liegt am schönen Wetter.”\nOh je, eine kausale Forschungsthese. Ein dezenter Hinweis auf das Werk von Judea Pearl und Dana Mackenzie “The Book of Why!” muss an dieser Stelle sein. – Aber da wir keine kausale Modellierung machen wollen, müssen wir das Problem sinngetreu umformulieren:\n“Es besteht ein Zusammenhang zwischen ‘schönem Wetter’ und dem ‘Iphone tagsüber unbeachtet’ lassen.”\nWarum diese neue Formulierung? – Nun, in der orginal Forschungsthese wird ein kausal Zusammenhang geprüft. Da es sich vermutlich um eine Beobachtungstudie handelt können wir einen solchen Ursache-Wirkungs-Zusammenhang aber hier nicht so einfach prüfen. Wie das gehen könnte, dazu schaut man mal bei J.Pearl und D.Mackenzie (s.o.) nach. Zwar kann man von außen sagen: “Wenn es einen Zusammenhang gibt, dann führt das schöne Wetter zur Nichtbeachtung.” mit klassischer Statistik können wir hier aber nur den Zusammenhang (und zwar ungerichtet!) testen. Liegt dieser nicht vor, so spricht erstmal auch nichts für einen kausalen Zusammenhang, aber ein Zusammenhang an sich spricht noch nicht für einen kausalen Zusammenhang! (Korrelation ist ebeb nicht Kausalität!)\nAus der umformulierten Forschungsfrage können wir die Alternativ- und auch die Nullhypothese ableiten:\nAlternativhypothese: Es besteht ein Zusammenhang zwischen ‘schönem Wetter’ und dem ‘Iphone tagsüber unbeachtet’ lassen.\nNullhypothese: Es besteht kein Zusammenhang zwischen ‘schönem Wetter’ und dem ‘Iphone tagsüber unbeachtet’ lassen.\n Wie kann man nun den Zusammenhang messen und wie sieht kein Zusammenhang dabei aus? Um zu sehen ob unsere Werte keinen Zusammenhang haben, also rein zufällig sind, oder es einen inneren Zusammenhang gibt müssen wir die äußeren von den inneren Häufigkeiten trennen.\nKonkret heißt das, wir schauen uns an wie die Häufigkeiten oder auch Verteilung der einzelnen Variabeln ausssehen:\ntally(~ wetter.ist.gut, data = daten) ## wetter.ist.gut ## Ja Nein ## 90 86 tally(~ iphones.tagsüber.unbeachtet, data = daten) ## iphones.tagsüber.unbeachtet ## 1xtäglich 2xtäglich 3xtäglich ## 62 60 54 Freiheitsgrade Die Werte innerhalb der Kreuztabelle oben werden im wesendlichen durch diese Werte bestimmt. Die außeren Werte sind also unsere Rahmenbedingungen. Dabei ist der Einfluss der sogenannten Randhäufigkeiten (Marginale Häufigkeit) nicht zu unterschätzen. Denn wenn wir diese als fix/gegeben ansehen, können wir nur mit den sechs Werten in der Mitte unserer Kreuztabelle spielen.\nDoch sind nicht alle sechs Werte wirklich frei wählbar. Denn um zum Beispiel die Summe 62 in der ersten Zeile zu erhalten haben wir ja die Summe von 29 und 33 gebildet.\nIst nun der Rand, also 62, fest, so kann ich nicht beide Summanden frei wählen, denn\n\\[62 = 29 + 33\\]\nimpliziert ja, dass allgemein\n\\[62 = x + y\\] gelten muss und somit durch\n\\[x = 62 - y \\qquad\\text{ bzw. }\\qquad y = 62 - x\\] immer maximal eine der Variabeln – \\(x\\) oder \\(y\\) – wirklich frei wählen kann.\nDa dies für jede Zeile, aber auch für jede Spalte gilt, denn z.B. ist die Summe der ersten Spalte gegeben durch\n\\[90 = 29 + 34 + 27,\\] sind von den sechs Werten in der Kreuztabelle in der Tat nur 2 Werte wirklich frei zu wählen. Wir haben also ein Problem mit 2 Freiheitsgraden, man schreibt das kurz mit \\(df=2\\) (df steht dabei für degree of freedom).\n Unabhängigkeit in der Statistik Wir sagen, in der Statistik, dass ein gemeinsames Ereignis unabhängig ist wenn sich das Ereignis als Produkt der beiden Einzelereignisse berechnen lässt. Seien \\(A\\) und \\(B\\) also zwei Ereignisse, dann gilt im Falle der Unabhängigkeit:\n\\[P(A \\cap B) = P(A) \\cdot P(B)\\] Oder etwas informeller: Die Wahrscheinlichkeit das beide Ereignisse eintreffen ist das Produkt der Wahrscheinlichkeiten, dass jeweils eines der beiden Ereignisse eintrifft.\nWir können diese Definition aus der Wahrscheinlichkeitstheorie an unser Problem adaptieren, in dem wir die Wahrscheinlichkeiten durch die relativen Häufigkeiten ersetzen.\nDer Wert für das gemeinsame Ereignis iphone.tagsüber.unbeachtet = 1xtäglich und das wetter.ist.gut=ja wird im Falle der Unabhägigkeit durch die beiden Randhäufigkeiten bestimmt:\n\\[62 \\cdot 90 = 31.7045455\\]\nWir können nun mit eine Kreuztabelle erstellen, wie sie seien müsste, falls wir tatsächlich statitische Unabhängigkeit hätten. Wir nutzen dafür eine sehr allgemein gehaltene, aber selbst programmierte, Funktion expectation.tab(), der wir eine Tabelle mit den Häufigkeiten der Beobachtungen geben und die uns dann die Tabelle liefert, wie sie aussehen würde, falls tatsächlich statitische Unabhängigkeit herrschen würde.\nDie Tabelle mit den beobchteten Werten speichern wir in obs.tab, die der erwarteten Werte in exp.tab:\nexpectation.tab \u0026lt;- function(tab.obs) { ret \u0026lt;- tab.obs max.i \u0026lt;- dim(tab.obs)[1] max.j \u0026lt;- dim(tab.obs)[2] # Randhäufigkeiten x \u0026lt;- rep(0, max.i) for (i in 0:max.i) x[i] = sum(tab.obs[i,]) y \u0026lt;- rep(0, max.j) for (j in 0:max.j) y[j] = sum(tab.obs[,j]) # Anzahl aller Beobachtungen n = sum(tab.obs) for (i in 0:max.i) { for (j in 0:max.j) { ret[i,j] \u0026lt;- (x[i] * y[j] / n) } } ret } # Kreuztabelle der beobachtete Werte obs.tab \u0026lt;- tally(iphones.tagsüber.unbeachtet ~ wetter.ist.gut, data = daten) # Kreuztabelle der erwarteten Werte auf Grundlage der beobachteten Werte exp.tab \u0026lt;- expectation.tab(obs.tab) Schauen wir uns die beiden Tabellen kurz an. Zuerst die der beobachteten Werte:\nobs.tab ## wetter.ist.gut ## iphones.tagsüber.unbeachtet Ja Nein ## 1xtäglich 29 33 ## 2xtäglich 34 26 ## 3xtäglich 27 27 Dann die der erwarteten Werte:\nexp.tab ## wetter.ist.gut ## iphones.tagsüber.unbeachtet Ja Nein ## 1xtäglich 31.70455 30.29545 ## 2xtäglich 30.68182 29.31818 ## 3xtäglich 27.61364 26.38636  Was können wir nun messen? Unsicherheit und Zufall spielen eine große Rolle. Wir können also nicht erwarten, dass die Werte für die Kreuztabelle in der Realität genau getroffen werden. (Vorallem, weil wir hier ja mit Nachkommastellen arbeiten!) Aber wir können versuchen den Abstand zu diesen Werten zu messen. Je weiter weg die Werte in der Kreuztabelle von den theoretischen Werten liegen, um so unwarscheinlicher ist es, dass die Werte zufällig aus einer unabhängigen Population gezogen wurden. D.h. wir könnten uns für eine Abhägigkeit aussprechen.\nMessen mit dem Absolutabstand? Man könnte nun auf die Idee kommen die Abstände an jeder Stelle zu messen und den absoluten Abstand zu summieren:\nsum(abs(obs.tab - exp.tab)) ## [1] 13.27273 Nur was sagt dieser Wert aus? – Ist das ein kleiner Abstand oder ein großer?\nWir brauchen Referenzwerte zur Orientierung. Eine Idee lautet: Permutationstest\nSind die Werte unabhängig von einander, dann spielt die konkrete Zuordnung keine Rolle, sondern nur die Anzahl der Ereignisse an sich. Ordnen wir nun zufällig einem iphones.tagsüber.unbeachtet-Wert einen beliebigen wetter.ist.gut-Wert zu, dann besteht kein Zusammenhang mehr zwischen den Werten. Dies machen wir mittels iphones.tagsüber.unbeachtet ~ shuffle(wetter.ist.gut).\nWir simulieren so den Zustand, dass es keine Abhängigkeit zwischen den Werten gibt.\nDabei messen wir den Abstand zwischen den Abstand zwischen den beobachteten Werten und den Werten, die wir erwarten würden, falls Unabhägigkeit vorliegen würde. Dafür nutzen wir die selbsterstellte Funktioen diffabsobsexp, welche die Summe der absoluten Abweichungen berechnet:\n\\[\\text{diffabsobsexp}(obs, exp) = \\sum\\limits_i \\left|obs_i - exp_i\\right|\\]\nWir Wiederholen das ganze mit Hilfe von do(loops) genau loops\\(=10^{4}\\) mal, geben dann das Histogramm aus und tragen als rote Linie den Wert ein, die wir bei unseren beobachteten Daten tatsächliche gemessen haben:\n# Funktion zur Berechnung der absoluten Differenz zwischen # beobachteten und erwarteten Werte diffabsobsexp \u0026lt;- function(obs, exp) { sum(abs(obs - exp)) } # Absolute Abweichung der gemessenen Werte obs.abs \u0026lt;- diffabsobsexp(obs.tab, exp.tab) # Erzeugen der Nullverteilung NullVert \u0026lt;- do(loops) * diffabsobsexp(tally(iphones.tagsüber.unbeachtet ~ shuffle(wetter.ist.gut), data = daten), exp.tab) gf_histogram(~ diffabsobsexp, data = NullVert) %\u0026gt;% gf_vline(xintercept = ~ obs.abs, color = \u0026quot;red\u0026quot;) Wir können nun den p-Wert, also die relative Fläche rechts von der roten Linie in unseren Histogramm, abschätzen mit:\nprop( ~ diffabsobsexp \u0026gt;= obs.abs, data = NullVert) ## prop_TRUE ## 0.5714 Absolute Abweichungen (oder auch absolute Fehler) haben die Tendenz bei großen Zahlen auch große Abweichungswerte zu liefern und bei kleinen Werten eher kleine Abweichungswerte. Das kann man als Markel ansehen. Daher arbeitet man vielleicht lieber mit relativen Abweichungen (oder auch relativen Fehlern). Dabei setzt man die absolute Abweichung jedesmal in Bezug auf den erwarteten Wert. Die dazu passenden Funktion haben wir unten mit diffabsobsexprel implementiert. Dabei ist:\n\\[\\text{diffabsobsexprel}(obs, exp) = \\sum\\limits_i \\frac{\\left|obs_i - exp_i\\right|}{exp_i}\\]\nWir Wiederholen das ganze mit Hilfe von do(loops) genau loops\\(=10^{4}\\) mal, geben dann das Histogramm aus und tragen als rote Linie den Wert ein, den wir bei unseren beobachteten Daten tatsächliche gemessen haben:\n# Funktion zur Berechnung der korrigierten absoluten # Differenz zwischen beobachteten und erwarteten Werten diffabsobsexprel \u0026lt;- function(obs, exp) { sum((abs(obs - exp))/exp) } # Absolute Abweichung der gemessenen Werte -- korrigiert obs.abs \u0026lt;- diffabsobsexprel(obs.tab, exp.tab) # Erzeugen der Nullverteilung NullVert \u0026lt;- do(loops) * diffabsobsexprel(tally(iphones.tagsüber.unbeachtet ~ shuffle(wetter.ist.gut), data = daten), exp.tab) gf_dhistogram(~ diffabsobsexprel, data = NullVert) %\u0026gt;% gf_vline(xintercept = ~ obs.abs, color = \u0026quot;red\u0026quot;) Auch hier können wir den p-Wert abschätzen:\nprop( ~ diffabsobsexprel \u0026gt;= obs.abs, data = NullVert) ## prop_TRUE ## 0.5983 Ist der absolute Abstand überhaupt gut gewählt? – Wäre nicht eher der quadratische Abstand angebracht?\nEin Vorteil des quadratischen Abstand ist es, dass er kleine Abstände kleiner und große Abstände größer bewertet, als der absolute Abstand. Außerdem hat er mathematisch einige Vorteile. Wir messen nun den quadratischen Abstande mit der Funktion diffquad, die wie folgt arbeitet:\n\\[\\text{diffquad}(obs, exp) = \\sum\\limits_i \\left(obs_i - exp_i\\right)^2\\]\nWir Wiederholen dies nun mit Hilfe von do(loops) genau loops\\(=10^{4}\\) mal, geben dann das Histogramm aus und tragen als rote Linie den Wert ein, den wir bei unseren beobachteten Daten tatsächliche gemessen haben:\n# Funktion zur Berechnung der quadratischen # Differenz zwischen beobachteten und erwarteten Werten diffquad \u0026lt;- function(obs, exp) { sum((obs - exp)^2) } # Quadratische Abweichung der gemessenen Werte obs.abs \u0026lt;- diffquad(obs.tab, exp.tab) # Erzeugen der Nullverteilung NullVert \u0026lt;- do(loops) * diffquad(tally(iphones.tagsüber.unbeachtet ~ shuffle(wetter.ist.gut), data = daten), exp.tab) gf_dhistogram(~ diffquad, data = NullVert) %\u0026gt;% gf_vline(xintercept = ~ obs.abs, color = \u0026quot;red\u0026quot;) Wir können nun den p-Wert abschätzen mit:\nprop( ~ diffquad \u0026gt;= obs.abs, data = NullVert) ## prop_TRUE ## 0.5389 Wie beim absoluten Abstand werden hier die Größe der Werte ausser acht gelassen und vielleicht fühlen wir uns etwas wohler, wenn wir statt des quadratischen Abstands den relativen quadratischen Abstand benutzen:\n\\[\\text{diffquadrel}(obs, exp) = \\sum\\limits_i \\frac{\\left(obs_i - exp_i\\right)^2}{exp_i}\\]\nDies wiederholen wir nun mit Hilfe von do(loops) genau loops\\(=10^{4}\\) mal, geben dann das Histogramm aus und tragen als rote Linie den Wert ein, den wir bei unseren beobachteten Daten tatsächliche gemessen haben:\n# Funktion zur Berechnung der korrigierten quadratischen # Differenz zwischen beobachteten und erwarteten Werten diffquadrel \u0026lt;- function(obs, exp) { sum(((obs - exp)^2)/exp) } # Quadratische Abweichung der gemessenen Werte -- korrigiert obs.abs \u0026lt;- diffquadrel(obs.tab, exp.tab) # Erzeugen der Nullverteilung NullVert \u0026lt;- do(loops) * diffquadrel(tally(iphones.tagsüber.unbeachtet ~ shuffle(wetter.ist.gut), data = daten), exp.tab) gf_histogram(~ diffquadrel, binwidth = 0.5, center = 0.25, data = NullVert) %\u0026gt;% gf_vline(xintercept = ~ obs.abs, color = \u0026quot;red\u0026quot;) Den Wert 1.2344597, den wir mit Hilfe der relativen quadratischen Abweichung berechnet haben, nennen wir auch \\(\\chi^2\\) Wert.\nWir können nun den p-Wert abschätzen mit:\nprop( ~ diffquadrel \u0026gt;= obs.abs, data = NullVert) ## prop_TRUE ## 0.5599 An Hand der p-Werte können wir nun über die Nullhypothese entscheiden:\n  Was sagt die klassische Statistik? In der klassischen Statistik könnte man hier den \\(\\chi^2\\)-Unabhängigkeitstest anwenden:\nxchisq.test(iphones.tagsüber.unbeachtet ~ wetter.ist.gut, data = daten) ## ## Pearson\u0026#39;s Chi-squared test ## ## data: x ## X-squared = 1.2345, df = 2, p-value = 0.5394 ## ## 29 33 ## (31.70) (30.30) ## [0.231] [0.241] ## \u0026lt;-0.48\u0026gt; \u0026lt; 0.49\u0026gt; ## ## 34 26 ## (30.68) (29.32) ## [0.359] [0.376] ## \u0026lt; 0.60\u0026gt; \u0026lt;-0.61\u0026gt; ## ## 27 27 ## (27.61) (26.39) ## [0.014] [0.014] ## \u0026lt;-0.12\u0026gt; \u0026lt; 0.12\u0026gt; ## ## key: ## observed ## (expected) ## [contribution to X-squared] ## \u0026lt;Pearson residual\u0026gt; Vergleichen wir nun die beiden Ansätze, SBI auf der einen und der klassische Ansatz auf der anderern Seite, einmal in einem Diagramm. Das (Dichte-)Histogramm sind die Daten aus der Nullverteilung für die quadratische, korrigierte Differenz. Die rote Linie ist der gemessene Abweichungswert. Die schwarze Linie ist der Graph der \\(\\chi^2\\)-Verteilung mit zwei Freiheitsgraden:\ngf_dhistogram(~ diffquadrel, binwidth = 0.5, center = 0.25, data = NullVert) %\u0026gt;% gf_fun(dchisq(x, df=2) ~ x, xlim = c(0:20), color = \u0026quot;blue\u0026quot;) %\u0026gt;% gf_vline(xintercept = ~ obs.abs, color = \u0026quot;red\u0026quot;) Aber es gibt auch den (exakten) Fisher-Test:\nfisher.test(obs.tab, alternative = \u0026quot;greater\u0026quot;) ## ## Fisher\u0026#39;s Exact Test for Count Data ## ## data: obs.tab ## p-value = 0.5609 ## alternative hypothesis: greater  Fazit Wir können die p-Werte der einzelnen Tests nun gegenüber stellen:\n## Scale for \u0026#39;y\u0026#39; is already present. Adding another scale for \u0026#39;y\u0026#39;, which will ## replace the existing scale. Gewöhnlich haben wir ein Signifikanznivau von \\(5\\% = 0{,}05\\) angenommen. Die rote Linie zeigt diese Grenze. Liegt der Balken links vor dieser Linie, so sprechen wir davon, dass der gemessene Wert selten bei unabhänigen Daten vorliegt und würden uns gegen die Nullhypothese und damit quasi für die Alternativhypothese entscheiden. Liegt der Balken recht der roten Linie, so haben wir übliche Werte für unabhängige Daten und keinen Grund gefunden, der gegen die Nullhypothese spricht. Warum wir sie dann, auf Grundlage unserer Daten, auch nicht ablehnen können.\nBleibt Sie Frage, gibt es Situationen in denen die Entscheidung über die Nullhypothese bei den einzelen betrachteten Verfahren unterschiedlichen ist? Und wenn ja, wann und wieoft?\nDiese Fragen sind nicht Thema dieses Beitrags, aber vielleicht habe ich Zeit und betrachte das später einmal.\n  ","date":1581465600,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1582879242,"objectID":"8cd3c575da52f71bb60472b4bb1c56cc","permalink":"https://sefiroth.net/nab/post/eine-typische-frage-von-studierenden/","publishdate":"2020-02-12T00:00:00Z","relpermalink":"/nab/post/eine-typische-frage-von-studierenden/","section":"post","summary":"Vor kurzem fand ich mal wieder eine Anfrage einer Studierenden in meinem Email Postfach. Die Frage lautete in etwa wie folgt:\n Guten Tag Herr Markgraf,\nich würde gerne die Hypothese untersuchen: Die reduzierte Abhängigkeit des Iphones tagsüber liegt am schönen Wetter.","tags":["Lehre","R","Statistik","Nullhypothese","Nullhypothesentest","SBI","Klassische Inferenz","Chi-Quadrat","Chi-Quadrat-Test","Unabhängigkeit","Unabhängigkeitstest","Fisher-Test","Permutationstest"],"title":"Eine typische Frage von Studierenden","type":"post"},{"authors":[],"categories":["Statistisches"],"content":"  Bei der Analyse von Zusammenhängen tauchen sowohl Moderation als auch Mediation auf. Es geht um Zusammenhänge zwischen drei Variablen \\(X\\), \\(Y\\) und \\(M\\). Untersucht wird der Effekt einer unabhägigen Variable \\(X\\) (Prädiktor, UV) auf ein abhängige Variable \\(Y\\) (AV). Wir untersuchen dies mit einem Regressionsmodell \\(Y \\sim X\\). Dabei wird zusätzlich eine dritte Variable \\(M\\) berücksichtigt, die man entweder der Moderator oder Mediator nennt.\nIst die abhängige Variable metrisch, so können wir mittels eine linearer Regression vorgehen, ist die AB dagegen dichotom, so nutzen wir eine logistische Regression.\nModeration Bei einer Moderation wirkt die dritte Variable \\(M\\) (Moderator) auf die Beziehung zwischen \\(X\\) und \\(Y\\).\nDer Einfluss von \\(M\\) ändert also den Effekt von \\(X\\) auf \\(Y\\). Der Zusammenhang zwischen \\(Y\\) und \\(X\\) ist also je nach \\(M\\) unterschiedlich.\nStatistisch gesehen liegt eine Interaktion zwischen \\(M\\) und \\(X\\) vor.\nWie untersucht man einen Zusammenhang auf eine Moderation? Dazu stellen wir ein Regressionsmodell mit den drei Faktoren \\(X\\), \\(M\\) und der Interaktion zwischen \\(X\\) und \\(M\\) auf.\nlm(Y ~ X * M, data=daten) Oder alternativ:\nlm(Y ~ X + M + M:X, data=daten) Diese drei Faktoren wirken auf \\(Y\\). Ist in diesem Modell die Interaktion \\(M:X\\) signifikant, so liegt eine (signifikante) Moderation vor.\n  Mediation Bei der Mediation steht die Variable \\(M\\) (der Mediator) sowohl zu \\(X\\) als auch zu \\(Y\\) in Beziehung. Der direkte Effekt zwischen \\(X\\) und \\(Y\\) wird durch den indirekten Effekt über \\(M\\) erklärt, also durch \\(X \\to M \\to Y\\).\nWie untersucht man auf eine Mediation? In diesem Fall stellen wir mehrere Regressionsmodelle auf. Eine (signifikante) Mediation liegt dann vor, wenn die folgenden Bedinungen erfüllt sind:\nerstesModell \u0026lt;- lm(Y ~ X, data=daten) zweitesModell \u0026lt;- lm(M ~ X, data=daten) drittesModell \u0026lt;- lm(Y ~ X + M, data=daten) Im ersten Modell (\\(X \\to Y\\)) ist der Regressionskoeffizient von \\(X\\) signifikant.\n Im zweiten Modell (\\(X \\to M\\)) ist der Regressionskoeffizient von \\(X\\) signifikant.\n Im dritten Modell (\\(X,M \\to Y\\)) ist der Regressionskoeffizient von \\(M\\) signifikant und\n der Regressionskoeffizient von \\(X\\) im dritten Modell kleiner als im ersten Modell.\n    ","date":1577750400,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1577795625,"objectID":"c4e2b6d3899edee451df21b607176158","permalink":"https://sefiroth.net/nab/post/moderator-und-mediation-formen-der-interaktion-bei-analyse-von-zusammenhaengen/","publishdate":"2019-12-31T00:00:00Z","relpermalink":"/nab/post/moderator-und-mediation-formen-der-interaktion-bei-analyse-von-zusammenhaengen/","section":"post","summary":"Bei der Analyse von Zusammenhängen tauchen sowohl Moderation als auch Mediation auf. Es geht um Zusammenhänge zwischen drei Variablen \\(X\\), \\(Y\\) und \\(M\\). Untersucht wird der Effekt einer unabhägigen Variable \\(X\\) (Prädiktor, UV) auf ein abhängige Variable \\(Y\\) (AV).","tags":["Statistik","Moderator","Mediator","Moderatorenanalyse","Mediatorenanalyse","Interaktion","Zusammemhangsanalyse","Regression","Regressionsanalyse"],"title":"Moderator und Mediation - Formen der Interaktion bei Analyse von Zusammenhängen","type":"post"},{"authors":[],"categories":["Statistisches"],"content":"  Prognosen sind ein wichtiger Bestandteil von Data Science und ist durchaus nicht nur auf moderne Ansätze, wie Neuronale Netze, deep lerning etc. begrenzt. Auch die gute, alte Regression kann ein sehr sinnvolles Mittel sein solche Prognosen zu erstellen.\nUm ein wenig die Ideen hinter Prognosen zu beleuchten wollen wir uns an Prognosen mit dem tipping-Daten heranwagen.\nEinlesen der tipping-Daten Zuerst laden wir die notwenidgen Pakete:\nlibrary(mosaic) Falls die tipping-Daten noch nicht im Verzeichnis liegen, laden wir sie aus dem Internet nach:\nif (!file.exists(\u0026quot;tips.csv\u0026quot;)) { download.file(\u0026quot;https://goo.gl/whKjnl\u0026quot;, destfile = \u0026quot;tips.csv\u0026quot;) } Nun laden wir die tipping-Daten in den Speicher in den Datenrahmen tips:\ntips \u0026lt;- read.csv2(\u0026quot;tips.csv\u0026quot;) Wir werfen einen ersten Blick auf die tipping-Daten:\ninspect(tips) ## ## categorical variables: ## name class levels n missing ## 1 sex character 2 244 0 ## 2 smoker character 2 244 0 ## 3 day character 4 244 0 ## 4 time character 2 244 0 ## distribution ## 1 Male (64.3%), Female (35.7%) ## 2 No (61.9%), Yes (38.1%) ## 3 Sat (35.7%), Sun (31.1%), Thur (25.4%) ... ## 4 Dinner (72.1%), Lunch (27.9%) ## ## quantitative variables: ## name class min Q1 median Q3 max mean sd ## ...1 total_bill numeric 3.07 13.3475 17.795 24.1275 50.81 19.785943 8.9024120 ## ...2 tip numeric 1.00 2.0000 2.900 3.5625 10.00 2.998279 1.3836382 ## ...3 size integer 1.00 2.0000 2.000 3.0000 6.00 2.569672 0.9510998 ## n missing ## ...1 244 0 ## ...2 244 0 ## ...3 244 0  Vorbereiten der Test-/Trainings- und Auswertungesdaten Zunächst schränken wir die tipping-Daten auf die Variabeln “total_bill”, “sex”, “smoker”, “day”, “time”, “size” ein und speichern das Ergebnis wieder in tips:\ntips %\u0026gt;% select(c(\u0026quot;total_bill\u0026quot;, \u0026quot;sex\u0026quot;, \u0026quot;smoker\u0026quot;, \u0026quot;day\u0026quot;, \u0026quot;time\u0026quot;, \u0026quot;size\u0026quot;)) -\u0026gt; tips Ziel ist es, den Rechnungsbetrag (“total_bill”) auf Grundlage der Variabeln “sex”, “smoker”, “day”, “time” und/oder “size” vorherzusagen.\nWir teilen den tipping-Datensatz auf in eine Trainingsdatensatz (“tipstrain”), einem Testdatensatz (“tipstest”) und einem Prüfdatensatz (“tipspruef”). Der Trainingsdatensatz sollte rund zweidrittel der Daten die wir haben umfassen. Der Testdatensatz die restlich ca. eindrittel.\ntrainings_anteil = 2/3 # n.train ist ein Index für alle Werte, # die wir im Trainingsdatensatz haben wollen: x.train \u0026lt;- sample(1:nrow(tips), floor(trainings_anteil*nrow(tips))) # Trainingsdatensatz erstellen: tipstrain \u0026lt;- slice(tips, x.train) # Prüfdatensatz erstellen, also alles was # nicht in den Trainingsdatensatz gekommen ist: tipspruef \u0026lt;- slice(tips, -(x.train)) # Der Testdatensatz ist der Prüfdatensatz # ohne die Variable total_bill: tipspruef %\u0026gt;% select(-total_bill) -\u0026gt; tipstest Mit dem Tainingsdatensatz versuchen wir nun ein Prognosemodell zu erstellen, um aus den Testdatensatz eine Prognose für “total_bill” zu erstellen.\nDas Prognose-Modell wird ausschließlich auf Grundlage des Trainingsdatensatzes erstellt. Am Ende wollen wir unser Modell dann aber mit Hilfe des Prüfdatensatzes bewertet.\n Die Datenlage Ein (paar) Blick(e) auf unsere Trainingsdaten:\ngf_point(total_bill ~ jitter(size), color=~time, data=tipstrain) gf_point(total_bill ~ day | time, color = ~ sex, data=tipstrain)  Prognosemodel: Nullmodell Aufstellen des Nullmodel aka Regression mit der Achse Wir erstellen das Nullmodell wie folgt:\nlm.null \u0026lt;- lm( total_bill ~ 1, data=tipstrain) summary(lm.null) ## ## Call: ## lm(formula = total_bill ~ 1, data = tipstrain) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.159 -6.989 -2.429 4.171 30.401 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 20.4086 0.7311 27.91 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 9.306 on 161 degrees of freedom Das Nullmodell sagt in jedem Fall den Rechnungsbetrag vorher als den Mittelwert der Trainingsdaten!\nmean(~ total_bill, data=tipstrain) ## [1] 20.40864 Nun bestimmten wir mit Hilfe des Nullmodells “lm.null” eine Vorhersage für die Testdaten:\npredict.null \u0026lt;- predict(lm.null, newdata=tipstest) head(predict.null) ## 1 2 3 4 5 6 ## 20.40864 20.40864 20.40864 20.40864 20.40864 20.40864 Wie gesagt, das Nullmodell liefert als Prognose immer den Mittelwert der Trainingsdaten zurück, das mathematische Nullmodell lautet also:\n\\[\\widehat{total\\_bill_i} = 20.408642 \\]\n Auswertung des Nullmodells Zur Auswertung Nutzen wir den mittleren Absolutabstand zwischen der Vorhersage und den Prüfdaten:\nmaa.null \u0026lt;-sum( abs( tipspruef$total_bill - predict.null)) maa.null ## [1] 544.772   Prognosemodell: Lineare Regression gegen “size” als metrischer Wert Aufstellen des Modells  Auswertung des Regressionsmodell   ","date":1576627200,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1576670579,"objectID":"95cf43c1cb55120f67197e8b62ffce03","permalink":"https://sefiroth.net/nab/post/die-ersten-schritte-zur-prognose-mitteles-linearer-regression/","publishdate":"2019-12-18T00:00:00Z","relpermalink":"/nab/post/die-ersten-schritte-zur-prognose-mitteles-linearer-regression/","section":"post","summary":"An hand der **tipping**-Daten werden die ersten Schritte zur Prognose von Daten gezeigt.","tags":["R","Lehre","Statistik"],"title":"Die ersten Schritte zur Prognose mitteles linearer Regression","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://sefiroth.net/nab/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/nab/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":[],"content":"Es sind oft die Dinge die man nicht sucht die einem weiterhelfen. So habe ich auch nach etwas ganz anderes gesucht und bin dabei über ein nettes kleines Werk gestoplert, dass einem sagt, wie man in Python Dinge besser nicht macht.\nUnter https://docs.quantifiedcode.com/python-anti-patterns/index.html gibt es das The Little Book of Python Anti-Patterns.\nMit manchen Dingen kann ich mich spontan nicht unbedingt anfreunden. Aber im Prinzip eine sehr sinnvolle und gute Sammlung von Dingen, die man oft falsch macht und die man durch aus besser machen könnte.\n","date":1546646400,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1546646400,"objectID":"7ce9b3e4281d393bcea08abbcac8dca4","permalink":"https://sefiroth.net/nab/post/wie-man-es-nicht-machen-sollte-in-python/","publishdate":"2019-01-05T00:00:00Z","relpermalink":"/nab/post/wie-man-es-nicht-machen-sollte-in-python/","section":"post","summary":"Es sind oft die Dinge die man nicht sucht die einem weiterhelfen. So habe ich auch nach etwas ganz anderes gesucht und bin dabei über ein nettes kleines Werk gestoplert, dass einem sagt, wie man in Python Dinge besser nicht macht.","tags":["Python"],"title":"Wie man es nicht machen sollte in Python!","type":"post"},{"authors":null,"categories":["Fundstücke"],"content":"Wenn man zu Weihnachten einen kleinen neuen Freund bekommt, so wie ich einen Raspberry Pi Zero WH, dann möchte man ihn auch mit der aktuellen Version eines Betriebssystems austatten. Als macOS Nutzer gibt es dafür viele Möglichkeiten. Hier meine Schritte bei der Installation, damit ich mich daran später erinnere\nVorbereiten der SD Karte Die SD(HC) Karte sollte auf FAT32 formatiert werden. Das geht unter macOS sehr schön mit dem Festplattenmanager!\nLaden der aktuellen Version von Rasbian Die aktuellen Versionen findet man unter https://www.raspberrypi.org/downloads/raspbian/. Es ginge auch alternativ NOOBS, aber da ich \u0026ldquo;headless\u0026rdquo; installieren möchte, wähle ich Raspbian direkt aus.\nDie ZIP-Datei wird entpackt (Archiveprogramm) und wir befinden uns mit der Konsole in dem Verzeichnis in dem das aktuelle Image liegt.\nZur Zeit dieses Eintrages war das die Datei \u0026ldquo;2018-11-13-raspbian-stretch-full.img\u0026rdquo;\nSpeichern des Images auf die SD Karte Mit Hilfe von\ndiskutil list  ermittelt man die SD Karte (hier \u0026ldquo;/dev/disk5\u0026rdquo;) und mit Hilfe von\ndiskutil unmountdisk /dev/disk5  wird die SD Karte komplett \u0026ldquo;unmounted\u0026rdquo;.\nNun kann man mit\nsudo dd bs=1m if=2018-11-13-raspbian-stretch-full.img of=/dev/disk5 conv=sync  das Image auf die SD Karte gespeichert. Das kann etwas dauern! (Bis zu 30 Minuten!) Mit \u0026ldquo;ctrl-t\u0026rdquo; kann man sich regelmässig über der aktuellen Stand informieren! ;-)\nNetzwerk einrichten Nach dem man das Image auf die SD Karte gespeichert hat wirft man die SD Karte aus und steckt sie danach wieder ein. Damit hat man die SD Karte wieder \u0026ldquo;gemounted\u0026rdquo; und wir finden unter \u0026ldquo;/Volues/boot\u0026rdquo; und Boot-Verzeichnis der SD-Karte.\nIn diesem Verzeichnis erzeigen wir (z.B. mit vi) eine Datei mit dem Namen \u0026ldquo;wpa_supplicant.conf\u0026rdquo; mit dem Inhalt:\ncountry=DE ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\u0026quot;YOUR_NETWORK_NAME\u0026quot; psk=\u0026quot;YOUR_PASSWORD\u0026quot; key_mgmt=WPA-PSK scan_ssid=1 }  Diese Datei wird beim Booten von Rasbpian in das Verzeichnis \u0026ldquo;/etc/wpa_supplicant/\u0026rdquo; verschoben.\nSSH aktivieren! SSH ist in der Regel deaktiviert. Wir können es aber schnell einschalten. Dafür gehen wir wieder in das Verzeichnis \u0026ldquo;/Volumes/boot\u0026rdquo; und führen dort den Befehl\ntouch ssh  aus. Damit wird eine (leere) Datei \u0026ldquo;ssh\u0026rdquo; erzeugt. Findet Raspbain diese Datei, so aktiviert es beim Booten automatisch den SSH-Dämon.\nStarten von der neuen SD Karte Jetzt kann die Karte ausgeworfen werden und in den Raspberry eingelegt werden. Danach kann von der SD Karte gestartet werden.\n","date":1546387200,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1546387200,"objectID":"3e9572e0c51fc9ee7f8307bacdf01645","permalink":"https://sefiroth.net/nab/post/raspbian-auf-dem-raspberry-pi-installieren/","publishdate":"2019-01-02T00:00:00Z","relpermalink":"/nab/post/raspbian-auf-dem-raspberry-pi-installieren/","section":"post","summary":"Wenn man zu Weihnachten einen kleinen neuen Freund bekommt, so wie ich einen Raspberry Pi Zero WH, dann möchte man ihn auch mit der aktuellen Version eines Betriebssystems austatten. Als macOS Nutzer gibt es dafür viele Möglichkeiten.","tags":["Raspberry Pi","Raspbian"],"title":"Raspbian auf dem Raspberry Pi installieren","type":"post"},{"authors":null,"categories":["Allgemeines"],"content":"Hier eine Liste von Software für den Raspberry Pi, die man noch installieren sollte/ kann und wie man es macht:\nMathematica sudo apt-get update sudo apt-get install wolfram-engine  Python3 auf den aktuellen Stand bringen pip3 install --upgrade pip  Python3 und das Paket \u0026ldquo;matplotlib\u0026rdquo; sudo apt-get install python3-matplotlib  ","date":1546387200,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1546387200,"objectID":"6f040a8afd07a288b0d1849f0d9024f9","permalink":"https://sefiroth.net/nab/post/weitere-software-auf-den-raspberry-pi-installieren/","publishdate":"2019-01-02T00:00:00Z","relpermalink":"/nab/post/weitere-software-auf-den-raspberry-pi-installieren/","section":"post","summary":"Hier eine Liste von Software für den Raspberry Pi, die man noch installieren sollte/ kann und wie man es macht:\nMathematica sudo apt-get update sudo apt-get install wolfram-engine  Python3 auf den aktuellen Stand bringen pip3 install --upgrade pip  Python3 und das Paket \u0026ldquo;matplotlib\u0026rdquo; sudo apt-get install python3-matplotlib  ","tags":["Raspberry Pi","Raspbian","Mathematica","Python","matplotlib"],"title":"Weitere Software auf den Raspberry PI installieren.","type":"post"},{"authors":null,"categories":["Allgemeines"],"content":"  Klingt ja bedrohlich, aber es ist wirklich Möglich R und Python sinnvoll zu kombinieren. Nicht nur in den Anwendungen, sondern auch beim Erstellen von Skripten mit R markdown.\nZu Beginn des letzten Semesters hatte ich die Idee in der Vorlesung “Mathematischen Grundlagen der Wirtschaftsinformatik” ein paar der Begriffe der Mengenlehre denen daraus abgeleiteten Begriffen der abstrakten Datentypen gegenüberzustellen. So gibt es die Idee der Menge u.a. in Python als set.\nWie aber kann man solche Python-Fragmente in ein R markdown Sktipr einbauen? - Kann man R markdown überhaupt mit Python zusammen bringen? - Ein wenig suchen im Internet und ein paar Stunden später hatte ich es geschaft. Dank einer Netzpython…\nDie Netzpython als Bindeglied zwischen R und Python Eine Netzpython (engl. reticulated python) stand Pate für den Namen des R Paketes reticulate, welches R und Python miteinander verbindet. So ist es möglich Python-Befehle direkt in ein R markdown Skript ausführen zulassen, diese Fragmente adequat durchzustellen – ganz wie R Skripte – und sogar Daten zwoschen R und Python hin und her (aus) zu tauschen.\nNach der Installation mittels\ninstall.packages(\u0026quot;reticulate\u0026quot;) bedarf es aber durch aus noch einiger Anpassungen, bis alles zur Zufriedenheit funktioniert.\nStandardmässig sucht die Netzpython nach ihrem Gefährten mit der Hilfe des Befehls Sys.which(\"python\"), welcher bei mir leider zu einer vollkommen alten, aber noch benutzen, Python Version führte. Möchte man eine ganz bestimmte Python Version haben, so hilft einem der Befehl use_python():\nlibrary(reticulate) use_python(\u0026quot;/usr/local/bin/python\u0026quot;) # Pfad zum Python-Befehl der benutz werden soll. Es werden auch virtuelle Umgebungen unterstützt:\nlibrary(reticulate) use_virtualenv(\u0026quot;myenv\u0026quot;) Und auch eine ganz andere Schlangenart kann benutzt werden, Anacondas:\nlibrary(reticulate) use_condaenv(\u0026quot;mycondaenv\u0026quot;)  Der Einbau in ein R markdown Dokument Einen Python Quellcode in ein R markdown einzubauen ist dann wieder sehr einfach. Man ändert einfach ein r in python im Codeblock und schon steht einem der knitr-Chunk als Python Quelle zur Verfügung.\nSp liefert der knitr-Chunk ```{python} # Etwas Python gefällig? def quadrat(x): return x**2 print(quadrat(2)) ```  in einem R markdown, dann die Ausgabe:\n# Etwas Python geföllig? def quadrat(x): return x**2 print(quadrat(2)) ## 4 Das war es aber noch lange nicht. R und Python können nämlich nicht nur nebeneinander, sondern auch miteinander!\nDazu dann aber mehr in einem späteren Blog-Eintrag.\n ","date":1543795200,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1543795200,"objectID":"128f142baa74ce88d6a32f3eb6c1a5db","permalink":"https://sefiroth.net/nab/post/der-angriff-der-riesenschlangen/","publishdate":"2018-12-03T00:00:00Z","relpermalink":"/nab/post/der-angriff-der-riesenschlangen/","section":"post","summary":"Klingt ja bedrohlich, aber es ist wirklich Möglich R und Python sinnvoll zu kombinieren. Nicht nur in den Anwendungen, sondern auch beim Erstellen von Skripten mit R markdown.","tags":["Python","R","R markdown","Technisches"],"title":"Der Angriff der Riesenschlangen.","type":"post"},{"authors":null,"categories":null,"content":"Personenbezogene Daten (nachfolgend zumeist nur „Daten“ genannt) werden von uns nur im Rahmen der Erforderlichkeit sowie zum Zwecke der Bereitstellung eines funktionsfähigen und nutzerfreundlichen Internetauftritts, inklusive seiner Inhalte und der dort angebotenen Leistungen, verarbeitet. Gemäß Art. 4 Ziffer 1. der Verordnung (EU) 2016/679, also der Datenschutz-Grundverordnung (nachfolgend nur „DSGVO“ genannt), gilt als „Verarbeitung“ jeder mit oder ohne Hilfe automatisierter Verfahren ausgeführter Vorgang oder jede solche Vorgangsreihe im Zusammenhang mit personenbezogenen Daten, wie das Erheben, das Erfassen, die Organisation, das Ordnen, die Speicherung, die Anpassung oder Veränderung, das Auslesen, das Abfragen, die Verwendung, die Offenlegung durch Übermittlung, Verbreitung oder eine andere Form der Bereitstellung, den Abgleich oder die Verknüpfung, die Einschränkung, das Löschen oder die Vernichtung. Mit der nachfolgenden Datenschutzerklärung informieren wir Sie insbesondere über Art, Umfang, Zweck, Dauer und Rechtsgrundlage der Verarbeitung personenbezogener Daten, soweit wir entweder allein oder gemeinsam mit anderen über die Zwecke und Mittel der Verarbeitung entscheiden. Zudem informieren wir Sie nachfolgend über die von uns zu Optimierungszwecken sowie zur Steigerung der Nutzungsqualität eingesetzten Fremdkomponenten, soweit hierdurch Dritte Daten in wiederum eigener Verantwortung verarbeiten.\nUnsere Datenschutzerklärung ist wie folgt gegliedert:\nI. Informationen über uns als Verantwortliche\nII. Rechte der Nutzer und Betroffenen\nIII. Informationen zur Datenverarbeitung\n\u0026hellip;\nI. Informationen über uns als Verantwortliche Verantwortlicher Anbieter dieses Internetauftritts im datenschutzrechtlichen Sinne ist:\nNorman Markgraf\nGuts-Muths-Weg 19\n45136 Essen\nDeutschland\nTelefon: +49-176-20077335\nE-Mail: admin(at)sefiroth.net\nDatenschutzbeauftragte/r beim Anbieter ist:\nNorman Markgraf\nII. Rechte der Nutzer und Betroffenen Mit Blick auf die nachfolgend noch näher beschriebene Datenverarbeitung haben die Nutzer und Betroffenen das Recht auf Bestätigung, ob sie betreffende Daten verarbeitet werden, auf Auskunft über die verarbeiteten Daten, auf weitere Informationen über die Datenverarbeitung sowie auf Kopien der Daten (vgl. auch Art. 15 DSGVO); auf Berichtigung oder Vervollständigung unrichtiger bzw. unvollständiger Daten (vgl. auch Art. 16 DSGVO); auf unverzügliche Löschung der sie betreffenden Daten (vgl. auch Art. 17 DSGVO), oder, alternativ, soweit eine weitere Verarbeitung gemäß Art. 17 Abs. 3 DSGVO erforderlich ist, auf Einschränkung der Verarbeitung nach Maßgabe von Art. 18 DSGVO; auf Erhalt der sie betreffenden und von ihnen bereitgestellten Daten und auf Übermittlung dieser Daten an andere Anbieter/Verantwortliche (vgl. auch Art. 20 DSGVO); auf Beschwerde gegenüber der Aufsichtsbehörde, sofern sie der Ansicht sind, dass die sie betreffenden Daten durch den Anbieter unter Verstoß gegen datenschutzrechtliche Bestimmungen verarbeitet werden (vgl. auch Art. 77 DSGVO). Darüber hinaus ist der Anbieter dazu verpflichtet, alle Empfänger, denen gegenüber Daten durch den Anbieter offengelegt worden sind, über jedwede Berichtigung oder Löschung von Daten oder die Einschränkung der Verarbeitung, die aufgrund der Artikel 16, 17 Abs. 1, 18 DSGVO erfolgt, zu unterrichten. Diese Verpflichtung besteht jedoch nicht, soweit diese Mitteilung unmöglich oder mit einem unverhältnismäßigen Aufwand verbunden ist. Unbeschadet dessen hat der Nutzer ein Recht auf Auskunft über diese Empfänger. Ebenfalls haben die Nutzer und Betroffenen nach Art. 21 DSGVO das Recht auf Widerspruch gegen die künftige Verarbeitung der sie betreffenden Daten, sofern die Daten durch den Anbieter nach Maßgabe von Art. 6 Abs. 1 lit. f) DSGVO verarbeitet werden. Insbesondere ist ein Widerspruch gegen die Datenverarbeitung zum Zwecke der Direktwerbung statthaft.\nIII. Informationen zur Datenverarbeitung Ihre bei Nutzung unseres Internetauftritts verarbeiteten Daten werden gelöscht oder gesperrt, sobald der Zweck der Speicherung entfällt, der Löschung der Daten keine gesetzlichen Aufbewahrungspflichten entgegenstehen und nachfolgend keine anderslautenden Angaben zu einzelnen Verarbeitungsverfahren gemacht werden.\nCookies\na) Sitzungs-Cookies/Session-Cookies\nWir verwenden mit unserem Internetauftritt sog. Cookies. Cookies sind kleine Textdateien oder andere Speichertechnologien, die durch den von Ihnen eingesetzten Internet-Browser auf Ihrem Endgerät ablegt und gespeichert werden. Durch diese Cookies werden im individuellen Umfang bestimmte Informationen von Ihnen, wie beispielsweise Ihre Browser- oder Standortdaten oder Ihre IP-Adresse, verarbeitet. Durch diese Verarbeitung wird unser Internetauftritt benutzerfreundlicher, effektiver und sicherer, da die Verarbeitung bspw. die Wiedergabe unseres Internetauftritts in unterschiedlichen Sprachen oder das Angebot einer Warenkorbfunktion ermöglicht. Rechtsgrundlage dieser Verarbeitung ist Art. 6 Abs. 1 lit b.) DSGVO, sofern diese Cookies Daten zur Vertragsanbahnung oder Vertragsabwicklung verarbeitet werden. Falls die Verarbeitung nicht der Vertragsanbahnung oder Vertragsabwicklung dient, liegt unser berechtigtes Interesse in der Verbesserung der Funktionalität unseres Internetauftritts. Rechtsgrundlage ist in dann Art. 6 Abs. 1 lit. f) DSGVO. Mit Schließen Ihres Internet-Browsers werden diese Session-Cookies gelöscht.\nb) Drittanbieter-Cookies\nGegebenenfalls werden mit unserem Internetauftritt auch Cookies von Partnerunternehmen, mit denen wir zum Zwecke der Werbung, der Analyse oder der Funktionalitäten unseres Internetauftritts zusammenarbeiten, verwendet. Die Einzelheiten hierzu, insbesondere zu den Zwecken und den Rechtsgrundlagen der Verarbeitung solcher Drittanbieter-Cookies, entnehmen Sie bitte den nachfolgenden Informationen.\nc) Beseitigungsmöglichkeit\nSie können die Installation der Cookies durch eine Einstellung Ihres Internet-Browsers verhindern oder einschränken. Ebenfalls können Sie bereits gespeicherte Cookies jederzeit löschen. Die hierfür erforderlichen Schritte und Maßnahmen hängen jedoch von Ihrem konkret genutzten Internet-Browser ab. Bei Fragen benutzen Sie daher bitte die Hilfefunktion oder Dokumentation Ihres Internet-Browsers oder wenden sich an dessen Hersteller bzw. Support. Bei sog. Flash-Cookies kann die Verarbeitung allerdings nicht über die Einstellungen des Browsers unterbunden werden. Stattdessen müssen Sie insoweit die Einstellung Ihres Flash-Players ändern. Auch die hierfür erforderlichen Schritte und Maßnahmen hängen von Ihrem konkret genutzten Flash-Player ab. Bei Fragen benutzen Sie daher bitte ebenso die Hilfefunktion oder Dokumentation Ihres Flash-Players oder wenden sich an den Hersteller bzw. Benutzer-Support. Sollten Sie die Installation der Cookies verhindern oder einschränken, kann dies allerdings dazu führen, dass nicht sämtliche Funktionen unseres Internetauftritts vollumfänglich nutzbar sind.\nGoogle Analytics\nIn unserem Internetauftritt setzen wir Google Analytics ein. Hierbei handelt es sich um einen Webanalysedienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI\u0026amp;status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Der Dienst Google Analytics dient zur Analyse des Nutzungsverhaltens unseres Internetauftritts. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Analyse, Optimierung und dem wirtschaftlichen Betrieb unseres Internetauftritts. Nutzungs- und nutzerbezogene Informationen, wie bspw. IP-Adresse, Ort, Zeit oder Häufigkeit des Besuchs unseres Internetauftritts, werden dabei an einen Server von Google in den USA übertragen und dort gespeichert. Allerdings nutzen wir Google Analytics mit der sog. Anonymisierungsfunktion. Durch diese Funktion kürzt Google die IP-Adresse schon innerhalb der EU bzw. des EWR. Die so erhobenen Daten werden wiederum von Google genutzt, um uns eine Auswertung über den Besuch unseres Internetauftritts sowie über die dortigen Nutzungsaktivitäten zur Verfügung zu stellen. Auch können diese Daten genutzt werden, um weitere Dienstleistungen zu erbringen, die mit der Nutzung unseres Internetauftritts und der Nutzung des Internets zusammenhängen. Google gibt an, Ihre IP-Adresse nicht mit anderen Daten zu verbinden. Zudem hält Google unter https://www.google.com/intl/de/policies/privacy/partners weitere datenschutzrechtliche Informationen für Sie bereit, so bspw. auch zu den Möglichkeiten, die Datennutzung zu unterbinden. Zudem bietet Google unter https://tools.google.com/dlpage/gaoptout?hl=de ein sog. Deaktivierungs-Add-on nebst weiteren Informationen hierzu an. Dieses Add-on lässt sich mit den gängigen Internet-Browsern installieren und bietet Ihnen weitergehende Kontrollmöglichkeit über die Daten, die Google bei Aufruf unseres Internetauftritts erfasst. Dabei teilt das Add-on dem JavaScript (ga.js) von Google Analytics mit, dass Informationen zum Besuch unseres Internetauftritts nicht an Google Analytics übermittelt werden sollen. Dies verhindert aber nicht, dass Informationen an uns oder an andere Webanalysedienste übermittelt werden. Ob und welche weiteren Webanalysedienste von uns eingesetzt werden, erfahren Sie natürlich ebenfalls in dieser Datenschutzerklärung.\nGoogle-Maps\nIn unserem Internetauftritt setzen wir Google Maps zur Darstellung unseres Standorts sowie zur Erstellung einer Anfahrtsbeschreibung ein. Es handelt sich hierbei um einen Dienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI\u0026amp;status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Um die Darstellung bestimmter Schriften in unserem Internetauftritt zu ermöglichen, wird bei Aufruf unseres Internetauftritts eine Verbindung zu dem Google-Server in den USA aufgebaut. Sofern Sie die in unseren Internetauftritt eingebundene Komponente Google Maps aufrufen, speichert Google über Ihren Internet-Browser ein Cookie auf Ihrem Endgerät. Um unseren Standort anzuzeigen und eine Anfahrtsbeschreibung zu erstellen, werden Ihre Nutzereinstellungen und -daten verarbeitet. Hierbei können wir nicht ausschließen, dass Google Server in den USA einsetzt. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Optimierung der Funktionalität unseres Internetauftritts. Durch die so hergestellte Verbindung zu Google kann Google ermitteln, von welcher Website Ihre Anfrage gesendet worden ist und an welche IP-Adresse die Anfahrtsbeschreibung zu übermitteln ist. Sofern Sie mit dieser Verarbeitung nicht einverstanden sind, haben Sie die Möglichkeit, die Installation der Cookies durch die entsprechenden Einstellungen in Ihrem Internet-Browser zu verhindern. Einzelheiten hierzu finden Sie vorstehend unter dem Punkt „Cookies“. Zudem erfolgt die Nutzung von Google Maps sowie der über Google Maps erlangten Informationen nach den Google-Nutzungsbedingungen https://policies.google.com/terms?gl=DE\u0026amp;hl=de und den Geschäftsbedingungen für Google Maps https://www.google.com/intl/de_de/help/terms_maps.html. Überdies bietet Google unter https://adssettings.google.com/authenticated https://policies.google.com/privacy weitergehende Informationen an.\nGoogle reCAPTCHA\nIn unserem Internetauftritt setzen wir Google reCAPTCHA zur Überprüfung und Vermeidung von Interaktionen auf unserer Internetseite durch automatisierte Zugriffe, bspw. durch sog. Bots, ein. Es handelt sich hierbei um einen Dienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI\u0026amp;status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Durch diesen Dienst kann Google ermitteln, von welcher Webseite eine Anfrage gesendet wird sowie von welcher IP-Adresse aus Sie die sog. reCAPTCHA-Eingabebox verwenden. Neben Ihrer IP-Adresse werden womöglich noch weitere Informationen durch Google erfasst, die für das Angebot und die Gewährleistung dieses Dienstes notwendig sind. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Sicherheit unseres Internetauftritts sowie in der Abwehr unerwünschter, automatisierter Zugriffe in Form von Spam o.ä.. Google bietet unter https://policies.google.com/privacy weitergehende Informationen zu dem allgemeinen Umgang mit Ihren Nutzerdaten an.\nGoogle Fonts\nIn unserem Internetauftritt setzen wir Google Fonts zur Darstellung externer Schriftarten ein. Es handelt sich hierbei um einen Dienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI\u0026amp;status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Um die Darstellung bestimmter Schriften in unserem Internetauftritt zu ermöglichen, wird bei Aufruf unseres Internetauftritts eine Verbindung zu dem Google-Server in den USA aufgebaut. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Optimierung und dem wirtschaftlichen Betrieb unseres Internetauftritts. Durch die bei Aufruf unseres Internetauftritts hergestellte Verbindung zu Google kann Google ermitteln, von welcher Website Ihre Anfrage gesendet worden ist und an welche IP-Adresse die Darstellung der Schrift zu übermitteln ist. Google bietet unter https://adssettings.google.com/authenticated https://policies.google.com/privacy weitere Informationen an und zwar insbesondere zu den Möglichkeiten der Unterbindung der Datennutzung.\n„Facebook“-Social-Plug-in\nIn unserem Internetauftritt setzen wir das Plug-in des Social-Networks Facebook ein. Bei Facebook handelt es sich um einen Internetservice der facebook Inc., 1601 S. California Ave, Palo Alto, CA 94304, USA. In der EU wird dieser Service wiederum von der Facebook Ireland Limited, 4 Grand Canal Square, Dublin 2, Irland, betrieben, nachfolgend beide nur „Facebook“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt0000000GnywAAC\u0026amp;status=Active garantiert Facebook, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Qualitätsverbesserung unseres Internetauftritts. Weitergehende Informationen über die möglichen Plug-ins sowie über deren jeweilige Funktionen hält Facebook unter https://developers.facebook.com/docs/plugins/ für Sie bereit.\nSofern das Plug-in auf einer der von Ihnen besuchten Seiten unseres Internetauftritts hinterlegt ist, lädt Ihr Internet-Browser eine Darstellung des Plug-ins von den Servern von Facebook in den USA herunter. Aus technischen Gründen ist es dabei notwendig, dass Facebook Ihre IP-Adresse verarbeitet. Daneben werden aber auch Datum und Uhrzeit des Besuchs unserer Internetseiten erfasst. Sollten Sie bei Facebook eingeloggt sein, während Sie eine unserer mit dem Plug-in versehenen Internetseite besuchen, werden die durch das Plug-in gesammelten Informationen Ihres konkreten Besuchs von Facebook erkannt. Die so gesammelten Informationen weist Facebook womöglich Ihrem dortigen persönlichen Nutzerkonto zu. Sofern Sie also bspw. den sog. „Gefällt mir“-Button von Facebook benutzen, werden diese Informationen in Ihrem Facebook-Nutzerkonto gespeichert und ggf. über die Plattform von Facebook veröffentlicht. Wenn Sie das verhindern möchten, müssen Sie sich entweder vor dem Besuch unseres Internetauftritts bei Facebook ausloggen oder durch den Einsatz eines Add-ons für Ihren Internetbrowser verhindern, dass das Laden des Facebook-Plug-in blockiert wird. Weitergehende Informationen über die Erhebung und Nutzung von Daten sowie Ihre diesbezüglichen Rechte und Schutzmöglichkeiten hält Facebook in den unter https://www.facebook.com/policy.php abrufbaren Datenschutzhinweisen bereit.\n„Twitter“-Social-Plug-in\nIn unserem Internetauftritt setzen wir das Plug-in des Social-Networks Twitter ein. Bei Twitter handelt es sich um einen Internetservice der Twitter Inc., 795 Folsom St., Suite 600, San Francisco, CA 94107, USA, nachfolgend nur „Twitter“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt0000000TORzAAO\u0026amp;status=Active garantiert Twitter, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Qualitätsverbesserung unseres Internetauftritts. Sofern das Plug-in auf einer der von Ihnen besuchten Seiten unseres Internetauftritts hinterlegt ist, lädt Ihr Internet-Browser eine Darstellung des Plug-ins von den Servern von Twitter in den USA herunter. Aus technischen Gründen ist es dabei notwendig, dass Twitter Ihre IP-Adresse verarbeitet. Daneben werden aber auch Datum und Uhrzeit des Besuchs unserer Internetseiten erfasst. Sollten Sie bei Twitter eingeloggt sein, während Sie eine unserer mit dem Plug-in versehenen Internetseite besuchen, werden die durch das Plug-in gesammelten Informationen Ihres konkreten Besuchs von Twitter erkannt. Die so gesammelten Informationen weist Twitter womöglich Ihrem dortigen persönlichen Nutzerkonto zu. Sofern Sie also bspw. den sog. „Teilen“-Button von Twitter benutzen, werden diese Informationen in Ihrem Twitter-Nutzerkonto gespeichert und ggf. über die Plattform von Twitter veröffentlicht. Wenn Sie das verhindern möchten, müssen Sie sich entweder vor dem Besuch unseres Internetauftritts bei Twitter ausloggen oder die entsprechenden Einstellungen in Ihrem Twitter-Benutzerkonto vornehmen. Weitergehende Informationen über die Erhebung und Nutzung von Daten sowie Ihre diesbezüglichen Rechte und Schutzmöglichkeiten hält Twitter in den unter https://twitter.com/privacy abrufbaren Datenschutzhinweisen bereit.\nMuster-Datenschutzerklärung der Anwaltskanzlei Weiß \u0026amp; Partner\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1530140400,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://sefiroth.net/nab/privacy/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/nab/privacy/","section":"","summary":"Personenbezogene Daten (nachfolgend zumeist nur „Daten“ genannt) werden von uns nur im Rahmen der Erforderlichkeit sowie zum Zwecke der Bereitstellung eines funktionsfähigen und nutzerfreundlichen Internetauftritts, inklusive seiner Inhalte und der dort angebotenen Leistungen, verarbeitet.","tags":null,"title":"Datenschutzerklärung (Privacy Policy)","type":"page"},{"authors":null,"categories":["Statistisches"],"content":"  Wenn meine Tochter SBI hört, denkt sie an Sally Bollywood Investigation. – Und ich oft auch. – Mit SBI ist hier aber nicht der Trickfilm für Kinder, sondern Simulation Based Inference, gemeint.\nAngestachelt von Prof. Dr. Karsten Lübke und im Schlepptau von Prof. Dr. Oliver Gansser, Prof. Dr. Matthias Gehrke und Prof. Dr. Bianca Krol haben ein paar kluge Köpfe bei der FOM den Unterricht für Statistik auf eine neue Grundlage gestellt. Und ich habe das Glück gehabt,dabei mitwirken zu dürfen.\nUnser Mastermind, Karsten Lübke, hat dazu einen sehr schönen und lesenswerten Blog-Eintrag geschrieben: https://www.causeweb.org/sbi/?p=1559\n","date":1528675200,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1528675200,"objectID":"eded8bceb46fc9fd80a8d90ec1566da6","permalink":"https://sefiroth.net/nab/post/sbi-simulation-based-inference/","publishdate":"2018-06-11T00:00:00Z","relpermalink":"/nab/post/sbi-simulation-based-inference/","section":"post","summary":"Wenn meine Tochter SBI hört, denkt sie an Sally Bollywood Investigation. – Und ich oft auch. – Mit SBI ist hier aber nicht der Trickfilm für Kinder, sondern Simulation Based Inference, gemeint.","tags":["Lehre","Allgemein","Statistik"],"title":"SBI - Simulation Based Inference","type":"post"},{"authors":null,"categories":["Allgemeines","Information"],"content":"  Jede Programmiersprache hat Regeln. Neben dem Regelwerk welches durch den Syntax einer Sprache festgelegt wird, gib es aber noch Regeln über die Form in der man den Quelltext schreibt. Diese sogenannte Stilregeln (engl. style guides) sind von Programmieren aufgestellte Regeln um ein einheitliches “Schriftbild” des Quelltextes zu erhalten. Das Ziel der Stilregeln ist es, den Quelltext lesbarer zu gestallten, um leichter Änderungen einzupflegen oder um unnötiges zu vermeiden.\nEine Programmiersprache wie Python zum Beispiel hat mit PEP8 einen eigenen Standard wie ein Python Programm geschrieben seien sollte. Dazu gibt es auch gleich das passenden Prüfprogramm (früher pep8, neuerdings pycodestyle).\nSchreibt man ein R markdown Text mag man vielleicht nicht daran denken, dass so eine Idee auch hier sehr sinnvoll ist. Neben den gängigen Style-Guides für den R Quellcode (z. B.: Google’s R Style Guide, Hadley Wickham’s Advanced R - Style guide, jef.works R Style Guide, R Style Guide oder R-Style-Guide) gibt es aber kaum Regeln (z. B.: Pimp my Rmd) für die Gestaltung von R markdown.\nStil-Regeln für gutes R markdown, ein erster Vorschlag Keine unnützen Zeichen am Ende von Textzeilen. / No whitespaces at the end of a line\nEine Textzeile sollte mit einem ‘echtem’ Zeichen enden und nicht mit einem ‘unsichtbarem’ Zeichen. Das heisst: Leerzeichen, Tabs, harte Leerzeichen etc. gehören nicht ans Ende einer Zeile.\n Zwei Leerzeilen vor einer jeden Kopfzeile. / Two blank lines before every header\nUm die Inhalte auch klar voneinander trennen zu können sollte man vor der Kopfzeile zwei Leerzeilen eingefügt werden. Statt\n# Das ist eine Kopfzeile auf der 1. Ebene ## Das is eine Kopfzeile auf der 2. Ebene Das hier ist einfacher Text sollte es so gegliedert sein:\n # Das ist eine Kopfzeile auf der 1. Ebene ## Das is eine Kopfzeile auf der 2. Ebene Das hier ist einfacher Text Vor und nach Aufzählungen sollte immer eine Leerzeile stehen. / One blank line before and after itemizations or enumerations\nStatt\nDas ist eine Liste: - Ein Punkt - Ein anderer Punkt Und hier geht der Text weiter. 1. Der erste Punkt. 2. Der zweite Punkt. Und wieder mal ein Text. sollte es so gegliedert sein:\nDas ist eine Liste: - Ein Punkt - Ein anderer Punkt Und hier geht der Text weiter. 1. Der erste Punkt. 2. Der zweite Punkt. Und wieder mal ein Text. Vor und nach Codeblöcken sollte immer eine Leerzeile stehen. / One blank line before and after a codeblock\nStatt\n Etwas Text vorher ```{r}1+1 ```und danach.  sollte man es besser wie folgt gliedern:\n Etwas Text vorher ```{r}1+1 ``` und danach.  Keine anderen Sprachen als R markdown für Inhalte oder Design nutzen. / Use no other languages to create content or design, other than (R) markdown.\nKeine anderen Sprachen, insbesondere LaTeX, um besondere Effekte zu erzielen. Dafür sollten (native) DIV oder SPAN Abschnitte benutzt werden und entsprechend durch spätere (Filter-)Programme umgesetzt werden. So ist es immer möglich Design-Ideen für alle möglichen Zielsprachen zu erhalten.\n   RmdStyleChecker, ein erster Style Checker für R markdown Die ersten drei Punkte der Liste habe ich zu Testzwecken in einem kleinen Projekt mit Hilfe von Python implementiert. Den Python-Quelltext findet man unter RmdStyleChecker. Er läuft unter Python 3.5+.\n ","date":1525219200,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1525219200,"objectID":"fbd2d9474dec3dc967107598b9f96114","permalink":"https://sefiroth.net/nab/post/auch-r-markdown-dateien-sollten-sich-an-regeln-halten/","publishdate":"2018-05-02T00:00:00Z","relpermalink":"/nab/post/auch-r-markdown-dateien-sollten-sich-an-regeln-halten/","section":"post","summary":"Jede Programmiersprache hat Regeln. Neben dem Regelwerk welches durch den Syntax einer Sprache festgelegt wird, gib es aber noch Regeln über die Form in der man den Quelltext schreibt.","tags":["R","Allgemein","R markdown","Python"],"title":"Auch R markdown Dateien sollten sich an Regeln halten","type":"post"},{"authors":null,"categories":["Statistisches"],"content":"         Ein Nullhypothesentest ist schnell geschrieben. Will man den approximativen Weg gehen, so hilft R einem mit entsprechenden Tests mit einfachen Befehlen. Nimmt man MOSAIC dazu, so bekommt man u.a. für den Test auf Anteils- oder Mittelwerte sogar einen sehr einfachen, weil einheitlichen, Syntax.\nZwei Beispiele für approximative Hypothesentests mit MOSAIC Laden wir unsere Testdaten, die tipping Daten wie folgt:\nlibrary(mosaic) download.file(\u0026quot;https://goo.gl/whKjnl\u0026quot;, destfile = \u0026quot;tips.csv\u0026quot;) tips \u0026lt;- read.csv2(\u0026quot;tips.csv\u0026quot;) set.seed(2009) Dann erstellen wir zwei Forschungsfragen:\nIst der mittlere Frauenanteil unter der Bezahler*innen zu den Zeitpunkten Lunch und Dinner gleich? Ist der mittlere Rechnungsbetrag zu den Zeitpunkten Lunch und Dinner gleich?  Im ersten Fall ist die Hypothese schnell geschrieben:\n\\[ H_0 : \\pi_{\\text{Lunch}} = \\pi_{\\text{Dinner}} \\quad\\text{vs.}\\quad H_1 : \\pi_{\\text{Lunch}} \\neq \\pi_{\\text{Dinner}} \\] Der approximative Test mit R und MOSAIC lautet nun:\nprop.test(sex ~ time, success = \u0026quot;Female\u0026quot;, data = tips) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: tally(sex ~ time) ## X-squared = 9.3438, df = 1, p-value = 0.002237 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.36602563 -0.07247705 ## sample estimates: ## prop 1 prop 2 ## 0.2954545 0.5147059 Ähnlich sieht es für den zweiten Fall aus. Die Hypothese lautet hier:\n\\[ H_0 : \\mu_{Lunch} = \\mu_{Dinner} \\quad\\text{vs.}\\quad H_1 : \\mu_{Lunch} \\neq \\mu_{Dinner} \\]\nDer dazugehörige Test lautet dann:\nt.test(total_bill ~ time, data = tips) ## ## Welch Two Sample t-test ## ## data: total_bill by time ## t = 3.123, df = 143.29, p-value = 0.002167 ## alternative hypothesis: true difference in means between group Dinner and group Lunch is not equal to 0 ## 95 percent confidence interval: ## 1.331877 5.925088 ## sample estimates: ## mean in group Dinner mean in group Lunch ## 20.79716 17.16868  Simulation der Nullverteilung mit MOSAIC Ein anderer Weg ist es die Stichprobe selber zu nutzen um daraus eine Verteilung der Nullhypothese (die Nullverteilung) ableiten zu können. Im ersten Fall schaut man sich die Anteilsunterschiede an, wenn man die (potentielle) Abhängigkeit von der Tageszeit (Lunch und Dinner) künstlich “abschaltet”:\nset.seed(2009) NullVtlgAntwert \u0026lt;- do(10000) * diffprop(sex ~ shuffle(time), success = \u0026quot;Female\u0026quot;, data = tips) gf_histogram(~diffprop, nint = 25, data = NullVtlgAntwert) Schaut man sich nun die Lage der Anteilsdifferenz der Stichprobe \\(\\hat\\pi=0.2192513\\) in Bezug auf diese Nullverteilung geometrisch an, so kann man schon einen ersten Eindruck erlangen, ob die Nullhypothese abzulehnen ist oder nicht:\ndiffpropdach \u0026lt;- diffprop(sex ~ time, success = \u0026quot;Female\u0026quot;, data = tips) gf_histogram(~diffprop, nint = 25, data = NullVtlgAntwert) + geom_vline(xintercept = diffpropdach, color = \u0026quot;blue\u0026quot;) Offenbar ist \\(\\hat\\pi\\) kein sehr häufiges Ereignis.\nDer p-Wert ist ebenfalls leicht zu ermitteln:\npvalue_aw \u0026lt;- prop(~abs(diffprop) \u0026gt;= abs(diffpropdach), data = NullVtlgAntwert) pvalue_aw ## prop_TRUE ## 0.0018 Mit einem Anteilswert (p-Wert) von 0.0018 zweigen wir wie selten das Ereignis unter der \\(H_0\\) ist.\nÄhnlich sieht die Situation im zweien Fall aus. Mit Hilfe weniger Befehle erzeugen wir die Nullverteilung:\nset.seed(2009) NullVtlgMittelwert \u0026lt;- do(10000) * diffmean(total_bill ~ shuffle(time), data = tips) gf_histogram(~diffmean, nint = 25, data = NullVtlgMittelwert) Und können im Anschluss die Mittelwertsdifferenz der Stichprobe geometrisch einordnen:\ndiffmeandach \u0026lt;- diffmean(total_bill ~ time, data = tips) gf_histogram(~diffmean, nint = 25, data = NullVtlgMittelwert) + geom_vline(xintercept = diffmeandach, color = \u0026quot;blue\u0026quot;) Auch den p-Wert können wir wieder leicht bestimmen:\npvalue_mw \u0026lt;- prop(~abs(diffmean) \u0026gt;= abs(diffmeandach), data = NullVtlgMittelwert) pvalue_mw ## prop_TRUE ## 0.0047  Das Problem – Zeit Das Problem bei der Simulation ist die Zeit, die R braucht um die Nullverteilungen zu generieren. Das liegt im wesentlichen an Mosaic. Mit den Routinen aus FastSimNullDistR lassen sich die Nullverteilungen deutlich schneller berechnen. Ein Vergleich:\n {\"x\":{\"message\":{\"prof\":{\"time\":[1,1,1,1,1,1,1,1,1,2,2,2,2,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,7,7,7,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,10,10,10,10,10,11,11,11,11,11,12,12,12,12,12,12,12,12,13,13,13,13,13,13,13,14,14,15,15,15,15,15,15,15,15,15,15,16,16,16,16,16,16,16,17,17,17,17,17,17,17,17,17,18,18,18,18,18,18,18,18,18,18,19,19,19,19,19,19,19,19,20,20,20,20,20,20,20,20,21,21,22,22,22,22,22,23,23,23,23,23,23,24,24,24,24,24,24,24,24,25,25,25,25,25,25,25,25,26,26,26,26,26,26,26,27,27,27,27,27,27,27,27,27,27,27,27,27,27,28,28,29,29,29,29,29,29,29,30,30,30,30,30,30,30,30,30,30,30,30,30,30,31,31,31,31,31,31,31,31,31,31,32,32,32,32,32,32,32,32,32,32,33,33,33,33,33,33,33,33,34,34,34,34,34,34,34,34,34,34,34,35,35,35,35,35,35,35,36,36,36,36,36,36,36,37,37,37,37,37,37,37,37,37,37,38,38,38,38,38,38,39,39,39,39,39,39,39,40,40,40,40,40,40,40,41,41,41,41,41,42,42,42,42,42,43,43,43,43,43,44,44,44,44,44,45,45,45,45,45,45,45,45,46,46,46,46,46,47,47,47,47,47,47,47,47,47,47,47,47,48,48,48,48,48,48,49,49,49,49,49,49,49,49,49,49,50,50,50,50,50,50,51,51,52,52,52,52,52,52,52,52,53,53,53,53,53,53,53,53,53,53,54,54,54,54,54,54,54,54,54,54,54,55,55,55,55,55,55,55,55,55,55,55,55,55,56,56,57,57,57,57,57,58,58,58,58,58,58,58,59,59,59,59,59,59,60,60,60,60,60,60,61,61,61,61,61,61,61,62,62,62,62,62,62,62,63,63,63,63,63,63,63,64,64,64,64,64,64,64,65,65,65,65,65,65,65,66,66,66,66,66,66,66,67,67,67,67,67,68,68,68,68,68,69,69,69,69,69,70,70,70,70,70,71,71,71,71,71,72,72,72,72,72,73,73,73,73,73,73,73,73,74,74,74,74,74,74,75,75,76,76,76,76,76,76,76,77,77,78,78,78,78,78,78,78,78,78,78,78,79,79,79,79,79,79,79,80,80,80,80,80,80,80,80,81,81,81,81,81,81,81,82,82,83,83,83,83,83,83,83,83,83,83,83,84,84,84,84,84,85,85,85,85,85,85,85,85,85,86,86,86,87,87,87,87,87,87,88,88,88,88,88,88,88,89,89,89,89,89,89,89,89,89,89,89,89,89,90,90,91,91,91,91,91,92,92,92,92,92,92,92,92,93,93,94,94,94,94,94,94,94,94,95,95,95,95,95,95,95,96,96,96,96,96,96,96,97,97,97,97,97,97,98,98,98,98,98,98,98,98,98,99,99,100,100,100,100,100,100,100,100,100,100,101,101,102,102,102,102,102,102,102,103,103,103,103,103,103,103,103,103,104,104,104,104,104,105,105,105,105,105,105,105,105,105,106,106,106,106,106,106,106,106,106,106,107,107,108,108,108,108,108,108,109,109,109,109,109,109,109,110,110,110,110,110,110,110,110,111,111,111,111,111,111,111,111,112,112,112,112,112,112,112,112,113,113,113,113,113,113,113,113,114,114,115,115,115,115,115,115,115,115,115,115,115,116,116,116,116,116,116,116,116,116,117,117,117,117,117,117,117,118,118,118,118,118,118,118,118,119,119,119,119,119,120,120,120,120,120,121,121,121,121,121,121,121,121,121,121,121,122,122,122,122,122,122,122,122,122,122,122,122,122,123,123,123,123,123,123,123,123,123,123,123,124,124,124,124,124,125,125,125,125,125,126,126,126,126,126,126,126,127,127,127,127,127,127,127,127,127,127,128,128,128,128,128,128,128,128,129,129,129,129,129,129,130,130,130,130,130,130,130,131,131,131,131,131,131,131,131,132,132,132,132,132,132,132,132,133,133,133,133,133,133,133,133,134,134,134,134,134,134,134,135,135,135,135,135,135,135,135,135,135,135,136,136,137,137,137,137,137,138,138,138,138,138,138,138,138,139,139,139,139,139,139,139,139,139,140,140,140,140,140,140,141,141,141,141,141,141,142,142,142,142,142,143,143,143,143,143,144,144,144,144,144,144,144,144,145,145,145,145,145,146,146,146,146,146,146,146,146,147,147,148,148,149,149,149,149,149,150,150,150,150,150,150,150,150,150,150,151,151,151,151,151,151,152,152,152,152,152,152,152,152,152,153,153,153,153,153,153,153,154,154,154,154,154,154,155,155,156,156,156,156,156,156,156,157,157,157,157,157,157,157,157,158,158,158,158,158,158,158,158,159,159,159,159,159,159,159,159,159,159,159,160,160,160,160,160,160,160,160,160,160,160,161,161,161,161,161,161,161,161,162,162,162,162,162,162,162,163,163,163,163,163,163,163,164,164,164,164,164,164,165,165,165,165,165,165,165,165,165,166,166,166,166,166,166,166,167,167,167,167,167,167,167,168,168,168,168,168,168,168,169,169,170,170,170,170,170,170,170,170,171,171,171,171,171,171,171,171,172,172,172,173,173,173,173,173,173,173,173,174,174,174,175,175,175,175,175,175,175,175,176,176,176,176,176,176,176,176,176,176,176,177,177,177,177,177,177,177,177,177,177,177,177,178,178,178,178,178,178,178,179,179,179,179,179,179,179,179,179,179,180,180,180,180,180,181,181,181,181,181,182,182,182,182,182,182,182,182,182,182,183,183,184,184,184,184,184,185,185,185,185,185,185,185,185,185,185,185,185,185,186,186,186,186,186,186,186,186,186,187,187,187,187,187,187,188,188,189,189,189,189,189,189,189,189,190,190,190,190,190,190,190,190,191,191,191,191,191,191,191,191,192,192,193,193,193,193,193,193,193,194,194,194,194,194,195,195,195,195,195,196,196,196,196,196,197,197,197,197,197,197,197,198,198,198,198,198,199,199,199,199,199,199,199,200,200,201,201,202,202,202,202,202,202,202,202,202,202,203,203,204,204,204,204,204,204,204,204,205,205,205,205,205,205,206,206,206,206,206,207,207,207,207,207,207,207,208,208,208,208,208,208,208,209,209,209,209,209,209,209,210,210,210,210,210,210,210,211,211,211,211,211,212,212,212,212,212,212,212,212,212,213,213,213,213,213,213,213,213,214,214,214,214,214,214,215,215,215,216,216,216,216,216,216,217,217,217,217,217,217,217,218,218,218,218,218,218,218,219,219,219,219,219,219,219,219,219,220,220,220,220,220,220,220,221,221,221,221,221,221,221,221,221,221,222,222,222,222,222,222,222,223,223,223,223,223,223,223,223,223,223,224,224,224,224,224,224,224,225,225,226,226,226,226,226,226,226,227,227,227,227,227,227,227,227,228,228,228,228,228,228,228,228,228,228,229,229,229,229,229,230,230,230,230,230,231,231,231,231,231,231,231,232,232,233,233,233,233,233,233,234,234,234,234,234,234,234,234,234,234,234,235,235,235,235,235,236,236,236,236,236,236,236,236,236,236,236,236,237,237,237,237,237,237,237,237,237,237,237,238,238,238,238,238,239,239,239,239,239,239,239,239,239,239,239,240,240,240,240,240,240,240,241,241,241,241,241,241,241,241,242,242,242,242,242,243,243,243,243,243,243,243,243,244,244,244,244,244,244,244,245,245,245,245,245,245,245,245,245,245,246,246,246,246,246,246,246,246,246,246,247,247,247,247,247,248,248,249,249,250,250,250,250,250,250,250,250,251,251,251,251,251,251,251,251,251,251,252,252,252,252,253,253,253,253,253,253,253,254,254,254,254,254,254,255,255,255,255,255,255,255,255,255,255,255,256,256,256,256,256,256,256,257,257,257,257,257,257,258,258,258,258,258,258,258,259,259,259,259,259,260,260,260,260,260,260,260,260,261,261,261,261,261,261,261,262,262,262,262,262,262,262,262,262,263,263,263,263,263,263,263,263,264,264,264,264,264,264,264,265,265,265,265,265,265,265,265,265,265,266,266,266,266,266,267,267,268,268,268,268,268,269,269,270,270,270,270,270,270,270,270,270,270,270,271,271,271,271,272,272,272,273,273,273,273,273,273,273,273,273,273,273,274,274,274,274,274,274,274,274,275,275,275,275,275,275,275,275,275,275,275,275,275,276,276,276,276,276,276,276,276,277,277,277,277,277,277,277,278,278,278,278,278,278,278,278,278,279,279,279,280,280,280,280,280,280,280,281,281,281,281,281,281,281,281,281,281,282,282,282,282,282,282,282,283,283,283,283,283,283,283,283,283,283,283,283,283,283,284,284,284,285,285,285,285,285,285,285,285,286,286,286,286,286,286,286,287,287,287,287,287,287,287,287,288,288,289,289,289,289,289,289,289,289,289,289,289,290,290,290,290,290,290,291,291,291,291,291,291,291,291,291,291,292,292,292,292,292,292,292,292,292,292,293,293,293,293,293,293,293,294,294,295,295,295,295,295,295,295,295,295,295,296,296,296,296,296,296,296,297,297,297,297,297,298,298,298,298,298,298,298,299,299,299,299,299,300,300,300,300,300,300,300,300,301,301,301,301,301,301,301,301,301,301,301,302,302,302,303,303,303,303,303,303,303,303,304,304,304,304,304,304,304,304,304,304,305,305,305,305,305,305,305,305,305,306,306,307,307,307,307,307,307,307,308,308,308,308,308,308,308,308,308,309,309,309,309,309,309,309,309,309,309,310,310,310,310,310,310,310,310,310,310,311,311,311,311,311,311,311,312,312,312,312,312,313,313,313,313,313,313,314,314,314,314,314,314,314,315,315,316,316,316,316,316,316,316,317,317,317,317,317,317,317,317,318,318,318,318,318,318,318,318,318,318,318,318,318,319,319,319,319,319,319,319,319,319,319,320,320,320,320,320,320,321,321,322,322,322,322,322,322,322,322,322,322,323,323,323,323,323,323,323,324,324,325,325,325,325,325,325,325,326,326,327,327,327,327,327,327,327,327,327,328,328,328,328,328,328,328,328,328,328,328,328,329,329,329,329,330,330,330,330,330,330,331,331,331,331,331,332,332,332,332,332,332,332,332,333,333,333,334,334,334,334,334,335,335,335,335,335,335,335,336,336,336,336,336,336,337,337,338,338,338,338,339,339,339,339,339,340,340,340,340,340,340,341,341,341,341,341,341,341,341,341,341,341,342,342,343,343,343,343,343,343,343,343,344,344,344,344,344,344,344,344,345,345,345,345,345,345,345,345,346,346,346,346,346,346,346,346,346,347,347,347,347,347,347,348,348,348,348,348,348,349,349,350,350,350,350,350,350,350,350,351,351,351,351,351,351,351,351,352,352,352,353,353,353,353,353,353,353,354,354,354,354,354,354,354,355,355,355,355,355,355,355,356,356,356,356,356,356,356,356,356,356,357,357,357,357,357,358,358,358,358,358,358,358,358,359,359,359,359,359,359,359,359,360,360,360,360,360,360,360,360,360,360,361,361,362,362,362,362,362,362,362,362,362,362,362,363,363,363,363,363,363,363,364,364,364,364,364,364,364,364,364,365,365,365,365,365,365,365,365,365,365,365,366,366,366,366,366,366,366,366,366,366,367,367,367,367,367,368,368,368,368,368,368,368,368,368,369,369,369,369,369,369,369,369,369,369,370,370,370,370,370,370,371,371,371,371,371,371,371,372,372,372,372,372,373,373,373,373,373,373,373,374,374,374,374,374,374,374,374,374,374,374,375,375,375,375,375,375,375,375,376,376,377,377,377,377,377,377,377,377,378,378,378,378,378,378,378,379,379,379,379,379,379,379,379,379,380,380,380,380,380,380,380,380,381,381,381,381,381,381,381,381,381,381,381,382,382,382,382,382,382,382,382,383,383,383,383,383,383,384,384,384,384,384,385,385,385,385,385,385,385,386,386,386,386,386,386,386,386,386,386,386,386,386,387,387,387,387,387,387,387,387,388,388,388,388,388,388,388,388,388,388,388,388,389,389,389,389,389,389,389,389,390,390,391,391,391,391,391,391,392,392,392,392,392,393,393,393,393,393,394,394,395,395,395,396,396,397,397,397,397,397,397,397,398,398,398,398,398,398,398,398,398,398,399,399,399,399,399,399,399,399,399,399,399,399,400,400,400,400,400,400,400,401,401,401,401,401,401,401,402,402,402,402,402,402,402,403,403,403,403,403,403,403,404,404,405,405,405,405,405,405,405,406,406,406,406,407,407,407,407,407,407,407,407,408,408,408,408,408,408,408,408,408,408,409,409,409,409,409,410,410,410,410,410,410,410,410,411,411,411,411,411,411,411,412,412,412,412,412,412,412,412,412,412,413,413,413,413,413,413,413,413,414,414,414,414,414,414,414,415,415,415,415,415,415,415,416,416,416,416,416,416,416,416,417,417,417,417,418,418,419,419,419,419,419,419,419,419,420,420,420,420,420,420,420,420,421,421,422,422,422,423,423,423,423,423,423,423,424,424,424,424,424,424,424,425,425,426,426,426,426,426,426,426,427,427,427,427,427,427,428,428,429,429,429,429,429,429,430,430,430,430,430,431,431,431,431,431,431,431,432,432,432,432,432,432,432,432,433,433,433,433,433,433,433,434,434,434,434,435,435,435,435,435,435,435,435,436,436,436,436,436,436,436,436,437,437,437,437,437,437,438,438,438,438,438,438,438,438,438,438,438,439,439,439,439,439,439,439,440,440,440,440,440,440,441,441,441,441,441,442,442,442,442,442,442,442,442,442,443,443,443,443,443,443,443,443,444,444,444,444,444,444,444,444,444,444,444,444,445,445,445,445,445,445,446,446,446,446,446,446,447,447,447,447,447,447,447,447,447,448,448,448,448,448,448,448,448,448,448,449,449,449,449,449,449,449,449,450,450,450,450,450,450,451,451,451,451,451,451,451,451,451,452,452,453,453,453,453,453,453,453,453,453,454,454,454,454,454,454,454,454,454,454,454,454,454,454,455,455,455,455,455,456,456,456,456,456,456,456,456,457,457,457,457,457,457,457,457,457,458,458,458,458,458,458,458,459,459,459,459,459,459,460,460,460,460,460,460,460,461,461,461,461,461,461,461,462,462,463,463,463,463,463,464,464,464,464,464,464,464,464,464,464,465,465,465,465,465,465,465,465,465,465,465,465,465,466,466,466,466,466,466,466,466,467,467,467,467,467,467,467,467,468,468,468,468,468,468,468,469,469,469,469,469,469,469,469,470,470,470,470,470,470,470,471,471,471,471,471,472,472,472,473,473,473,473,473,473,473,473,474,474,475,475,475,475,475,475,475,475,476,476,476,476,476,476,476,476,477,477,477,477,477,477,478,478,478,478,478,478,479,479,479,479,479,480,480,480,480,480,480,480,480,480,481,481,481,481,481,482,482,482,482,482,482,483,483,483,483,483,484,484,484,484,484,484,484,485,485,485,485,485,485,486,486,486,486,486,486,486,486,486,486,487,487,487,487,487,487,487,488,488,488,488,488,488,488,489,489,489,489,489,489,489,489,490,490,490,491,491,491,491,491,491,491,492,492,492,492,492,493,493,493,493,493,493,493,493,494,494,495,495,495,495,495,495,496,496,496,496,496,496,496,496,496,496,497,497,497,497,497,497,497,497,498,498,499,499,499,499,499,499,499,499,500,500,500,500,500,500,500,501,501,501,501,501,501,501,501,502,502,502,502,502,502,502,502,502,503,503,504,504,504,504,504,504,504,504,505,505,505,505,505,505,505,506,506,506,506,506,506,506,506,506,507,507,508,508,508,508,508,508,508,508,508,508,509,509,510,510,510,510,510,510,511,511,511,511,511,511,511,511,511,512,512,512,512,512,512,513,513,513,513,513,513,513,513,513,513,514,514,514,514,514,515,515,515,515,515,515,515,515,516,516,516,516,516,516,516,516,517,517,517,517,517,517,518,518,518,518,518,518,518,518,518,518,519,519,520,520,520,520,520,520,521,521,521,521,521,521,521,522,522,522,522,522,522,522,523,523,523,524,524,524,524,524,524,524,525,525,525,525,525,525,525,525,526,526,526,526,526,526,526,527,527,527,527,528,528,529,529,529,529,529,529,530,530,530,531,531,531,531,531,532,532,532,532,532,532,532,532,533,533,533,533,533,533,533,533,534,534,534,534,534,534,534,535,535,535,536,536,536,536,536,536,536,536,536,536,536,536,537,537,537,537,537,538,538,538,538,538,539,539,539,539,539,539,539,539,539,539,540,540,540,540,540,540,540,540,541,541,542,542,543,543,543,543,543,543,543,543,544,544,544,544,544,545,545,545,545,545,545,545,545,546,546,546,546,546,546,547,547,547,547,547,547,547,548,548,548,548,548,548,549,549,549,549,549,549,549,549,549,550,550,550,550,550,550,551,551,552,552,552,552,552,552,552,552,553,553,553,553,553,553,553,554,554,554,554,554,554,555,555,555,555,555,555,555,555,556,556,557,557,558,558,559,559,559,559,559,559,559,559,560,560,560,560,560,560,560,560,560,561,561,561,561,561,561,561,562,562,562,562,562,562,562,563,563,563,563,563,563,563,563,564,564,564,564,564,564,564,565,565,565,565,565,565,565,565,565,566,566,566,566,566,566,566,567,567,567,567,567,567,567,567,568,568,568,568,568,568,568,568,569,569,570,570,570,570,571,571,571,571,571,571,571,571,572,572,572,572,572,572,573,573,573,573,573,574,574,574,574,574,574,575,575,575,575,575,575,575,576,576,576,576,576,576,576,576,576,576,576,577,577,577,577,577,577,578,578,578,578,578,578,578,578,578,579,579,579,579,579,579,579,579,579,579,579,579,579,580,580,580,580,580,580,580,581,581,581,581,581,581,581,581,581,581,582,582,582,582,582,582,582,582,582,582,582,583,583,583,583,583,583,583,583,583,583,584,584,584,584,584,585,585,585,585,585,585,586,586,586,586,586,586,586,587,587,588,588,588,588,588,589,589,589,589,589,589,589],\"depth\":[9,8,7,6,5,4,3,2,1,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,3,2,1,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,2,1,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,2,1,10,9,8,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,2,1,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,3,2,1,8,7,6,5,4,3,2,1,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,2,1,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,2,1,2,1,10,9,8,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,2,1,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,2,1,5,4,3,2,1,2,1,11,10,9,8,7,6,5,4,3,2,1,4,3,2,1,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,2,1,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,2,1,3,2,1,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,4,3,2,1,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,2,1,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,3,2,1,8,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,2,1,10,9,8,7,6,5,4,3,2,1,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,4,3,2,1,2,1,6,5,4,3,2,1,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,2,1,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,5,4,3,2,1,7,6,5,4,3,2,1],\"label\":[\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"mosaicCore::tally\",\"prop\",\"diffprop\",\"local\",\"as.list.default\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"structure\",\"parse.formula\",\"lhs.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"names\",\"eval\",\"match.arg\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"is.na\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"match.arg\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"c\",\"local\",\"as.list\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"parse.formula\",\"condition.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"enexpr\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"eval\",\"match.arg\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"diffprop\",\"local\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"is.call\",\"local\",\"parse.formula\",\"condition.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"order\",\"make.names\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"match.fun\",\"vapply\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"match.arg\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"c\",\"local\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"unique.default\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.list\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"structure\",\"parse.formula\",\"lhs.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.data.frame\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"is.na\",\"local\",\"FUN\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"logical\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"NextMethod\",\"[.table\",\"prop\",\"diffprop\",\"local\",\"utils::tail\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"%in%\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"match.arg\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"FUN\",\"vapply\",\"list.names\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"match.arg\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"names\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"dim\",\"local\",\"attributes\",\"local\",\"logical2factor\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"lhs\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"formals\",\"match.arg\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"rhs_or_expr\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"is.numeric\",\"local\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.list\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"vapply\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sys.function\",\"match.arg\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"match.fun\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"structure\",\"parse.formula\",\"condition.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"unique.default\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"array\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sys.function\",\"match.arg\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"eval\",\"match.arg\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.list.default\",\"alist\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.integer\",\"local\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"prop\",\"diffprop\",\"local\",\"as.data.frame\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"\",\"diffprop\",\"local\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"mode\",\"deparse\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"enexpr\",\"rhs_or_expr\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"paste\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"NextMethod\",\"[.table\",\"prop\",\"diffprop\",\"local\",\"logical2factor\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"logical\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"attributes\",\"local\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"formals\",\"match.arg\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"lhs\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"c\",\"local\",\"[[\",\"parse.formula\",\"condition.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"structure\",\"parse.formula\",\"rhs.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.list.default\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"c\",\"local\",\"formals\",\"match.arg\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"unique.default\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"list\",\"local\",\"attributes\",\"local\",\"deparse\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"length\",\"local\",\"match.arg\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"environment\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"parse.formula\",\"condition.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"parse.formula\",\"lhs.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"rhs.parsedFormula\",\"rhs.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"%in%\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.list\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"prop\",\"diffprop\",\"local\",\"lhs\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"names\",\"lhs\",\"lhs.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"parse.formula\",\"rhs.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\".deparseOpts\",\"deparse\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\".External\",\"local\",\"formals\",\"match.arg\",\"order\",\"make.names\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"match.arg\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"logical\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"paste\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.list\",\"list.names\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sum\",\"local\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"unique.default\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"as.data.frame.data.frame\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sys.function\",\"match.arg\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sample\",\"shuffle\",\"local\",\"as.data.frame.data.frame\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"match.arg\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"pmatch\",\".deparseOpts\",\"deparse\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"logical\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"utils::tail\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"structure\",\"parse.formula\",\"lhs.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"any\",\"local\",\".deparseOpts\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\".deparseOpts\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"order\",\"make.names\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"match.arg\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"unique.default\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"vapply\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"any\",\"local\",\"make.names\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse\",\"mode\",\"deparse\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"[[\",\"parse.formula\",\"lhs.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"match.fun\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.list.default\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"match.arg\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"eval\",\"match.arg\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"unlist\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"structure\",\"parse.formula\",\"rhs.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"c\",\"local\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"utils::tail\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"c\",\"local\",\"diff.default\",\"diffprop\",\"local\",\"length\",\"local\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"%in%\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"parse.formula\",\"condition.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"prod\",\"local\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"unlist\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sys.frame\",\"match.arg\",\"prop\",\"diffprop\",\"local\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"structure\",\"as.data.frame.vector\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"length\",\"parse.formula\",\"lhs.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"unique\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"structure\",\"parse.formula\",\"lhs.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"paste\",\"prop\",\"diffprop\",\"local\",\".External2\",\"local\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"make.names\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sum\",\"local\",\"sample\",\"shuffle\",\"local\",\"lhs\",\"lhs.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"unique.default\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"is.pairlist\",\"local\",\"%in%\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"levels\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"length\",\"local\",\"checkHT\",\"tail.default\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"parse.formula\",\"rhs.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"match.fun\",\"vapply\",\"list.names\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"colSums\",\"prop\",\"diffprop\",\"local\",\"structure\",\"parse.formula\",\"rhs.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"length\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"eval\",\"match.arg\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"checkHT\",\"tail.default\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.list\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"eval\",\"match.arg\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.data.frame.vector\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"unlist\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"lhs\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\".deparseOpts\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"c\",\"local\",\"sys.parent\",\"match.arg\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"%in%\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.data.frame\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.data.frame.vector\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sys.call\",\"alist\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"%in%\",\"[.data.frame\",\"logical2factor.data.frame\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"diffprop\",\"local\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"logical\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.data.frame\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.list.default\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"logical\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sys.call\",\"~\",\"local\",\"as.list\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"c\",\"local\",\"$\",\"lhs.parsedFormula\",\"lhs.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"length\",\"parse.formula\",\"condition.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"levels\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"order\",\"make.names\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"utils::tail\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"parse.formula\",\"condition.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"lapply\",\"*\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"rhs.parsedFormula\",\"rhs.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"logical\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"prop\",\"diffprop\",\"local\",\"condition\",\"condition.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"seq.int\",\"local\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"structure\",\"as.data.frame.vector\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\".deparseOpts\",\"deparse\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"c\",\"local\",\"structure\",\"parse.formula\",\"lhs.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"$\",\"lhs.parsedFormula\",\"lhs.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse\",\"mode\",\"deparse\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"c\",\"local\",\"vapply\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"names\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"list\",\"local\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"c\",\"local\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"paste\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.list\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.data.frame\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"levels\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"structure\",\"as.data.frame.vector\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"is.null\",\"local\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"rhs\",\"rhs.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"condition.parsedFormula\",\"condition.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.list.default\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"formals\",\"match.arg\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"dimnames\",\"prop\",\"diffprop\",\"local\",\".External\",\"local\",\"names\",\"logical2factor.data.frame\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"prop\",\"diffprop\",\"local\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"make.names\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sys.call\",\"alist\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sys.call\",\"~\",\"local\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"structure\",\"parse.formula\",\"condition.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"c\",\"local\",\"sum\",\"local\",\"unlist\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"condition\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"match.arg\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"%in%\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"length\",\"local\",\"structure\",\"parse.formula\",\"rhs.formula\",\"mosaic_formula_q\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"joinTwoFrames\",\"\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.integer\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"length\",\"parse.formula\",\"rhs.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"any\",\"local\",\"c\",\"local\",\"shuffle\",\"local\",\".set_row_names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"match.fun\",\"vapply\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"as.list.default\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"unique.default\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"min\",\"local\",\"%in%\",\"prop\",\"diffprop\",\"local\",\"cull\",\"FUN\",\"lapply\",\"*\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"sys.parent\",\"match.arg\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"rhs\",\"rhs.formula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"match.fun\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"lhs\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sys.frame\",\"match.arg\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"eval\",\"match.arg\",\"order\",\"make.names\",\"data.frame\",\"joinTwoFrames\",\"joinFrames\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"pmatch\",\".deparseOpts\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"length\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"order\",\"factor\",\"table\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"c\",\"local\",\"logical2factor\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\",\"data.frame\",\"evalSubFormula\",\"evalFormula\",\"mosaic_tally.formula\",\"prop\",\"diffprop\",\"local\"],\"filenum\":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],\"linenum\":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],\"memalloc\":[27.7805709838867,27.7805709838867,27.7805709838867,27.7805709838867,27.7805709838867,27.7805709838867,27.7805709838867,27.7805709838867,27.7805709838867,28.3874588012695,28.3874588012695,28.3874588012695,28.3874588012695,28.9589157104492,28.9589157104492,28.9589157104492,28.9589157104492,28.9589157104492,28.9589157104492,28.9589157104492,28.9589157104492,29.673095703125,29.673095703125,29.673095703125,29.673095703125,29.673095703125,29.673095703125,29.673095703125,29.673095703125,29.673095703125,30.3136291503906,30.3136291503906,30.3136291503906,30.3136291503906,30.3136291503906,30.3136291503906,30.3136291503906,30.3136291503906,31.0767364501953,31.0767364501953,31.0767364501953,31.0767364501953,31.0767364501953,31.0767364501953,31.0767364501953,31.6766662597656,31.6766662597656,31.6766662597656,32.3892211914062,32.3892211914062,32.3892211914062,32.3892211914062,32.3892211914062,32.3892211914062,32.3892211914062,32.3892211914062,32.3892211914062,32.3892211914062,32.3892211914062,32.9736709594727,32.9736709594727,32.9736709594727,32.9736709594727,32.9736709594727,32.9736709594727,33.6134338378906,33.6134338378906,33.6134338378906,33.6134338378906,33.6134338378906,34.4901885986328,34.4901885986328,34.4901885986328,34.4901885986328,34.4901885986328,35.3649826049805,35.3649826049805,35.3649826049805,35.3649826049805,35.3649826049805,35.3649826049805,35.3649826049805,35.3649826049805,36.1131973266602,36.1131973266602,36.1131973266602,36.1131973266602,36.1131973266602,36.1131973266602,36.1131973266602,36.9279556274414,36.9279556274414,27.2034301757812,27.2034301757812,27.2034301757812,27.2034301757812,27.2034301757812,27.2034301757812,27.2034301757812,27.2034301757812,27.2034301757812,27.2034301757812,28.3361358642578,28.3361358642578,28.3361358642578,28.3361358642578,28.3361358642578,28.3361358642578,28.3361358642578,29.0114212036133,29.0114212036133,29.0114212036133,29.0114212036133,29.0114212036133,29.0114212036133,29.0114212036133,29.0114212036133,29.0114212036133,30.2216262817383,30.2216262817383,30.2216262817383,30.2216262817383,30.2216262817383,30.2216262817383,30.2216262817383,30.2216262817383,30.2216262817383,30.2216262817383,30.8097534179688,30.8097534179688,30.8097534179688,30.8097534179688,30.8097534179688,30.8097534179688,30.8097534179688,30.8097534179688,31.4588012695312,31.4588012695312,31.4588012695312,31.4588012695312,31.4588012695312,31.4588012695312,31.4588012695312,31.4588012695312,32.1905517578125,32.1905517578125,33.1924667358398,33.1924667358398,33.1924667358398,33.1924667358398,33.1924667358398,34.1846542358398,34.1846542358398,34.1846542358398,34.1846542358398,34.1846542358398,34.1846542358398,34.7411346435547,34.7411346435547,34.7411346435547,34.7411346435547,34.7411346435547,34.7411346435547,34.7411346435547,34.7411346435547,35.3837203979492,35.3837203979492,35.3837203979492,35.3837203979492,35.3837203979492,35.3837203979492,35.3837203979492,35.3837203979492,35.9701385498047,35.9701385498047,35.9701385498047,35.9701385498047,35.9701385498047,35.9701385498047,35.9701385498047,36.5658264160156,36.5658264160156,36.5658264160156,36.5658264160156,36.5658264160156,36.5658264160156,36.5658264160156,36.5658264160156,36.5658264160156,36.5658264160156,36.5658264160156,36.5658264160156,36.5658264160156,36.5658264160156,37.1768035888672,37.1768035888672,37.7760543823242,37.7760543823242,37.7760543823242,37.7760543823242,37.7760543823242,37.7760543823242,37.7760543823242,27.5875701904297,27.5875701904297,27.5875701904297,27.5875701904297,27.5875701904297,27.5875701904297,27.5875701904297,27.5875701904297,27.5875701904297,27.5875701904297,27.5875701904297,27.5875701904297,27.5875701904297,27.5875701904297,28.6407241821289,28.6407241821289,28.6407241821289,28.6407241821289,28.6407241821289,28.6407241821289,28.6407241821289,28.6407241821289,28.6407241821289,28.6407241821289,29.4275054931641,29.4275054931641,29.4275054931641,29.4275054931641,29.4275054931641,29.4275054931641,29.4275054931641,29.4275054931641,29.4275054931641,29.4275054931641,30.6539154052734,30.6539154052734,30.6539154052734,30.6539154052734,30.6539154052734,30.6539154052734,30.6539154052734,30.6539154052734,31.8022079467773,31.8022079467773,31.8022079467773,31.8022079467773,31.8022079467773,31.8022079467773,31.8022079467773,31.8022079467773,31.8022079467773,31.8022079467773,31.8022079467773,32.4189605712891,32.4189605712891,32.4189605712891,32.4189605712891,32.4189605712891,32.4189605712891,32.4189605712891,33.0316314697266,33.0316314697266,33.0316314697266,33.0316314697266,33.0316314697266,33.0316314697266,33.0316314697266,33.6384582519531,33.6384582519531,33.6384582519531,33.6384582519531,33.6384582519531,33.6384582519531,33.6384582519531,33.6384582519531,33.6384582519531,33.6384582519531,34.2805099487305,34.2805099487305,34.2805099487305,34.2805099487305,34.2805099487305,34.2805099487305,34.9846801757812,34.9846801757812,34.9846801757812,34.9846801757812,34.9846801757812,34.9846801757812,34.9846801757812,35.5951538085938,35.5951538085938,35.5951538085938,35.5951538085938,35.5951538085938,35.5951538085938,35.5951538085938,36.1692657470703,36.1692657470703,36.1692657470703,36.1692657470703,36.1692657470703,36.7828674316406,36.7828674316406,36.7828674316406,36.7828674316406,36.7828674316406,37.3964767456055,37.3964767456055,37.3964767456055,37.3964767456055,37.3964767456055,27.5428695678711,27.5428695678711,27.5428695678711,27.5428695678711,27.5428695678711,28.3362045288086,28.3362045288086,28.3362045288086,28.3362045288086,28.3362045288086,28.3362045288086,28.3362045288086,28.3362045288086,28.9825744628906,28.9825744628906,28.9825744628906,28.9825744628906,28.9825744628906,29.9959335327148,29.9959335327148,29.9959335327148,29.9959335327148,29.9959335327148,29.9959335327148,29.9959335327148,29.9959335327148,29.9959335327148,29.9959335327148,29.9959335327148,29.9959335327148,30.6982727050781,30.6982727050781,30.6982727050781,30.6982727050781,30.6982727050781,30.6982727050781,31.3637161254883,31.3637161254883,31.3637161254883,31.3637161254883,31.3637161254883,31.3637161254883,31.3637161254883,31.3637161254883,31.3637161254883,31.3637161254883,32.4448547363281,32.4448547363281,32.4448547363281,32.4448547363281,32.4448547363281,32.4448547363281,33.6714172363281,33.6714172363281,34.4176406860352,34.4176406860352,34.4176406860352,34.4176406860352,34.4176406860352,34.4176406860352,34.4176406860352,34.4176406860352,35.2163696289062,35.2163696289062,35.2163696289062,35.2163696289062,35.2163696289062,35.2163696289062,35.2163696289062,35.2163696289062,35.2163696289062,35.2163696289062,35.8289260864258,35.8289260864258,35.8289260864258,35.8289260864258,35.8289260864258,35.8289260864258,35.8289260864258,35.8289260864258,35.8289260864258,35.8289260864258,35.8289260864258,36.4721984863281,36.4721984863281,36.4721984863281,36.4721984863281,36.4721984863281,36.4721984863281,36.4721984863281,36.4721984863281,36.4721984863281,36.4721984863281,36.4721984863281,36.4721984863281,36.4721984863281,37.2122421264648,37.2122421264648,27.4598846435547,27.4598846435547,27.4598846435547,27.4598846435547,27.4598846435547,28.0758285522461,28.0758285522461,28.0758285522461,28.0758285522461,28.0758285522461,28.0758285522461,28.0758285522461,28.6935577392578,28.6935577392578,28.6935577392578,28.6935577392578,28.6935577392578,28.6935577392578,29.9251861572266,29.9251861572266,29.9251861572266,29.9251861572266,29.9251861572266,29.9251861572266,30.5644226074219,30.5644226074219,30.5644226074219,30.5644226074219,30.5644226074219,30.5644226074219,30.5644226074219,31.7151260375977,31.7151260375977,31.7151260375977,31.7151260375977,31.7151260375977,31.7151260375977,31.7151260375977,32.3195190429688,32.3195190429688,32.3195190429688,32.3195190429688,32.3195190429688,32.3195190429688,32.3195190429688,32.9321899414062,32.9321899414062,32.9321899414062,32.9321899414062,32.9321899414062,32.9321899414062,32.9321899414062,33.5457916259766,33.5457916259766,33.5457916259766,33.5457916259766,33.5457916259766,33.5457916259766,33.5457916259766,34.1542053222656,34.1542053222656,34.1542053222656,34.1542053222656,34.1542053222656,34.1542053222656,34.1542053222656,34.7568740844727,34.7568740844727,34.7568740844727,34.7568740844727,34.7568740844727,35.3686065673828,35.3686065673828,35.3686065673828,35.3686065673828,35.3686065673828,35.9822158813477,35.9822158813477,35.9822158813477,35.9822158813477,35.9822158813477,36.5972747802734,36.5972747802734,36.5972747802734,36.5972747802734,36.5972747802734,37.2056884765625,37.2056884765625,37.2056884765625,37.2056884765625,37.2056884765625,37.8192901611328,37.8192901611328,37.8192901611328,37.8192901611328,37.8192901611328,27.6670227050781,27.6670227050781,27.6670227050781,27.6670227050781,27.6670227050781,27.6670227050781,27.6670227050781,27.6670227050781,28.4045181274414,28.4045181274414,28.4045181274414,28.4045181274414,28.4045181274414,28.4045181274414,29.3753662109375,29.3753662109375,29.988037109375,29.988037109375,29.988037109375,29.988037109375,29.988037109375,29.988037109375,29.988037109375,30.600830078125,30.600830078125,31.3517913818359,31.3517913818359,31.3517913818359,31.3517913818359,31.3517913818359,31.3517913818359,31.3517913818359,31.3517913818359,31.3517913818359,31.3517913818359,31.3517913818359,31.9527053833008,31.9527053833008,31.9527053833008,31.9527053833008,31.9527053833008,31.9527053833008,31.9527053833008,32.6229705810547,32.6229705810547,32.6229705810547,32.6229705810547,32.6229705810547,32.6229705810547,32.6229705810547,32.6229705810547,33.2656784057617,33.2656784057617,33.2656784057617,33.2656784057617,33.2656784057617,33.2656784057617,33.2656784057617,33.9217987060547,33.9217987060547,34.5804061889648,34.5804061889648,34.5804061889648,34.5804061889648,34.5804061889648,34.5804061889648,34.5804061889648,34.5804061889648,34.5804061889648,34.5804061889648,34.5804061889648,35.2948150634766,35.2948150634766,35.2948150634766,35.2948150634766,35.2948150634766,35.9560394287109,35.9560394287109,35.9560394287109,35.9560394287109,35.9560394287109,35.9560394287109,35.9560394287109,35.9560394287109,35.9560394287109,36.9386596679688,36.9386596679688,36.9386596679688,37.5314025878906,37.5314025878906,37.5314025878906,37.5314025878906,37.5314025878906,37.5314025878906,27.2070541381836,27.2070541381836,27.2070541381836,27.2070541381836,27.2070541381836,27.2070541381836,27.2070541381836,27.428840637207,27.428840637207,27.428840637207,27.428840637207,27.428840637207,27.428840637207,27.428840637207,27.428840637207,27.428840637207,27.428840637207,27.428840637207,27.428840637207,27.428840637207,28.1190414428711,28.1190414428711,28.7913589477539,28.7913589477539,28.7913589477539,28.7913589477539,28.7913589477539,29.6809997558594,29.6809997558594,29.6809997558594,29.6809997558594,29.6809997558594,29.6809997558594,29.6809997558594,29.6809997558594,30.8897094726562,30.8897094726562,31.4704818725586,31.4704818725586,31.4704818725586,31.4704818725586,31.4704818725586,31.4704818725586,31.4704818725586,31.4704818725586,32.2887725830078,32.2887725830078,32.2887725830078,32.2887725830078,32.2887725830078,32.2887725830078,32.2887725830078,32.8262023925781,32.8262023925781,32.8262023925781,32.8262023925781,32.8262023925781,32.8262023925781,32.8262023925781,33.3791732788086,33.3791732788086,33.3791732788086,33.3791732788086,33.3791732788086,33.3791732788086,34.2759857177734,34.2759857177734,34.2759857177734,34.2759857177734,34.2759857177734,34.2759857177734,34.2759857177734,34.2759857177734,34.2759857177734,34.8647155761719,34.8647155761719,35.4099731445312,35.4099731445312,35.4099731445312,35.4099731445312,35.4099731445312,35.4099731445312,35.4099731445312,35.4099731445312,35.4099731445312,35.4099731445312,35.9742965698242,35.9742965698242,36.5022888183594,36.5022888183594,36.5022888183594,36.5022888183594,36.5022888183594,36.5022888183594,36.5022888183594,37.2040710449219,37.2040710449219,37.2040710449219,37.2040710449219,37.2040710449219,37.2040710449219,37.2040710449219,37.2040710449219,37.2040710449219,27.2999877929688,27.2999877929688,27.2999877929688,27.2999877929688,27.2999877929688,28.3296661376953,28.3296661376953,28.3296661376953,28.3296661376953,28.3296661376953,28.3296661376953,28.3296661376953,28.3296661376953,28.3296661376953,28.9351806640625,28.9351806640625,28.9351806640625,28.9351806640625,28.9351806640625,28.9351806640625,28.9351806640625,28.9351806640625,28.9351806640625,28.9351806640625,29.8823776245117,29.8823776245117,31.0951309204102,31.0951309204102,31.0951309204102,31.0951309204102,31.0951309204102,31.0951309204102,32.0054702758789,32.0054702758789,32.0054702758789,32.0054702758789,32.0054702758789,32.0054702758789,32.0054702758789,32.9037246704102,32.9037246704102,32.9037246704102,32.9037246704102,32.9037246704102,32.9037246704102,32.9037246704102,32.9037246704102,33.519157409668,33.519157409668,33.519157409668,33.519157409668,33.519157409668,33.519157409668,33.519157409668,33.519157409668,34.696159362793,34.696159362793,34.696159362793,34.696159362793,34.696159362793,34.696159362793,34.696159362793,34.696159362793,35.633415222168,35.633415222168,35.633415222168,35.633415222168,35.633415222168,35.633415222168,35.633415222168,35.633415222168,36.2071304321289,36.2071304321289,37.1081085205078,37.1081085205078,37.1081085205078,37.1081085205078,37.1081085205078,37.1081085205078,37.1081085205078,37.1081085205078,37.1081085205078,37.1081085205078,37.1081085205078,27.223258972168,27.223258972168,27.223258972168,27.223258972168,27.223258972168,27.223258972168,27.223258972168,27.223258972168,27.223258972168,27.8945693969727,27.8945693969727,27.8945693969727,27.8945693969727,27.8945693969727,27.8945693969727,27.8945693969727,28.5991744995117,28.5991744995117,28.5991744995117,28.5991744995117,28.5991744995117,28.5991744995117,28.5991744995117,28.5991744995117,29.4938430786133,29.4938430786133,29.4938430786133,29.4938430786133,29.4938430786133,30.1062469482422,30.1062469482422,30.1062469482422,30.1062469482422,30.1062469482422,30.9966430664062,30.9966430664062,30.9966430664062,30.9966430664062,30.9966430664062,30.9966430664062,30.9966430664062,30.9966430664062,30.9966430664062,30.9966430664062,30.9966430664062,31.7293319702148,31.7293319702148,31.7293319702148,31.7293319702148,31.7293319702148,31.7293319702148,31.7293319702148,31.7293319702148,31.7293319702148,31.7293319702148,31.7293319702148,31.7293319702148,31.7293319702148,32.5734939575195,32.5734939575195,32.5734939575195,32.5734939575195,32.5734939575195,32.5734939575195,32.5734939575195,32.5734939575195,32.5734939575195,32.5734939575195,32.5734939575195,33.4080657958984,33.4080657958984,33.4080657958984,33.4080657958984,33.4080657958984,34.3054122924805,34.3054122924805,34.3054122924805,34.3054122924805,34.3054122924805,34.7659225463867,34.7659225463867,34.7659225463867,34.7659225463867,34.7659225463867,34.7659225463867,34.7659225463867,35.5063934326172,35.5063934326172,35.5063934326172,35.5063934326172,35.5063934326172,35.5063934326172,35.5063934326172,35.5063934326172,35.5063934326172,35.5063934326172,36.0793075561523,36.0793075561523,36.0793075561523,36.0793075561523,36.0793075561523,36.0793075561523,36.0793075561523,36.0793075561523,36.646240234375,36.646240234375,36.646240234375,36.646240234375,36.646240234375,36.646240234375,37.5646743774414,37.5646743774414,37.5646743774414,37.5646743774414,37.5646743774414,37.5646743774414,37.5646743774414,27.6211013793945,27.6211013793945,27.6211013793945,27.6211013793945,27.6211013793945,27.6211013793945,27.6211013793945,27.6211013793945,28.4282302856445,28.4282302856445,28.4282302856445,28.4282302856445,28.4282302856445,28.4282302856445,28.4282302856445,28.4282302856445,29.1147766113281,29.1147766113281,29.1147766113281,29.1147766113281,29.1147766113281,29.1147766113281,29.1147766113281,29.1147766113281,29.9477233886719,29.9477233886719,29.9477233886719,29.9477233886719,29.9477233886719,29.9477233886719,29.9477233886719,30.5728607177734,30.5728607177734,30.5728607177734,30.5728607177734,30.5728607177734,30.5728607177734,30.5728607177734,30.5728607177734,30.5728607177734,30.5728607177734,30.5728607177734,31.208869934082,31.208869934082,32.0680770874023,32.0680770874023,32.0680770874023,32.0680770874023,32.0680770874023,32.7877883911133,32.7877883911133,32.7877883911133,32.7877883911133,32.7877883911133,32.7877883911133,32.7877883911133,32.7877883911133,33.3422775268555,33.3422775268555,33.3422775268555,33.3422775268555,33.3422775268555,33.3422775268555,33.3422775268555,33.3422775268555,33.3422775268555,33.9688339233398,33.9688339233398,33.9688339233398,33.9688339233398,33.9688339233398,33.9688339233398,34.8883285522461,34.8883285522461,34.8883285522461,34.8883285522461,34.8883285522461,34.8883285522461,35.5621643066406,35.5621643066406,35.5621643066406,35.5621643066406,35.5621643066406,36.1539993286133,36.1539993286133,36.1539993286133,36.1539993286133,36.1539993286133,36.7246780395508,36.7246780395508,36.7246780395508,36.7246780395508,36.7246780395508,36.7246780395508,36.7246780395508,36.7246780395508,37.3018646240234,37.3018646240234,37.3018646240234,37.3018646240234,37.3018646240234,37.8377456665039,37.8377456665039,37.8377456665039,37.8377456665039,37.8377456665039,37.8377456665039,37.8377456665039,37.8377456665039,27.6940765380859,27.6940765380859,28.2678604125977,28.2678604125977,28.8524703979492,28.8524703979492,28.8524703979492,28.8524703979492,28.8524703979492,29.8150329589844,29.8150329589844,29.8150329589844,29.8150329589844,29.8150329589844,29.8150329589844,29.8150329589844,29.8150329589844,29.8150329589844,29.8150329589844,30.4964828491211,30.4964828491211,30.4964828491211,30.4964828491211,30.4964828491211,30.4964828491211,31.0961532592773,31.0961532592773,31.0961532592773,31.0961532592773,31.0961532592773,31.0961532592773,31.0961532592773,31.0961532592773,31.0961532592773,31.7005310058594,31.7005310058594,31.7005310058594,31.7005310058594,31.7005310058594,31.7005310058594,31.7005310058594,32.4230117797852,32.4230117797852,32.4230117797852,32.4230117797852,32.4230117797852,32.4230117797852,33.4593048095703,33.4593048095703,34.6663360595703,34.6663360595703,34.6663360595703,34.6663360595703,34.6663360595703,34.6663360595703,34.6663360595703,35.2682647705078,35.2682647705078,35.2682647705078,35.2682647705078,35.2682647705078,35.2682647705078,35.2682647705078,35.2682647705078,35.9101486206055,35.9101486206055,35.9101486206055,35.9101486206055,35.9101486206055,35.9101486206055,35.9101486206055,35.9101486206055,37.0654754638672,37.0654754638672,37.0654754638672,37.0654754638672,37.0654754638672,37.0654754638672,37.0654754638672,37.0654754638672,37.0654754638672,37.0654754638672,37.0654754638672,27.247802734375,27.247802734375,27.247802734375,27.247802734375,27.247802734375,27.247802734375,27.247802734375,27.247802734375,27.247802734375,27.247802734375,27.247802734375,28.0778961181641,28.0778961181641,28.0778961181641,28.0778961181641,28.0778961181641,28.0778961181641,28.0778961181641,28.0778961181641,28.6901397705078,28.6901397705078,28.6901397705078,28.6901397705078,28.6901397705078,28.6901397705078,28.6901397705078,29.349250793457,29.349250793457,29.349250793457,29.349250793457,29.349250793457,29.349250793457,29.349250793457,29.9841156005859,29.9841156005859,29.9841156005859,29.9841156005859,29.9841156005859,29.9841156005859,30.577522277832,30.577522277832,30.577522277832,30.577522277832,30.577522277832,30.577522277832,30.577522277832,30.577522277832,30.577522277832,31.5790328979492,31.5790328979492,31.5790328979492,31.5790328979492,31.5790328979492,31.5790328979492,31.5790328979492,32.2174987792969,32.2174987792969,32.2174987792969,32.2174987792969,32.2174987792969,32.2174987792969,32.2174987792969,33.108512878418,33.108512878418,33.108512878418,33.108512878418,33.108512878418,33.108512878418,33.108512878418,33.7755355834961,33.7755355834961,34.552619934082,34.552619934082,34.552619934082,34.552619934082,34.552619934082,34.552619934082,34.552619934082,34.552619934082,35.7342071533203,35.7342071533203,35.7342071533203,35.7342071533203,35.7342071533203,35.7342071533203,35.7342071533203,35.7342071533203,36.4174575805664,36.4174575805664,36.4174575805664,37.3837432861328,37.3837432861328,37.3837432861328,37.3837432861328,37.3837432861328,37.3837432861328,37.3837432861328,37.3837432861328,37.8622360229492,37.8622360229492,37.8622360229492,28.039665222168,28.039665222168,28.039665222168,28.039665222168,28.039665222168,28.039665222168,28.039665222168,28.039665222168,28.6511154174805,28.6511154174805,28.6511154174805,28.6511154174805,28.6511154174805,28.6511154174805,28.6511154174805,28.6511154174805,28.6511154174805,28.6511154174805,28.6511154174805,29.8754577636719,29.8754577636719,29.8754577636719,29.8754577636719,29.8754577636719,29.8754577636719,29.8754577636719,29.8754577636719,29.8754577636719,29.8754577636719,29.8754577636719,29.8754577636719,30.661735534668,30.661735534668,30.661735534668,30.661735534668,30.661735534668,30.661735534668,30.661735534668,31.6265029907227,31.6265029907227,31.6265029907227,31.6265029907227,31.6265029907227,31.6265029907227,31.6265029907227,31.6265029907227,31.6265029907227,31.6265029907227,32.3220367431641,32.3220367431641,32.3220367431641,32.3220367431641,32.3220367431641,32.9119033813477,32.9119033813477,32.9119033813477,32.9119033813477,32.9119033813477,33.5208969116211,33.5208969116211,33.5208969116211,33.5208969116211,33.5208969116211,33.5208969116211,33.5208969116211,33.5208969116211,33.5208969116211,33.5208969116211,34.7305755615234,34.7305755615234,35.7129440307617,35.7129440307617,35.7129440307617,35.7129440307617,35.7129440307617,36.3522033691406,36.3522033691406,36.3522033691406,36.3522033691406,36.3522033691406,36.3522033691406,36.3522033691406,36.3522033691406,36.3522033691406,36.3522033691406,36.3522033691406,36.3522033691406,36.3522033691406,37.1163101196289,37.1163101196289,37.1163101196289,37.1163101196289,37.1163101196289,37.1163101196289,37.1163101196289,37.1163101196289,37.1163101196289,37.7075805664062,37.7075805664062,37.7075805664062,37.7075805664062,37.7075805664062,37.7075805664062,27.917350769043,27.917350769043,29.1367263793945,29.1367263793945,29.1367263793945,29.1367263793945,29.1367263793945,29.1367263793945,29.1367263793945,29.1367263793945,29.7766189575195,29.7766189575195,29.7766189575195,29.7766189575195,29.7766189575195,29.7766189575195,29.7766189575195,29.7766189575195,30.4505310058594,30.4505310058594,30.4505310058594,30.4505310058594,30.4505310058594,30.4505310058594,30.4505310058594,30.4505310058594,31.2214202880859,31.2214202880859,31.8943023681641,31.8943023681641,31.8943023681641,31.8943023681641,31.8943023681641,31.8943023681641,31.8943023681641,32.4967575073242,32.4967575073242,32.4967575073242,32.4967575073242,32.4967575073242,33.1103668212891,33.1103668212891,33.1103668212891,33.1103668212891,33.1103668212891,33.7230377197266,33.7230377197266,33.7230377197266,33.7230377197266,33.7230377197266,34.3145217895508,34.3145217895508,34.3145217895508,34.3145217895508,34.3145217895508,34.3145217895508,34.3145217895508,34.8943405151367,34.8943405151367,34.8943405151367,34.8943405151367,34.8943405151367,35.4868545532227,35.4868545532227,35.4868545532227,35.4868545532227,35.4868545532227,35.4868545532227,35.4868545532227,36.4014663696289,36.4014663696289,37.0416488647461,37.0416488647461,37.7011337280273,37.7011337280273,37.7011337280273,37.7011337280273,37.7011337280273,37.7011337280273,37.7011337280273,37.7011337280273,37.7011337280273,37.7011337280273,28.0588989257812,28.0588989257812,29.2649688720703,29.2649688720703,29.2649688720703,29.2649688720703,29.2649688720703,29.2649688720703,29.2649688720703,29.2649688720703,30.1630325317383,30.1630325317383,30.1630325317383,30.1630325317383,30.1630325317383,30.1630325317383,30.774040222168,30.774040222168,30.774040222168,30.774040222168,30.774040222168,31.9726486206055,31.9726486206055,31.9726486206055,31.9726486206055,31.9726486206055,31.9726486206055,31.9726486206055,32.5727767944336,32.5727767944336,32.5727767944336,32.5727767944336,32.5727767944336,32.5727767944336,32.5727767944336,33.2247009277344,33.2247009277344,33.2247009277344,33.2247009277344,33.2247009277344,33.2247009277344,33.2247009277344,34.3685531616211,34.3685531616211,34.3685531616211,34.3685531616211,34.3685531616211,34.3685531616211,34.3685531616211,35.494514465332,35.494514465332,35.494514465332,35.494514465332,35.494514465332,36.083366394043,36.083366394043,36.083366394043,36.083366394043,36.083366394043,36.083366394043,36.083366394043,36.083366394043,36.083366394043,36.6835098266602,36.6835098266602,36.6835098266602,36.6835098266602,36.6835098266602,36.6835098266602,36.6835098266602,36.6835098266602,37.2766799926758,37.2766799926758,37.2766799926758,37.2766799926758,37.2766799926758,37.2766799926758,27.4386138916016,27.4386138916016,27.4386138916016,28.4461441040039,28.4461441040039,28.4461441040039,28.4461441040039,28.4461441040039,28.4461441040039,29.1199340820312,29.1199340820312,29.1199340820312,29.1199340820312,29.1199340820312,29.1199340820312,29.1199340820312,29.8916931152344,29.8916931152344,29.8916931152344,29.8916931152344,29.8916931152344,29.8916931152344,29.8916931152344,30.4904479980469,30.4904479980469,30.4904479980469,30.4904479980469,30.4904479980469,30.4904479980469,30.4904479980469,30.4904479980469,30.4904479980469,31.1794281005859,31.1794281005859,31.1794281005859,31.1794281005859,31.1794281005859,31.1794281005859,31.1794281005859,32.1380615234375,32.1380615234375,32.1380615234375,32.1380615234375,32.1380615234375,32.1380615234375,32.1380615234375,32.1380615234375,32.1380615234375,32.1380615234375,32.954704284668,32.954704284668,32.954704284668,32.954704284668,32.954704284668,32.954704284668,32.954704284668,33.5805358886719,33.5805358886719,33.5805358886719,33.5805358886719,33.5805358886719,33.5805358886719,33.5805358886719,33.5805358886719,33.5805358886719,33.5805358886719,34.415153503418,34.415153503418,34.415153503418,34.415153503418,34.415153503418,34.415153503418,34.415153503418,35.08056640625,35.08056640625,35.7306976318359,35.7306976318359,35.7306976318359,35.7306976318359,35.7306976318359,35.7306976318359,35.7306976318359,36.5482177734375,36.5482177734375,36.5482177734375,36.5482177734375,36.5482177734375,36.5482177734375,36.5482177734375,36.5482177734375,37.1690063476562,37.1690063476562,37.1690063476562,37.1690063476562,37.1690063476562,37.1690063476562,37.1690063476562,37.1690063476562,37.1690063476562,37.1690063476562,27.374137878418,27.374137878418,27.374137878418,27.374137878418,27.374137878418,28.0043487548828,28.0043487548828,28.0043487548828,28.0043487548828,28.0043487548828,28.7742538452148,28.7742538452148,28.7742538452148,28.7742538452148,28.7742538452148,28.7742538452148,28.7742538452148,29.4284515380859,29.4284515380859,30.0419464111328,30.0419464111328,30.0419464111328,30.0419464111328,30.0419464111328,30.0419464111328,30.6511383056641,30.6511383056641,30.6511383056641,30.6511383056641,30.6511383056641,30.6511383056641,30.6511383056641,30.6511383056641,30.6511383056641,30.6511383056641,30.6511383056641,31.3125534057617,31.3125534057617,31.3125534057617,31.3125534057617,31.3125534057617,32.3036804199219,32.3036804199219,32.3036804199219,32.3036804199219,32.3036804199219,32.3036804199219,32.3036804199219,32.3036804199219,32.3036804199219,32.3036804199219,32.3036804199219,32.3036804199219,33.0580215454102,33.0580215454102,33.0580215454102,33.0580215454102,33.0580215454102,33.0580215454102,33.0580215454102,33.0580215454102,33.0580215454102,33.0580215454102,33.0580215454102,33.805419921875,33.805419921875,33.805419921875,33.805419921875,33.805419921875,34.7690582275391,34.7690582275391,34.7690582275391,34.7690582275391,34.7690582275391,34.7690582275391,34.7690582275391,34.7690582275391,34.7690582275391,34.7690582275391,34.7690582275391,35.4999847412109,35.4999847412109,35.4999847412109,35.4999847412109,35.4999847412109,35.4999847412109,35.4999847412109,36.181884765625,36.181884765625,36.181884765625,36.181884765625,36.181884765625,36.181884765625,36.181884765625,36.181884765625,37.3685302734375,37.3685302734375,37.3685302734375,37.3685302734375,37.3685302734375,27.373664855957,27.373664855957,27.373664855957,27.373664855957,27.373664855957,27.373664855957,27.373664855957,27.373664855957,28.5984420776367,28.5984420776367,28.5984420776367,28.5984420776367,28.5984420776367,28.5984420776367,28.5984420776367,29.8196792602539,29.8196792602539,29.8196792602539,29.8196792602539,29.8196792602539,29.8196792602539,29.8196792602539,29.8196792602539,29.8196792602539,29.8196792602539,30.4233169555664,30.4233169555664,30.4233169555664,30.4233169555664,30.4233169555664,30.4233169555664,30.4233169555664,30.4233169555664,30.4233169555664,30.4233169555664,31.6448822021484,31.6448822021484,31.6448822021484,31.6448822021484,31.6448822021484,32.2553405761719,32.2553405761719,32.8445129394531,32.8445129394531,33.4972229003906,33.4972229003906,33.4972229003906,33.4972229003906,33.4972229003906,33.4972229003906,33.4972229003906,33.4972229003906,34.5354385375977,34.5354385375977,34.5354385375977,34.5354385375977,34.5354385375977,34.5354385375977,34.5354385375977,34.5354385375977,34.5354385375977,34.5354385375977,35.1428527832031,35.1428527832031,35.1428527832031,35.1428527832031,35.671257019043,35.671257019043,35.671257019043,35.671257019043,35.671257019043,35.671257019043,35.671257019043,36.4330139160156,36.4330139160156,36.4330139160156,36.4330139160156,36.4330139160156,36.4330139160156,37.0397872924805,37.0397872924805,37.0397872924805,37.0397872924805,37.0397872924805,37.0397872924805,37.0397872924805,37.0397872924805,37.0397872924805,37.0397872924805,37.0397872924805,37.6566162109375,37.6566162109375,37.6566162109375,37.6566162109375,37.6566162109375,37.6566162109375,37.6566162109375,27.5243072509766,27.5243072509766,27.5243072509766,27.5243072509766,27.5243072509766,27.5243072509766,28.1102294921875,28.1102294921875,28.1102294921875,28.1102294921875,28.1102294921875,28.1102294921875,28.1102294921875,28.9872894287109,28.9872894287109,28.9872894287109,28.9872894287109,28.9872894287109,29.5941848754883,29.5941848754883,29.5941848754883,29.5941848754883,29.5941848754883,29.5941848754883,29.5941848754883,29.5941848754883,30.8184967041016,30.8184967041016,30.8184967041016,30.8184967041016,30.8184967041016,30.8184967041016,30.8184967041016,31.8442611694336,31.8442611694336,31.8442611694336,31.8442611694336,31.8442611694336,31.8442611694336,31.8442611694336,31.8442611694336,31.8442611694336,32.6359558105469,32.6359558105469,32.6359558105469,32.6359558105469,32.6359558105469,32.6359558105469,32.6359558105469,32.6359558105469,33.2416534423828,33.2416534423828,33.2416534423828,33.2416534423828,33.2416534423828,33.2416534423828,33.2416534423828,34.1137390136719,34.1137390136719,34.1137390136719,34.1137390136719,34.1137390136719,34.1137390136719,34.1137390136719,34.1137390136719,34.1137390136719,34.1137390136719,34.7210693359375,34.7210693359375,34.7210693359375,34.7210693359375,34.7210693359375,35.3197250366211,35.3197250366211,35.9035873413086,35.9035873413086,35.9035873413086,35.9035873413086,35.9035873413086,36.5238265991211,36.5238265991211,37.1256408691406,37.1256408691406,37.1256408691406,37.1256408691406,37.1256408691406,37.1256408691406,37.1256408691406,37.1256408691406,37.1256408691406,37.1256408691406,37.1256408691406,37.7757110595703,37.7757110595703,37.7757110595703,37.7757110595703,27.8114013671875,27.8114013671875,27.8114013671875,28.5694046020508,28.5694046020508,28.5694046020508,28.5694046020508,28.5694046020508,28.5694046020508,28.5694046020508,28.5694046020508,28.5694046020508,28.5694046020508,28.5694046020508,29.1706848144531,29.1706848144531,29.1706848144531,29.1706848144531,29.1706848144531,29.1706848144531,29.1706848144531,29.1706848144531,29.7942810058594,29.7942810058594,29.7942810058594,29.7942810058594,29.7942810058594,29.7942810058594,29.7942810058594,29.7942810058594,29.7942810058594,29.7942810058594,29.7942810058594,29.7942810058594,29.7942810058594,30.5407409667969,30.5407409667969,30.5407409667969,30.5407409667969,30.5407409667969,30.5407409667969,30.5407409667969,30.5407409667969,31.1550674438477,31.1550674438477,31.1550674438477,31.1550674438477,31.1550674438477,31.1550674438477,31.1550674438477,32.116569519043,32.116569519043,32.116569519043,32.116569519043,32.116569519043,32.116569519043,32.116569519043,32.116569519043,32.116569519043,32.9747848510742,32.9747848510742,32.9747848510742,33.8948974609375,33.8948974609375,33.8948974609375,33.8948974609375,33.8948974609375,33.8948974609375,33.8948974609375,34.482666015625,34.482666015625,34.482666015625,34.482666015625,34.482666015625,34.482666015625,34.482666015625,34.482666015625,34.482666015625,34.482666015625,35.6767120361328,35.6767120361328,35.6767120361328,35.6767120361328,35.6767120361328,35.6767120361328,35.6767120361328,36.2713928222656,36.2713928222656,36.2713928222656,36.2713928222656,36.2713928222656,36.2713928222656,36.2713928222656,36.2713928222656,36.2713928222656,36.2713928222656,36.2713928222656,36.2713928222656,36.2713928222656,36.2713928222656,36.8334732055664,36.8334732055664,36.8334732055664,37.8791198730469,37.8791198730469,37.8791198730469,37.8791198730469,37.8791198730469,37.8791198730469,37.8791198730469,37.8791198730469,28.0858459472656,28.0858459472656,28.0858459472656,28.0858459472656,28.0858459472656,28.0858459472656,28.0858459472656,28.747673034668,28.747673034668,28.747673034668,28.747673034668,28.747673034668,28.747673034668,28.747673034668,28.747673034668,29.3557815551758,29.3557815551758,30.0735931396484,30.0735931396484,30.0735931396484,30.0735931396484,30.0735931396484,30.0735931396484,30.0735931396484,30.0735931396484,30.0735931396484,30.0735931396484,30.0735931396484,30.9040985107422,30.9040985107422,30.9040985107422,30.9040985107422,30.9040985107422,30.9040985107422,31.514762878418,31.514762878418,31.514762878418,31.514762878418,31.514762878418,31.514762878418,31.514762878418,31.514762878418,31.514762878418,31.514762878418,32.7274856567383,32.7274856567383,32.7274856567383,32.7274856567383,32.7274856567383,32.7274856567383,32.7274856567383,32.7274856567383,32.7274856567383,32.7274856567383,33.6628189086914,33.6628189086914,33.6628189086914,33.6628189086914,33.6628189086914,33.6628189086914,33.6628189086914,34.5350952148438,34.5350952148438,35.1470260620117,35.1470260620117,35.1470260620117,35.1470260620117,35.1470260620117,35.1470260620117,35.1470260620117,35.1470260620117,35.1470260620117,35.1470260620117,35.9246978759766,35.9246978759766,35.9246978759766,35.9246978759766,35.9246978759766,35.9246978759766,35.9246978759766,36.5390777587891,36.5390777587891,36.5390777587891,36.5390777587891,36.5390777587891,37.1305313110352,37.1305313110352,37.1305313110352,37.1305313110352,37.1305313110352,37.1305313110352,37.1305313110352,37.7206573486328,37.7206573486328,37.7206573486328,37.7206573486328,37.7206573486328,27.9091796875,27.9091796875,27.9091796875,27.9091796875,27.9091796875,27.9091796875,27.9091796875,27.9091796875,28.5521469116211,28.5521469116211,28.5521469116211,28.5521469116211,28.5521469116211,28.5521469116211,28.5521469116211,28.5521469116211,28.5521469116211,28.5521469116211,28.5521469116211,29.2976226806641,29.2976226806641,29.2976226806641,29.9223480224609,29.9223480224609,29.9223480224609,29.9223480224609,29.9223480224609,29.9223480224609,29.9223480224609,29.9223480224609,30.5321960449219,30.5321960449219,30.5321960449219,30.5321960449219,30.5321960449219,30.5321960449219,30.5321960449219,30.5321960449219,30.5321960449219,30.5321960449219,31.3533935546875,31.3533935546875,31.3533935546875,31.3533935546875,31.3533935546875,31.3533935546875,31.3533935546875,31.3533935546875,31.3533935546875,32.0110015869141,32.0110015869141,33.030632019043,33.030632019043,33.030632019043,33.030632019043,33.030632019043,33.030632019043,33.030632019043,33.7719192504883,33.7719192504883,33.7719192504883,33.7719192504883,33.7719192504883,33.7719192504883,33.7719192504883,33.7719192504883,33.7719192504883,34.3717727661133,34.3717727661133,34.3717727661133,34.3717727661133,34.3717727661133,34.3717727661133,34.3717727661133,34.3717727661133,34.3717727661133,34.3717727661133,35.0382080078125,35.0382080078125,35.0382080078125,35.0382080078125,35.0382080078125,35.0382080078125,35.0382080078125,35.0382080078125,35.0382080078125,35.0382080078125,35.6386337280273,35.6386337280273,35.6386337280273,35.6386337280273,35.6386337280273,35.6386337280273,35.6386337280273,36.2336502075195,36.2336502075195,36.2336502075195,36.2336502075195,36.2336502075195,37.4321823120117,37.4321823120117,37.4321823120117,37.4321823120117,37.4321823120117,37.4321823120117,27.8897018432617,27.8897018432617,27.8897018432617,27.8897018432617,27.8897018432617,27.8897018432617,27.8897018432617,28.6325225830078,28.6325225830078,29.8545761108398,29.8545761108398,29.8545761108398,29.8545761108398,29.8545761108398,29.8545761108398,29.8545761108398,30.4717407226562,30.4717407226562,30.4717407226562,30.4717407226562,30.4717407226562,30.4717407226562,30.4717407226562,30.4717407226562,31.1680603027344,31.1680603027344,31.1680603027344,31.1680603027344,31.1680603027344,31.1680603027344,31.1680603027344,31.1680603027344,31.1680603027344,31.1680603027344,31.1680603027344,31.1680603027344,31.1680603027344,32.002311706543,32.002311706543,32.002311706543,32.002311706543,32.002311706543,32.002311706543,32.002311706543,32.002311706543,32.002311706543,32.002311706543,33.2127838134766,33.2127838134766,33.2127838134766,33.2127838134766,33.2127838134766,33.2127838134766,34.0569839477539,34.0569839477539,34.6592025756836,34.6592025756836,34.6592025756836,34.6592025756836,34.6592025756836,34.6592025756836,34.6592025756836,34.6592025756836,34.6592025756836,34.6592025756836,35.8556594848633,35.8556594848633,35.8556594848633,35.8556594848633,35.8556594848633,35.8556594848633,35.8556594848633,36.4949188232422,36.4949188232422,37.654411315918,37.654411315918,37.654411315918,37.654411315918,37.654411315918,37.654411315918,37.654411315918,28.1306610107422,28.1306610107422,28.7432327270508,28.7432327270508,28.7432327270508,28.7432327270508,28.7432327270508,28.7432327270508,28.7432327270508,28.7432327270508,28.7432327270508,29.4707489013672,29.4707489013672,29.4707489013672,29.4707489013672,29.4707489013672,29.4707489013672,29.4707489013672,29.4707489013672,29.4707489013672,29.4707489013672,29.4707489013672,29.4707489013672,30.6922378540039,30.6922378540039,30.6922378540039,30.6922378540039,31.3044204711914,31.3044204711914,31.3044204711914,31.3044204711914,31.3044204711914,31.3044204711914,32.1199111938477,32.1199111938477,32.1199111938477,32.1199111938477,32.1199111938477,32.7123413085938,32.7123413085938,32.7123413085938,32.7123413085938,32.7123413085938,32.7123413085938,32.7123413085938,32.7123413085938,33.5364608764648,33.5364608764648,33.5364608764648,34.1339492797852,34.1339492797852,34.1339492797852,34.1339492797852,34.1339492797852,34.8119430541992,34.8119430541992,34.8119430541992,34.8119430541992,34.8119430541992,34.8119430541992,34.8119430541992,35.4392242431641,35.4392242431641,35.4392242431641,35.4392242431641,35.4392242431641,35.4392242431641,36.0385665893555,36.0385665893555,36.7502822875977,36.7502822875977,36.7502822875977,36.7502822875977,37.3099517822266,37.3099517822266,37.3099517822266,37.3099517822266,37.3099517822266,27.3527297973633,27.3527297973633,27.3527297973633,27.3527297973633,27.3527297973633,27.3527297973633,27.9964752197266,27.9964752197266,27.9964752197266,27.9964752197266,27.9964752197266,27.9964752197266,27.9964752197266,27.9964752197266,27.9964752197266,27.9964752197266,27.9964752197266,28.9571533203125,28.9571533203125,29.7071380615234,29.7071380615234,29.7071380615234,29.7071380615234,29.7071380615234,29.7071380615234,29.7071380615234,29.7071380615234,30.3098831176758,30.3098831176758,30.3098831176758,30.3098831176758,30.3098831176758,30.3098831176758,30.3098831176758,30.3098831176758,30.8888854980469,30.8888854980469,30.8888854980469,30.8888854980469,30.8888854980469,30.8888854980469,30.8888854980469,30.8888854980469,31.5413513183594,31.5413513183594,31.5413513183594,31.5413513183594,31.5413513183594,31.5413513183594,31.5413513183594,31.5413513183594,31.5413513183594,32.2288665771484,32.2288665771484,32.2288665771484,32.2288665771484,32.2288665771484,32.2288665771484,32.8803405761719,32.8803405761719,32.8803405761719,32.8803405761719,32.8803405761719,32.8803405761719,33.9422302246094,33.9422302246094,34.5433578491211,34.5433578491211,34.5433578491211,34.5433578491211,34.5433578491211,34.5433578491211,34.5433578491211,34.5433578491211,35.1549377441406,35.1549377441406,35.1549377441406,35.1549377441406,35.1549377441406,35.1549377441406,35.1549377441406,35.1549377441406,35.8175811767578,35.8175811767578,35.8175811767578,36.7023391723633,36.7023391723633,36.7023391723633,36.7023391723633,36.7023391723633,36.7023391723633,36.7023391723633,37.3161468505859,37.3161468505859,37.3161468505859,37.3161468505859,37.3161468505859,37.3161468505859,37.3161468505859,37.8859405517578,37.8859405517578,37.8859405517578,37.8859405517578,37.8859405517578,37.8859405517578,37.8859405517578,27.8910369873047,27.8910369873047,27.8910369873047,27.8910369873047,27.8910369873047,27.8910369873047,27.8910369873047,27.8910369873047,27.8910369873047,27.8910369873047,28.496940612793,28.496940612793,28.496940612793,28.496940612793,28.496940612793,29.1113510131836,29.1113510131836,29.1113510131836,29.1113510131836,29.1113510131836,29.1113510131836,29.1113510131836,29.1113510131836,29.8601455688477,29.8601455688477,29.8601455688477,29.8601455688477,29.8601455688477,29.8601455688477,29.8601455688477,29.8601455688477,30.5713424682617,30.5713424682617,30.5713424682617,30.5713424682617,30.5713424682617,30.5713424682617,30.5713424682617,30.5713424682617,30.5713424682617,30.5713424682617,31.2261657714844,31.2261657714844,31.7943725585938,31.7943725585938,31.7943725585938,31.7943725585938,31.7943725585938,31.7943725585938,31.7943725585938,31.7943725585938,31.7943725585938,31.7943725585938,31.7943725585938,32.7178726196289,32.7178726196289,32.7178726196289,32.7178726196289,32.7178726196289,32.7178726196289,32.7178726196289,33.361457824707,33.361457824707,33.361457824707,33.361457824707,33.361457824707,33.361457824707,33.361457824707,33.361457824707,33.361457824707,33.9386749267578,33.9386749267578,33.9386749267578,33.9386749267578,33.9386749267578,33.9386749267578,33.9386749267578,33.9386749267578,33.9386749267578,33.9386749267578,33.9386749267578,34.6727523803711,34.6727523803711,34.6727523803711,34.6727523803711,34.6727523803711,34.6727523803711,34.6727523803711,34.6727523803711,34.6727523803711,34.6727523803711,35.3536071777344,35.3536071777344,35.3536071777344,35.3536071777344,35.3536071777344,35.9569854736328,35.9569854736328,35.9569854736328,35.9569854736328,35.9569854736328,35.9569854736328,35.9569854736328,35.9569854736328,35.9569854736328,36.6872253417969,36.6872253417969,36.6872253417969,36.6872253417969,36.6872253417969,36.6872253417969,36.6872253417969,36.6872253417969,36.6872253417969,36.6872253417969,37.3564453125,37.3564453125,37.3564453125,37.3564453125,37.3564453125,37.3564453125,37.8918228149414,37.8918228149414,37.8918228149414,37.8918228149414,37.8918228149414,37.8918228149414,37.8918228149414,27.8430023193359,27.8430023193359,27.8430023193359,27.8430023193359,27.8430023193359,28.6574630737305,28.6574630737305,28.6574630737305,28.6574630737305,28.6574630737305,28.6574630737305,28.6574630737305,29.3333435058594,29.3333435058594,29.3333435058594,29.3333435058594,29.3333435058594,29.3333435058594,29.3333435058594,29.3333435058594,29.3333435058594,29.3333435058594,29.3333435058594,29.9734191894531,29.9734191894531,29.9734191894531,29.9734191894531,29.9734191894531,29.9734191894531,29.9734191894531,29.9734191894531,30.8157348632812,30.8157348632812,31.4051666259766,31.4051666259766,31.4051666259766,31.4051666259766,31.4051666259766,31.4051666259766,31.4051666259766,31.4051666259766,32.0118789672852,32.0118789672852,32.0118789672852,32.0118789672852,32.0118789672852,32.0118789672852,32.0118789672852,32.8033828735352,32.8033828735352,32.8033828735352,32.8033828735352,32.8033828735352,32.8033828735352,32.8033828735352,32.8033828735352,32.8033828735352,33.6180648803711,33.6180648803711,33.6180648803711,33.6180648803711,33.6180648803711,33.6180648803711,33.6180648803711,33.6180648803711,34.3235168457031,34.3235168457031,34.3235168457031,34.3235168457031,34.3235168457031,34.3235168457031,34.3235168457031,34.3235168457031,34.3235168457031,34.3235168457031,34.3235168457031,35.1626815795898,35.1626815795898,35.1626815795898,35.1626815795898,35.1626815795898,35.1626815795898,35.1626815795898,35.1626815795898,35.738639831543,35.738639831543,35.738639831543,35.738639831543,35.738639831543,35.738639831543,36.6375732421875,36.6375732421875,36.6375732421875,36.6375732421875,36.6375732421875,37.6183471679688,37.6183471679688,37.6183471679688,37.6183471679688,37.6183471679688,37.6183471679688,37.6183471679688,27.5996246337891,27.5996246337891,27.5996246337891,27.5996246337891,27.5996246337891,27.5996246337891,27.5996246337891,27.5996246337891,27.5996246337891,27.5996246337891,27.5996246337891,27.5996246337891,27.5996246337891,28.184928894043,28.184928894043,28.184928894043,28.184928894043,28.184928894043,28.184928894043,28.184928894043,28.184928894043,29.0441741943359,29.0441741943359,29.0441741943359,29.0441741943359,29.0441741943359,29.0441741943359,29.0441741943359,29.0441741943359,29.0441741943359,29.0441741943359,29.0441741943359,29.0441741943359,29.9667663574219,29.9667663574219,29.9667663574219,29.9667663574219,29.9667663574219,29.9667663574219,29.9667663574219,29.9667663574219,30.5728378295898,30.5728378295898,31.392204284668,31.392204284668,31.392204284668,31.392204284668,31.392204284668,31.392204284668,32.1031723022461,32.1031723022461,32.1031723022461,32.1031723022461,32.1031723022461,32.9447860717773,32.9447860717773,32.9447860717773,32.9447860717773,32.9447860717773,33.6087875366211,33.6087875366211,34.3793716430664,34.3793716430664,34.3793716430664,35.0805358886719,35.0805358886719,35.8720092773438,35.8720092773438,35.8720092773438,35.8720092773438,35.8720092773438,35.8720092773438,35.8720092773438,36.4852676391602,36.4852676391602,36.4852676391602,36.4852676391602,36.4852676391602,36.4852676391602,36.4852676391602,36.4852676391602,36.4852676391602,36.4852676391602,37.3166580200195,37.3166580200195,37.3166580200195,37.3166580200195,37.3166580200195,37.3166580200195,37.3166580200195,37.3166580200195,37.3166580200195,37.3166580200195,37.3166580200195,37.3166580200195,37.8120193481445,37.8120193481445,37.8120193481445,37.8120193481445,37.8120193481445,37.8120193481445,37.8120193481445,28.2017593383789,28.2017593383789,28.2017593383789,28.2017593383789,28.2017593383789,28.2017593383789,28.2017593383789,28.8581771850586,28.8581771850586,28.8581771850586,28.8581771850586,28.8581771850586,28.8581771850586,28.8581771850586,29.5107192993164,29.5107192993164,29.5107192993164,29.5107192993164,29.5107192993164,29.5107192993164,29.5107192993164,30.2087707519531,30.2087707519531,31.1910629272461,31.1910629272461,31.1910629272461,31.1910629272461,31.1910629272461,31.1910629272461,31.1910629272461,31.835807800293,31.835807800293,31.835807800293,31.835807800293,32.5450897216797,32.5450897216797,32.5450897216797,32.5450897216797,32.5450897216797,32.5450897216797,32.5450897216797,32.5450897216797,33.5112380981445,33.5112380981445,33.5112380981445,33.5112380981445,33.5112380981445,33.5112380981445,33.5112380981445,33.5112380981445,33.5112380981445,33.5112380981445,34.3230361938477,34.3230361938477,34.3230361938477,34.3230361938477,34.3230361938477,34.9941558837891,34.9941558837891,34.9941558837891,34.9941558837891,34.9941558837891,34.9941558837891,34.9941558837891,34.9941558837891,35.6155624389648,35.6155624389648,35.6155624389648,35.6155624389648,35.6155624389648,35.6155624389648,35.6155624389648,36.2963638305664,36.2963638305664,36.2963638305664,36.2963638305664,36.2963638305664,36.2963638305664,36.2963638305664,36.2963638305664,36.2963638305664,36.2963638305664,36.9508666992188,36.9508666992188,36.9508666992188,36.9508666992188,36.9508666992188,36.9508666992188,36.9508666992188,36.9508666992188,37.61865234375,37.61865234375,37.61865234375,37.61865234375,37.61865234375,37.61865234375,37.61865234375,27.5054168701172,27.5054168701172,27.5054168701172,27.5054168701172,27.5054168701172,27.5054168701172,27.5054168701172,28.0884780883789,28.0884780883789,28.0884780883789,28.0884780883789,28.0884780883789,28.0884780883789,28.0884780883789,28.0884780883789,28.6982345581055,28.6982345581055,28.6982345581055,28.6982345581055,29.5740814208984,29.5740814208984,30.1576156616211,30.1576156616211,30.1576156616211,30.1576156616211,30.1576156616211,30.1576156616211,30.1576156616211,30.1576156616211,30.775032043457,30.775032043457,30.775032043457,30.775032043457,30.775032043457,30.775032043457,30.775032043457,30.775032043457,31.4698638916016,31.4698638916016,32.0755844116211,32.0755844116211,32.0755844116211,33.0321426391602,33.0321426391602,33.0321426391602,33.0321426391602,33.0321426391602,33.0321426391602,33.0321426391602,33.6225662231445,33.6225662231445,33.6225662231445,33.6225662231445,33.6225662231445,33.6225662231445,33.6225662231445,34.3237915039062,34.3237915039062,35.0950164794922,35.0950164794922,35.0950164794922,35.0950164794922,35.0950164794922,35.0950164794922,35.0950164794922,35.683723449707,35.683723449707,35.683723449707,35.683723449707,35.683723449707,35.683723449707,36.2754898071289,36.2754898071289,37.0125579833984,37.0125579833984,37.0125579833984,37.0125579833984,37.0125579833984,37.0125579833984,37.5889587402344,37.5889587402344,37.5889587402344,37.5889587402344,37.5889587402344,27.4236221313477,27.4236221313477,27.4236221313477,27.4236221313477,27.4236221313477,27.4236221313477,27.4236221313477,28.0117721557617,28.0117721557617,28.0117721557617,28.0117721557617,28.0117721557617,28.0117721557617,28.0117721557617,28.0117721557617,28.6669235229492,28.6669235229492,28.6669235229492,28.6669235229492,28.6669235229492,28.6669235229492,28.6669235229492,29.5662307739258,29.5662307739258,29.5662307739258,29.5662307739258,30.4052658081055,30.4052658081055,30.4052658081055,30.4052658081055,30.4052658081055,30.4052658081055,30.4052658081055,30.4052658081055,31.06982421875,31.06982421875,31.06982421875,31.06982421875,31.06982421875,31.06982421875,31.06982421875,31.06982421875,31.6420059204102,31.6420059204102,31.6420059204102,31.6420059204102,31.6420059204102,31.6420059204102,32.2965240478516,32.2965240478516,32.2965240478516,32.2965240478516,32.2965240478516,32.2965240478516,32.2965240478516,32.2965240478516,32.2965240478516,32.2965240478516,32.2965240478516,32.9391784667969,32.9391784667969,32.9391784667969,32.9391784667969,32.9391784667969,32.9391784667969,32.9391784667969,33.5482330322266,33.5482330322266,33.5482330322266,33.5482330322266,33.5482330322266,33.5482330322266,34.3389663696289,34.3389663696289,34.3389663696289,34.3389663696289,34.3389663696289,35.0506820678711,35.0506820678711,35.0506820678711,35.0506820678711,35.0506820678711,35.0506820678711,35.0506820678711,35.0506820678711,35.0506820678711,35.7006225585938,35.7006225585938,35.7006225585938,35.7006225585938,35.7006225585938,35.7006225585938,35.7006225585938,35.7006225585938,36.6159439086914,36.6159439086914,36.6159439086914,36.6159439086914,36.6159439086914,36.6159439086914,36.6159439086914,36.6159439086914,36.6159439086914,36.6159439086914,36.6159439086914,36.6159439086914,37.2254028320312,37.2254028320312,37.2254028320312,37.2254028320312,37.2254028320312,37.2254028320312,37.8399505615234,37.8399505615234,37.8399505615234,37.8399505615234,37.8399505615234,37.8399505615234,28.1073303222656,28.1073303222656,28.1073303222656,28.1073303222656,28.1073303222656,28.1073303222656,28.1073303222656,28.1073303222656,28.1073303222656,28.9843673706055,28.9843673706055,28.9843673706055,28.9843673706055,28.9843673706055,28.9843673706055,28.9843673706055,28.9843673706055,28.9843673706055,28.9843673706055,29.6106033325195,29.6106033325195,29.6106033325195,29.6106033325195,29.6106033325195,29.6106033325195,29.6106033325195,29.6106033325195,30.5122985839844,30.5122985839844,30.5122985839844,30.5122985839844,30.5122985839844,30.5122985839844,31.0952072143555,31.0952072143555,31.0952072143555,31.0952072143555,31.0952072143555,31.0952072143555,31.0952072143555,31.0952072143555,31.0952072143555,31.727897644043,31.727897644043,32.5112686157227,32.5112686157227,32.5112686157227,32.5112686157227,32.5112686157227,32.5112686157227,32.5112686157227,32.5112686157227,32.5112686157227,33.0955123901367,33.0955123901367,33.0955123901367,33.0955123901367,33.0955123901367,33.0955123901367,33.0955123901367,33.0955123901367,33.0955123901367,33.0955123901367,33.0955123901367,33.0955123901367,33.0955123901367,33.0955123901367,33.733642578125,33.733642578125,33.733642578125,33.733642578125,33.733642578125,34.4516067504883,34.4516067504883,34.4516067504883,34.4516067504883,34.4516067504883,34.4516067504883,34.4516067504883,34.4516067504883,35.0762481689453,35.0762481689453,35.0762481689453,35.0762481689453,35.0762481689453,35.0762481689453,35.0762481689453,35.0762481689453,35.0762481689453,35.6746139526367,35.6746139526367,35.6746139526367,35.6746139526367,35.6746139526367,35.6746139526367,35.6746139526367,36.3484649658203,36.3484649658203,36.3484649658203,36.3484649658203,36.3484649658203,36.3484649658203,36.9195785522461,36.9195785522461,36.9195785522461,36.9195785522461,36.9195785522461,36.9195785522461,36.9195785522461,37.5619125366211,37.5619125366211,37.5619125366211,37.5619125366211,37.5619125366211,37.5619125366211,37.5619125366211,27.7499084472656,27.7499084472656,28.6186294555664,28.6186294555664,28.6186294555664,28.6186294555664,28.6186294555664,29.2545852661133,29.2545852661133,29.2545852661133,29.2545852661133,29.2545852661133,29.2545852661133,29.2545852661133,29.2545852661133,29.2545852661133,29.2545852661133,30.1711196899414,30.1711196899414,30.1711196899414,30.1711196899414,30.1711196899414,30.1711196899414,30.1711196899414,30.1711196899414,30.1711196899414,30.1711196899414,30.1711196899414,30.1711196899414,30.1711196899414,30.8162536621094,30.8162536621094,30.8162536621094,30.8162536621094,30.8162536621094,30.8162536621094,30.8162536621094,30.8162536621094,31.4332275390625,31.4332275390625,31.4332275390625,31.4332275390625,31.4332275390625,31.4332275390625,31.4332275390625,31.4332275390625,32.0896911621094,32.0896911621094,32.0896911621094,32.0896911621094,32.0896911621094,32.0896911621094,32.0896911621094,32.6750946044922,32.6750946044922,32.6750946044922,32.6750946044922,32.6750946044922,32.6750946044922,32.6750946044922,32.6750946044922,33.3269424438477,33.3269424438477,33.3269424438477,33.3269424438477,33.3269424438477,33.3269424438477,33.3269424438477,33.9226226806641,33.9226226806641,33.9226226806641,33.9226226806641,33.9226226806641,34.6209030151367,34.6209030151367,34.6209030151367,35.3398208618164,35.3398208618164,35.3398208618164,35.3398208618164,35.3398208618164,35.3398208618164,35.3398208618164,35.3398208618164,36.1987075805664,36.1987075805664,37.028678894043,37.028678894043,37.028678894043,37.028678894043,37.028678894043,37.028678894043,37.028678894043,37.028678894043,37.6145248413086,37.6145248413086,37.6145248413086,37.6145248413086,37.6145248413086,37.6145248413086,37.6145248413086,37.6145248413086,27.6121978759766,27.6121978759766,27.6121978759766,27.6121978759766,27.6121978759766,27.6121978759766,28.2574157714844,28.2574157714844,28.2574157714844,28.2574157714844,28.2574157714844,28.2574157714844,29.4439086914062,29.4439086914062,29.4439086914062,29.4439086914062,29.4439086914062,30.0476150512695,30.0476150512695,30.0476150512695,30.0476150512695,30.0476150512695,30.0476150512695,30.0476150512695,30.0476150512695,30.0476150512695,30.7269973754883,30.7269973754883,30.7269973754883,30.7269973754883,30.7269973754883,31.3289947509766,31.3289947509766,31.3289947509766,31.3289947509766,31.3289947509766,31.3289947509766,31.9299926757812,31.9299926757812,31.9299926757812,31.9299926757812,31.9299926757812,32.5401153564453,32.5401153564453,32.5401153564453,32.5401153564453,32.5401153564453,32.5401153564453,32.5401153564453,33.3097915649414,33.3097915649414,33.3097915649414,33.3097915649414,33.3097915649414,33.3097915649414,34.1142807006836,34.1142807006836,34.1142807006836,34.1142807006836,34.1142807006836,34.1142807006836,34.1142807006836,34.1142807006836,34.1142807006836,34.1142807006836,35.0674362182617,35.0674362182617,35.0674362182617,35.0674362182617,35.0674362182617,35.0674362182617,35.0674362182617,36.2587432861328,36.2587432861328,36.2587432861328,36.2587432861328,36.2587432861328,36.2587432861328,36.2587432861328,37.4554977416992,37.4554977416992,37.4554977416992,37.4554977416992,37.4554977416992,37.4554977416992,37.4554977416992,37.4554977416992,27.4069900512695,27.4069900512695,27.4069900512695,28.0386352539062,28.0386352539062,28.0386352539062,28.0386352539062,28.0386352539062,28.0386352539062,28.0386352539062,28.9763336181641,28.9763336181641,28.9763336181641,28.9763336181641,28.9763336181641,29.6011734008789,29.6011734008789,29.6011734008789,29.6011734008789,29.6011734008789,29.6011734008789,29.6011734008789,29.6011734008789,30.20751953125,30.20751953125,30.8236236572266,30.8236236572266,30.8236236572266,30.8236236572266,30.8236236572266,30.8236236572266,31.4487380981445,31.4487380981445,31.4487380981445,31.4487380981445,31.4487380981445,31.4487380981445,31.4487380981445,31.4487380981445,31.4487380981445,31.4487380981445,32.2389297485352,32.2389297485352,32.2389297485352,32.2389297485352,32.2389297485352,32.2389297485352,32.2389297485352,32.2389297485352,33.2437210083008,33.2437210083008,34.4540481567383,34.4540481567383,34.4540481567383,34.4540481567383,34.4540481567383,34.4540481567383,34.4540481567383,34.4540481567383,35.0459899902344,35.0459899902344,35.0459899902344,35.0459899902344,35.0459899902344,35.0459899902344,35.0459899902344,35.8984756469727,35.8984756469727,35.8984756469727,35.8984756469727,35.8984756469727,35.8984756469727,35.8984756469727,35.8984756469727,37.0963897705078,37.0963897705078,37.0963897705078,37.0963897705078,37.0963897705078,37.0963897705078,37.0963897705078,37.0963897705078,37.0963897705078,37.8395309448242,37.8395309448242,28.0529174804688,28.0529174804688,28.0529174804688,28.0529174804688,28.0529174804688,28.0529174804688,28.0529174804688,28.0529174804688,28.6618118286133,28.6618118286133,28.6618118286133,28.6618118286133,28.6618118286133,28.6618118286133,28.6618118286133,29.3150100708008,29.3150100708008,29.3150100708008,29.3150100708008,29.3150100708008,29.3150100708008,29.3150100708008,29.3150100708008,29.3150100708008,30.1310501098633,30.1310501098633,30.8446960449219,30.8446960449219,30.8446960449219,30.8446960449219,30.8446960449219,30.8446960449219,30.8446960449219,30.8446960449219,30.8446960449219,30.8446960449219,31.5913772583008,31.5913772583008,32.2591857910156,32.2591857910156,32.2591857910156,32.2591857910156,32.2591857910156,32.2591857910156,33.1084365844727,33.1084365844727,33.1084365844727,33.1084365844727,33.1084365844727,33.1084365844727,33.1084365844727,33.1084365844727,33.1084365844727,33.8316650390625,33.8316650390625,33.8316650390625,33.8316650390625,33.8316650390625,33.8316650390625,34.4332046508789,34.4332046508789,34.4332046508789,34.4332046508789,34.4332046508789,34.4332046508789,34.4332046508789,34.4332046508789,34.4332046508789,34.4332046508789,35.033203125,35.033203125,35.033203125,35.033203125,35.033203125,35.6902923583984,35.6902923583984,35.6902923583984,35.6902923583984,35.6902923583984,35.6902923583984,35.6902923583984,35.6902923583984,36.7844467163086,36.7844467163086,36.7844467163086,36.7844467163086,36.7844467163086,36.7844467163086,36.7844467163086,36.7844467163086,37.5169525146484,37.5169525146484,37.5169525146484,37.5169525146484,37.5169525146484,37.5169525146484,27.6176452636719,27.6176452636719,27.6176452636719,27.6176452636719,27.6176452636719,27.6176452636719,27.6176452636719,27.6176452636719,27.6176452636719,27.6176452636719,28.2225341796875,28.2225341796875,28.8020401000977,28.8020401000977,28.8020401000977,28.8020401000977,28.8020401000977,28.8020401000977,29.3978424072266,29.3978424072266,29.3978424072266,29.3978424072266,29.3978424072266,29.3978424072266,29.3978424072266,30.2904205322266,30.2904205322266,30.2904205322266,30.2904205322266,30.2904205322266,30.2904205322266,30.2904205322266,31.0593185424805,31.0593185424805,31.0593185424805,31.6918029785156,31.6918029785156,31.6918029785156,31.6918029785156,31.6918029785156,31.6918029785156,31.6918029785156,32.2878723144531,32.2878723144531,32.2878723144531,32.2878723144531,32.2878723144531,32.2878723144531,32.2878723144531,32.2878723144531,32.8725051879883,32.8725051879883,32.8725051879883,32.8725051879883,32.8725051879883,32.8725051879883,32.8725051879883,33.7722549438477,33.7722549438477,33.7722549438477,33.7722549438477,34.7815399169922,34.7815399169922,35.3684539794922,35.3684539794922,35.3684539794922,35.3684539794922,35.3684539794922,35.3684539794922,36.0041275024414,36.0041275024414,36.0041275024414,36.6196441650391,36.6196441650391,36.6196441650391,36.6196441650391,36.6196441650391,37.2015991210938,37.2015991210938,37.2015991210938,37.2015991210938,37.2015991210938,37.2015991210938,37.2015991210938,37.2015991210938,27.6818008422852,27.6818008422852,27.6818008422852,27.6818008422852,27.6818008422852,27.6818008422852,27.6818008422852,27.6818008422852,28.4501342773438,28.4501342773438,28.4501342773438,28.4501342773438,28.4501342773438,28.4501342773438,28.4501342773438,29.1500091552734,29.1500091552734,29.1500091552734,29.8200225830078,29.8200225830078,29.8200225830078,29.8200225830078,29.8200225830078,29.8200225830078,29.8200225830078,29.8200225830078,29.8200225830078,29.8200225830078,29.8200225830078,29.8200225830078,30.6907196044922,30.6907196044922,30.6907196044922,30.6907196044922,30.6907196044922,31.3007507324219,31.3007507324219,31.3007507324219,31.3007507324219,31.3007507324219,31.9126968383789,31.9126968383789,31.9126968383789,31.9126968383789,31.9126968383789,31.9126968383789,31.9126968383789,31.9126968383789,31.9126968383789,31.9126968383789,32.5213088989258,32.5213088989258,32.5213088989258,32.5213088989258,32.5213088989258,32.5213088989258,32.5213088989258,32.5213088989258,33.7180252075195,33.7180252075195,34.5473098754883,34.5473098754883,35.2522811889648,35.2522811889648,35.2522811889648,35.2522811889648,35.2522811889648,35.2522811889648,35.2522811889648,35.2522811889648,35.8546600341797,35.8546600341797,35.8546600341797,35.8546600341797,35.8546600341797,36.430778503418,36.430778503418,36.430778503418,36.430778503418,36.430778503418,36.430778503418,36.430778503418,36.430778503418,37.0011825561523,37.0011825561523,37.0011825561523,37.0011825561523,37.0011825561523,37.0011825561523,37.6151504516602,37.6151504516602,37.6151504516602,37.6151504516602,37.6151504516602,37.6151504516602,37.6151504516602,27.5621795654297,27.5621795654297,27.5621795654297,27.5621795654297,27.5621795654297,27.5621795654297,28.1611175537109,28.1611175537109,28.1611175537109,28.1611175537109,28.1611175537109,28.1611175537109,28.1611175537109,28.1611175537109,28.1611175537109,28.9061279296875,28.9061279296875,28.9061279296875,28.9061279296875,28.9061279296875,28.9061279296875,29.7794494628906,29.7794494628906,30.4769897460938,30.4769897460938,30.4769897460938,30.4769897460938,30.4769897460938,30.4769897460938,30.4769897460938,30.4769897460938,31.1945724487305,31.1945724487305,31.1945724487305,31.1945724487305,31.1945724487305,31.1945724487305,31.1945724487305,32.1216278076172,32.1216278076172,32.1216278076172,32.1216278076172,32.1216278076172,32.1216278076172,33.3267135620117,33.3267135620117,33.3267135620117,33.3267135620117,33.3267135620117,33.3267135620117,33.3267135620117,33.3267135620117,34.5336380004883,34.5336380004883,35.7320556640625,35.7320556640625,36.4781799316406,36.4781799316406,37.5272598266602,37.5272598266602,37.5272598266602,37.5272598266602,37.5272598266602,37.5272598266602,37.5272598266602,37.5272598266602,28.0461044311523,28.0461044311523,28.0461044311523,28.0461044311523,28.0461044311523,28.0461044311523,28.0461044311523,28.0461044311523,28.0461044311523,29.1261291503906,29.1261291503906,29.1261291503906,29.1261291503906,29.1261291503906,29.1261291503906,29.1261291503906,29.8713912963867,29.8713912963867,29.8713912963867,29.8713912963867,29.8713912963867,29.8713912963867,29.8713912963867,30.9096145629883,30.9096145629883,30.9096145629883,30.9096145629883,30.9096145629883,30.9096145629883,30.9096145629883,30.9096145629883,31.8013687133789,31.8013687133789,31.8013687133789,31.8013687133789,31.8013687133789,31.8013687133789,31.8013687133789,32.3963775634766,32.3963775634766,32.3963775634766,32.3963775634766,32.3963775634766,32.3963775634766,32.3963775634766,32.3963775634766,32.3963775634766,33.1223297119141,33.1223297119141,33.1223297119141,33.1223297119141,33.1223297119141,33.1223297119141,33.1223297119141,33.8401947021484,33.8401947021484,33.8401947021484,33.8401947021484,33.8401947021484,33.8401947021484,33.8401947021484,33.8401947021484,34.4634628295898,34.4634628295898,34.4634628295898,34.4634628295898,34.4634628295898,34.4634628295898,34.4634628295898,34.4634628295898,35.6717071533203,35.6717071533203,36.2844619750977,36.2844619750977,36.2844619750977,36.2844619750977,37.466667175293,37.466667175293,37.466667175293,37.466667175293,37.466667175293,37.466667175293,37.466667175293,37.466667175293,27.8402709960938,27.8402709960938,27.8402709960938,27.8402709960938,27.8402709960938,27.8402709960938,28.5718460083008,28.5718460083008,28.5718460083008,28.5718460083008,28.5718460083008,29.2606506347656,29.2606506347656,29.2606506347656,29.2606506347656,29.2606506347656,29.2606506347656,30.4052810668945,30.4052810668945,30.4052810668945,30.4052810668945,30.4052810668945,30.4052810668945,30.4052810668945,31.0168151855469,31.0168151855469,31.0168151855469,31.0168151855469,31.0168151855469,31.0168151855469,31.0168151855469,31.0168151855469,31.0168151855469,31.0168151855469,31.0168151855469,32.238395690918,32.238395690918,32.238395690918,32.238395690918,32.238395690918,32.238395690918,33.1433563232422,33.1433563232422,33.1433563232422,33.1433563232422,33.1433563232422,33.1433563232422,33.1433563232422,33.1433563232422,33.1433563232422,34.0446701049805,34.0446701049805,34.0446701049805,34.0446701049805,34.0446701049805,34.0446701049805,34.0446701049805,34.0446701049805,34.0446701049805,34.0446701049805,34.0446701049805,34.0446701049805,34.0446701049805,34.7350845336914,34.7350845336914,34.7350845336914,34.7350845336914,34.7350845336914,34.7350845336914,34.7350845336914,35.3193588256836,35.3193588256836,35.3193588256836,35.3193588256836,35.3193588256836,35.3193588256836,35.3193588256836,35.3193588256836,35.3193588256836,35.3193588256836,35.9756622314453,35.9756622314453,35.9756622314453,35.9756622314453,35.9756622314453,35.9756622314453,35.9756622314453,35.9756622314453,35.9756622314453,35.9756622314453,35.9756622314453,36.5845947265625,36.5845947265625,36.5845947265625,36.5845947265625,36.5845947265625,36.5845947265625,36.5845947265625,36.5845947265625,36.5845947265625,36.5845947265625,37.1887435913086,37.1887435913086,37.1887435913086,37.1887435913086,37.1887435913086,27.7025146484375,27.7025146484375,27.7025146484375,27.7025146484375,27.7025146484375,27.7025146484375,28.4496459960938,28.4496459960938,28.4496459960938,28.4496459960938,28.4496459960938,28.4496459960938,28.4496459960938,29.5219421386719,29.5219421386719,30.2827224731445,30.2827224731445,30.2827224731445,30.2827224731445,30.2827224731445,30.8838729858398,30.8838729858398,30.8838729858398,30.8838729858398,30.8838729858398,30.8838729858398,30.8838729858398],\"meminc\":[0,0,0,0,0,0,0,0,0,0.606887817382812,0,0,0,0.571456909179688,0,0,0,0,0,0,0,0.714179992675781,0,0,0,0,0,0,0,0,0.640533447265625,0,0,0,0,0,0,0,0.763107299804688,0,0,0,0,0,0,0.599929809570312,0,0,0.712554931640625,0,0,0,0,0,0,0,0,0,0,0.584449768066406,0,0,0,0,0,0.639762878417969,0,0,0,0,0.876754760742188,0,0,0,0,0.874794006347656,0,0,0,0,0,0,0,0.748214721679688,0,0,0,0,0,0,0.81475830078125,0,-9.72452545166016,0,0,0,0,0,0,0,0,0,1.13270568847656,0,0,0,0,0,0,0.675285339355469,0,0,0,0,0,0,0,0,1.210205078125,0,0,0,0,0,0,0,0,0,0.588127136230469,0,0,0,0,0,0,0,0.6490478515625,0,0,0,0,0,0,0,0.73175048828125,0,1.00191497802734,0,0,0,0,0.9921875,0,0,0,0,0,0.556480407714844,0,0,0,0,0,0,0,0.642585754394531,0,0,0,0,0,0,0,0.586418151855469,0,0,0,0,0,0,0.595687866210938,0,0,0,0,0,0,0,0,0,0,0,0,0,0.610977172851562,0,0.599250793457031,0,0,0,0,0,0,-10.1884841918945,0,0,0,0,0,0,0,0,0,0,0,0,0,1.05315399169922,0,0,0,0,0,0,0,0,0,0.786781311035156,0,0,0,0,0,0,0,0,0,1.22640991210938,0,0,0,0,0,0,0,1.14829254150391,0,0,0,0,0,0,0,0,0,0,0.616752624511719,0,0,0,0,0,0,0.6126708984375,0,0,0,0,0,0,0.606826782226562,0,0,0,0,0,0,0,0,0,0.642051696777344,0,0,0,0,0,0.704170227050781,0,0,0,0,0,0,0.6104736328125,0,0,0,0,0,0,0.574111938476562,0,0,0,0,0.613601684570312,0,0,0,0,0.613609313964844,0,0,0,0,-9.85360717773438,0,0,0,0,0.7933349609375,0,0,0,0,0,0,0,0.646369934082031,0,0,0,0,1.01335906982422,0,0,0,0,0,0,0,0,0,0,0,0.702339172363281,0,0,0,0,0,0.665443420410156,0,0,0,0,0,0,0,0,0,1.08113861083984,0,0,0,0,0,1.2265625,0,0.746223449707031,0,0,0,0,0,0,0,0.798728942871094,0,0,0,0,0,0,0,0,0,0.612556457519531,0,0,0,0,0,0,0,0,0,0,0.643272399902344,0,0,0,0,0,0,0,0,0,0,0,0,0.740043640136719,0,-9.75235748291016,0,0,0,0,0.615943908691406,0,0,0,0,0,0,0.617729187011719,0,0,0,0,0,1.23162841796875,0,0,0,0,0,0.639236450195312,0,0,0,0,0,0,1.15070343017578,0,0,0,0,0,0,0.604393005371094,0,0,0,0,0,0,0.6126708984375,0,0,0,0,0,0,0.613601684570312,0,0,0,0,0,0,0.608413696289062,0,0,0,0,0,0,0.602668762207031,0,0,0,0,0.611732482910156,0,0,0,0,0.613609313964844,0,0,0,0,0.615058898925781,0,0,0,0,0.608413696289062,0,0,0,0,0.613601684570312,0,0,0,0,-10.1522674560547,0,0,0,0,0,0,0,0.737495422363281,0,0,0,0,0,0.970848083496094,0,0.6126708984375,0,0,0,0,0,0,0.61279296875,0,0.750961303710938,0,0,0,0,0,0,0,0,0,0,0.600914001464844,0,0,0,0,0,0,0.670265197753906,0,0,0,0,0,0,0,0.642707824707031,0,0,0,0,0,0,0.656120300292969,0,0.658607482910156,0,0,0,0,0,0,0,0,0,0,0.714408874511719,0,0,0,0,0.661224365234375,0,0,0,0,0,0,0,0,0.982620239257812,0,0,0.592742919921875,0,0,0,0,0,-10.324348449707,0,0,0,0,0,0,0.221786499023438,0,0,0,0,0,0,0,0,0,0,0,0,0.690200805664062,0,0.672317504882812,0,0,0,0,0.889640808105469,0,0,0,0,0,0,0,1.20870971679688,0,0.580772399902344,0,0,0,0,0,0,0,0.818290710449219,0,0,0,0,0,0,0.537429809570312,0,0,0,0,0,0,0.552970886230469,0,0,0,0,0,0.896812438964844,0,0,0,0,0,0,0,0,0.588729858398438,0,0.545257568359375,0,0,0,0,0,0,0,0,0,0.564323425292969,0,0.527992248535156,0,0,0,0,0,0,0.7017822265625,0,0,0,0,0,0,0,0,-9.90408325195312,0,0,0,0,1.02967834472656,0,0,0,0,0,0,0,0,0.605514526367188,0,0,0,0,0,0,0,0,0,0.947196960449219,0,1.21275329589844,0,0,0,0,0,0.91033935546875,0,0,0,0,0,0,0.89825439453125,0,0,0,0,0,0,0,0.615432739257812,0,0,0,0,0,0,0,1.177001953125,0,0,0,0,0,0,0,0.937255859375,0,0,0,0,0,0,0,0.573715209960938,0,0.900978088378906,0,0,0,0,0,0,0,0,0,0,-9.88484954833984,0,0,0,0,0,0,0,0,0.671310424804688,0,0,0,0,0,0,0.704605102539062,0,0,0,0,0,0,0,0.894668579101562,0,0,0,0,0.612403869628906,0,0,0,0,0.890396118164062,0,0,0,0,0,0,0,0,0,0,0.732688903808594,0,0,0,0,0,0,0,0,0,0,0,0,0.844161987304688,0,0,0,0,0,0,0,0,0,0,0.834571838378906,0,0,0,0,0.897346496582031,0,0,0,0,0.46051025390625,0,0,0,0,0,0,0.740470886230469,0,0,0,0,0,0,0,0,0,0.572914123535156,0,0,0,0,0,0,0,0.566932678222656,0,0,0,0,0,0.918434143066406,0,0,0,0,0,0,-9.94357299804688,0,0,0,0,0,0,0,0.80712890625,0,0,0,0,0,0,0,0.686546325683594,0,0,0,0,0,0,0,0.83294677734375,0,0,0,0,0,0,0.625137329101562,0,0,0,0,0,0,0,0,0,0,0.636009216308594,0,0.859207153320312,0,0,0,0,0.719711303710938,0,0,0,0,0,0,0,0.554489135742188,0,0,0,0,0,0,0,0,0.626556396484375,0,0,0,0,0,0.91949462890625,0,0,0,0,0,0.673835754394531,0,0,0,0,0.591835021972656,0,0,0,0,0.5706787109375,0,0,0,0,0,0,0,0.577186584472656,0,0,0,0,0.535881042480469,0,0,0,0,0,0,0,-10.143669128418,0,0.573783874511719,0,0.584609985351562,0,0,0,0,0.962562561035156,0,0,0,0,0,0,0,0,0,0.681449890136719,0,0,0,0,0,0.59967041015625,0,0,0,0,0,0,0,0,0.604377746582031,0,0,0,0,0,0,0.722480773925781,0,0,0,0,0,1.03629302978516,0,1.20703125,0,0,0,0,0,0,0.6019287109375,0,0,0,0,0,0,0,0.641883850097656,0,0,0,0,0,0,0,1.15532684326172,0,0,0,0,0,0,0,0,0,0,-9.81767272949219,0,0,0,0,0,0,0,0,0,0,0.830093383789062,0,0,0,0,0,0,0,0.61224365234375,0,0,0,0,0,0,0.659111022949219,0,0,0,0,0,0,0.634864807128906,0,0,0,0,0,0.593406677246094,0,0,0,0,0,0,0,0,1.00151062011719,0,0,0,0,0,0,0.638465881347656,0,0,0,0,0,0,0.891014099121094,0,0,0,0,0,0,0.667022705078125,0,0.777084350585938,0,0,0,0,0,0,0,1.18158721923828,0,0,0,0,0,0,0,0.683250427246094,0,0,0.966285705566406,0,0,0,0,0,0,0,0.478492736816406,0,0,-9.82257080078125,0,0,0,0,0,0,0,0.6114501953125,0,0,0,0,0,0,0,0,0,0,1.22434234619141,0,0,0,0,0,0,0,0,0,0,0,0.786277770996094,0,0,0,0,0,0,0.964767456054688,0,0,0,0,0,0,0,0,0,0.695533752441406,0,0,0,0,0.589866638183594,0,0,0,0,0.608993530273438,0,0,0,0,0,0,0,0,0,1.20967864990234,0,0.982368469238281,0,0,0,0,0.639259338378906,0,0,0,0,0,0,0,0,0,0,0,0,0.764106750488281,0,0,0,0,0,0,0,0,0.591270446777344,0,0,0,0,0,-9.79022979736328,0,1.21937561035156,0,0,0,0,0,0,0,0.639892578125,0,0,0,0,0,0,0,0.673912048339844,0,0,0,0,0,0,0,0.770889282226562,0,0.672882080078125,0,0,0,0,0,0,0.602455139160156,0,0,0,0,0.613609313964844,0,0,0,0,0.6126708984375,0,0,0,0,0.591484069824219,0,0,0,0,0,0,0.579818725585938,0,0,0,0,0.592514038085938,0,0,0,0,0,0,0.91461181640625,0,0.640182495117188,0,0.65948486328125,0,0,0,0,0,0,0,0,0,-9.64223480224609,0,1.20606994628906,0,0,0,0,0,0,0,0.898063659667969,0,0,0,0,0,0.611007690429688,0,0,0,0,1.1986083984375,0,0,0,0,0,0,0.600128173828125,0,0,0,0,0,0,0.651924133300781,0,0,0,0,0,0,1.14385223388672,0,0,0,0,0,0,1.12596130371094,0,0,0,0,0.588851928710938,0,0,0,0,0,0,0,0,0.600143432617188,0,0,0,0,0,0,0,0.593170166015625,0,0,0,0,0,-9.83806610107422,0,0,1.00753021240234,0,0,0,0,0,0.673789978027344,0,0,0,0,0,0,0.771759033203125,0,0,0,0,0,0,0.5987548828125,0,0,0,0,0,0,0,0,0.688980102539062,0,0,0,0,0,0,0.958633422851562,0,0,0,0,0,0,0,0,0,0.816642761230469,0,0,0,0,0,0,0.625831604003906,0,0,0,0,0,0,0,0,0,0.834617614746094,0,0,0,0,0,0,0.665412902832031,0,0.650131225585938,0,0,0,0,0,0,0.817520141601562,0,0,0,0,0,0,0,0.62078857421875,0,0,0,0,0,0,0,0,0,-9.79486846923828,0,0,0,0,0.630210876464844,0,0,0,0,0.769905090332031,0,0,0,0,0,0,0.654197692871094,0,0.613494873046875,0,0,0,0,0,0.60919189453125,0,0,0,0,0,0,0,0,0,0,0.661415100097656,0,0,0,0,0.991127014160156,0,0,0,0,0,0,0,0,0,0,0,0.754341125488281,0,0,0,0,0,0,0,0,0,0,0.747398376464844,0,0,0,0,0.963638305664062,0,0,0,0,0,0,0,0,0,0,0.730926513671875,0,0,0,0,0,0,0.681900024414062,0,0,0,0,0,0,0,1.1866455078125,0,0,0,0,-9.99486541748047,0,0,0,0,0,0,0,1.22477722167969,0,0,0,0,0,0,1.22123718261719,0,0,0,0,0,0,0,0,0,0.6036376953125,0,0,0,0,0,0,0,0,0,1.22156524658203,0,0,0,0,0.610458374023438,0,0.58917236328125,0,0.6527099609375,0,0,0,0,0,0,0,1.03821563720703,0,0,0,0,0,0,0,0,0,0.607414245605469,0,0,0,0.528404235839844,0,0,0,0,0,0,0.761756896972656,0,0,0,0,0,0.606773376464844,0,0,0,0,0,0,0,0,0,0,0.616828918457031,0,0,0,0,0,0,-10.1323089599609,0,0,0,0,0,0.585922241210938,0,0,0,0,0,0,0.877059936523438,0,0,0,0,0.606895446777344,0,0,0,0,0,0,0,1.22431182861328,0,0,0,0,0,0,1.02576446533203,0,0,0,0,0,0,0,0,0.791694641113281,0,0,0,0,0,0,0,0.605697631835938,0,0,0,0,0,0,0.872085571289062,0,0,0,0,0,0,0,0,0,0.607330322265625,0,0,0,0,0.598655700683594,0,0.5838623046875,0,0,0,0,0.6202392578125,0,0.601814270019531,0,0,0,0,0,0,0,0,0,0,0.650070190429688,0,0,0,-9.96430969238281,0,0,0.758003234863281,0,0,0,0,0,0,0,0,0,0,0.601280212402344,0,0,0,0,0,0,0,0.62359619140625,0,0,0,0,0,0,0,0,0,0,0,0,0.7464599609375,0,0,0,0,0,0,0,0.614326477050781,0,0,0,0,0,0,0.961502075195312,0,0,0,0,0,0,0,0,0.85821533203125,0,0,0.920112609863281,0,0,0,0,0,0,0.5877685546875,0,0,0,0,0,0,0,0,0,1.19404602050781,0,0,0,0,0,0,0.594680786132812,0,0,0,0,0,0,0,0,0,0,0,0,0,0.562080383300781,0,0,1.04564666748047,0,0,0,0,0,0,0,-9.79327392578125,0,0,0,0,0,0,0.661827087402344,0,0,0,0,0,0,0,0.608108520507812,0,0.717811584472656,0,0,0,0,0,0,0,0,0,0,0.83050537109375,0,0,0,0,0,0.610664367675781,0,0,0,0,0,0,0,0,0,1.21272277832031,0,0,0,0,0,0,0,0,0,0.935333251953125,0,0,0,0,0,0,0.872276306152344,0,0.611930847167969,0,0,0,0,0,0,0,0,0,0.777671813964844,0,0,0,0,0,0,0.6143798828125,0,0,0,0,0.591453552246094,0,0,0,0,0,0,0.590126037597656,0,0,0,0,-9.81147766113281,0,0,0,0,0,0,0,0.642967224121094,0,0,0,0,0,0,0,0,0,0,0.745475769042969,0,0,0.624725341796875,0,0,0,0,0,0,0,0.609848022460938,0,0,0,0,0,0,0,0,0,0.821197509765625,0,0,0,0,0,0,0,0,0.657608032226562,0,1.01963043212891,0,0,0,0,0,0,0.741287231445312,0,0,0,0,0,0,0,0,0.599853515625,0,0,0,0,0,0,0,0,0,0.666435241699219,0,0,0,0,0,0,0,0,0,0.600425720214844,0,0,0,0,0,0,0.595016479492188,0,0,0,0,1.19853210449219,0,0,0,0,0,-9.54248046875,0,0,0,0,0,0,0.742820739746094,0,1.22205352783203,0,0,0,0,0,0,0.617164611816406,0,0,0,0,0,0,0,0.696319580078125,0,0,0,0,0,0,0,0,0,0,0,0,0.834251403808594,0,0,0,0,0,0,0,0,0,1.21047210693359,0,0,0,0,0,0.844200134277344,0,0.602218627929688,0,0,0,0,0,0,0,0,0,1.19645690917969,0,0,0,0,0,0,0.639259338378906,0,1.15949249267578,0,0,0,0,0,0,-9.52375030517578,0,0.612571716308594,0,0,0,0,0,0,0,0,0.727516174316406,0,0,0,0,0,0,0,0,0,0,0,1.22148895263672,0,0,0,0.6121826171875,0,0,0,0,0,0.81549072265625,0,0,0,0,0.592430114746094,0,0,0,0,0,0,0,0.824119567871094,0,0,0.597488403320312,0,0,0,0,0.677993774414062,0,0,0,0,0,0,0.627281188964844,0,0,0,0,0,0.599342346191406,0,0.711715698242188,0,0,0,0.559669494628906,0,0,0,0,-9.95722198486328,0,0,0,0,0,0.643745422363281,0,0,0,0,0,0,0,0,0,0,0.960678100585938,0,0.749984741210938,0,0,0,0,0,0,0,0.602745056152344,0,0,0,0,0,0,0,0.579002380371094,0,0,0,0,0,0,0,0.6524658203125,0,0,0,0,0,0,0,0,0.687515258789062,0,0,0,0,0,0.651473999023438,0,0,0,0,0,1.0618896484375,0,0.601127624511719,0,0,0,0,0,0,0,0.611579895019531,0,0,0,0,0,0,0,0.662643432617188,0,0,0.884757995605469,0,0,0,0,0,0,0.613807678222656,0,0,0,0,0,0,0.569793701171875,0,0,0,0,0,0,-9.99490356445312,0,0,0,0,0,0,0,0,0,0.605903625488281,0,0,0,0,0.614410400390625,0,0,0,0,0,0,0,0.748794555664062,0,0,0,0,0,0,0,0.711196899414062,0,0,0,0,0,0,0,0,0,0.654823303222656,0,0.568206787109375,0,0,0,0,0,0,0,0,0,0,0.923500061035156,0,0,0,0,0,0,0.643585205078125,0,0,0,0,0,0,0,0,0.577217102050781,0,0,0,0,0,0,0,0,0,0,0.734077453613281,0,0,0,0,0,0,0,0,0,0.680854797363281,0,0,0,0,0.603378295898438,0,0,0,0,0,0,0,0,0.730239868164062,0,0,0,0,0,0,0,0,0,0.669219970703125,0,0,0,0,0,0.535377502441406,0,0,0,0,0,0,-10.0488204956055,0,0,0,0,0.814460754394531,0,0,0,0,0,0,0.675880432128906,0,0,0,0,0,0,0,0,0,0,0.64007568359375,0,0,0,0,0,0,0,0.842315673828125,0,0.589431762695312,0,0,0,0,0,0,0,0.606712341308594,0,0,0,0,0,0,0.79150390625,0,0,0,0,0,0,0,0,0.814682006835938,0,0,0,0,0,0,0,0.705451965332031,0,0,0,0,0,0,0,0,0,0,0.839164733886719,0,0,0,0,0,0,0,0.575958251953125,0,0,0,0,0,0.898933410644531,0,0,0,0,0.98077392578125,0,0,0,0,0,0,-10.0187225341797,0,0,0,0,0,0,0,0,0,0,0,0,0.585304260253906,0,0,0,0,0,0,0,0.859245300292969,0,0,0,0,0,0,0,0,0,0,0,0.922592163085938,0,0,0,0,0,0,0,0.606071472167969,0,0.819366455078125,0,0,0,0,0,0.710968017578125,0,0,0,0,0.84161376953125,0,0,0,0,0.66400146484375,0,0.770584106445312,0,0,0.701164245605469,0,0.791473388671875,0,0,0,0,0,0,0.613258361816406,0,0,0,0,0,0,0,0,0,0.831390380859375,0,0,0,0,0,0,0,0,0,0,0,0.495361328125,0,0,0,0,0,0,-9.61026000976562,0,0,0,0,0,0,0.656417846679688,0,0,0,0,0,0,0.652542114257812,0,0,0,0,0,0,0.698051452636719,0,0.982292175292969,0,0,0,0,0,0,0.644744873046875,0,0,0,0.709281921386719,0,0,0,0,0,0,0,0.966148376464844,0,0,0,0,0,0,0,0,0,0.811798095703125,0,0,0,0,0.671119689941406,0,0,0,0,0,0,0,0.621406555175781,0,0,0,0,0,0,0.680801391601562,0,0,0,0,0,0,0,0,0,0.654502868652344,0,0,0,0,0,0,0,0.66778564453125,0,0,0,0,0,0,-10.1132354736328,0,0,0,0,0,0,0.583061218261719,0,0,0,0,0,0,0,0.609756469726562,0,0,0,0.875846862792969,0,0.583534240722656,0,0,0,0,0,0,0,0.617416381835938,0,0,0,0,0,0,0,0.694831848144531,0,0.605720520019531,0,0,0.956558227539062,0,0,0,0,0,0,0.590423583984375,0,0,0,0,0,0,0.701225280761719,0,0.771224975585938,0,0,0,0,0,0,0.588706970214844,0,0,0,0,0,0.591766357421875,0,0.737068176269531,0,0,0,0,0,0.576400756835938,0,0,0,0,-10.1653366088867,0,0,0,0,0,0,0.588150024414062,0,0,0,0,0,0,0,0.6551513671875,0,0,0,0,0,0,0.899307250976562,0,0,0,0.839035034179688,0,0,0,0,0,0,0,0.664558410644531,0,0,0,0,0,0,0,0.572181701660156,0,0,0,0,0,0.654518127441406,0,0,0,0,0,0,0,0,0,0,0.642654418945312,0,0,0,0,0,0,0.609054565429688,0,0,0,0,0,0.790733337402344,0,0,0,0,0.711715698242188,0,0,0,0,0,0,0,0,0.649940490722656,0,0,0,0,0,0,0,0.915321350097656,0,0,0,0,0,0,0,0,0,0,0,0.609458923339844,0,0,0,0,0,0.614547729492188,0,0,0,0,0,-9.73262023925781,0,0,0,0,0,0,0,0,0.877037048339844,0,0,0,0,0,0,0,0,0,0.626235961914062,0,0,0,0,0,0,0,0.901695251464844,0,0,0,0,0,0.582908630371094,0,0,0,0,0,0,0,0,0.6326904296875,0,0.783370971679688,0,0,0,0,0,0,0,0,0.584243774414062,0,0,0,0,0,0,0,0,0,0,0,0,0,0.638130187988281,0,0,0,0,0.717964172363281,0,0,0,0,0,0,0,0.624641418457031,0,0,0,0,0,0,0,0,0.598365783691406,0,0,0,0,0,0,0.673851013183594,0,0,0,0,0,0.571113586425781,0,0,0,0,0,0,0.642333984375,0,0,0,0,0,0,-9.81200408935547,0,0.868721008300781,0,0,0,0,0.635955810546875,0,0,0,0,0,0,0,0,0,0.916534423828125,0,0,0,0,0,0,0,0,0,0,0,0,0.645133972167969,0,0,0,0,0,0,0,0.616973876953125,0,0,0,0,0,0,0,0.656463623046875,0,0,0,0,0,0,0.585403442382812,0,0,0,0,0,0,0,0.651847839355469,0,0,0,0,0,0,0.595680236816406,0,0,0,0,0.698280334472656,0,0,0.718917846679688,0,0,0,0,0,0,0,0.85888671875,0,0.829971313476562,0,0,0,0,0,0,0,0.585845947265625,0,0,0,0,0,0,0,-10.002326965332,0,0,0,0,0,0.645217895507812,0,0,0,0,0,1.18649291992188,0,0,0,0,0.603706359863281,0,0,0,0,0,0,0,0,0.67938232421875,0,0,0,0,0.601997375488281,0,0,0,0,0,0.600997924804688,0,0,0,0,0.610122680664062,0,0,0,0,0,0,0.769676208496094,0,0,0,0,0,0.804489135742188,0,0,0,0,0,0,0,0,0,0.953155517578125,0,0,0,0,0,0,1.19130706787109,0,0,0,0,0,0,1.19675445556641,0,0,0,0,0,0,0,-10.0485076904297,0,0,0.631645202636719,0,0,0,0,0,0,0.937698364257812,0,0,0,0,0.624839782714844,0,0,0,0,0,0,0,0.606346130371094,0,0.616104125976562,0,0,0,0,0,0.625114440917969,0,0,0,0,0,0,0,0,0,0.790191650390625,0,0,0,0,0,0,0,1.00479125976562,0,1.2103271484375,0,0,0,0,0,0,0,0.591941833496094,0,0,0,0,0,0,0.852485656738281,0,0,0,0,0,0,0,1.19791412353516,0,0,0,0,0,0,0,0,0.743141174316406,0,-9.78661346435547,0,0,0,0,0,0,0,0.608894348144531,0,0,0,0,0,0,0.6531982421875,0,0,0,0,0,0,0,0,0.8160400390625,0,0.713645935058594,0,0,0,0,0,0,0,0,0,0.746681213378906,0,0.667808532714844,0,0,0,0,0,0.849250793457031,0,0,0,0,0,0,0,0,0.723228454589844,0,0,0,0,0,0.601539611816406,0,0,0,0,0,0,0,0,0,0.599998474121094,0,0,0,0,0.657089233398438,0,0,0,0,0,0,0,1.09415435791016,0,0,0,0,0,0,0,0.732505798339844,0,0,0,0,0,-9.89930725097656,0,0,0,0,0,0,0,0,0,0.604888916015625,0,0.579505920410156,0,0,0,0,0,0.595802307128906,0,0,0,0,0,0,0.892578125,0,0,0,0,0,0,0.768898010253906,0,0,0.632484436035156,0,0,0,0,0,0,0.5960693359375,0,0,0,0,0,0,0,0.584632873535156,0,0,0,0,0,0,0.899749755859375,0,0,0,1.00928497314453,0,0.5869140625,0,0,0,0,0,0.635673522949219,0,0,0.615516662597656,0,0,0,0,0.581954956054688,0,0,0,0,0,0,0,-9.51979827880859,0,0,0,0,0,0,0,0.768333435058594,0,0,0,0,0,0,0.699874877929688,0,0,0.670013427734375,0,0,0,0,0,0,0,0,0,0,0,0.870697021484375,0,0,0,0,0.610031127929688,0,0,0,0,0.611946105957031,0,0,0,0,0,0,0,0,0,0.608612060546875,0,0,0,0,0,0,0,1.19671630859375,0,0.82928466796875,0,0.704971313476562,0,0,0,0,0,0,0,0.602378845214844,0,0,0,0,0.576118469238281,0,0,0,0,0,0,0,0.570404052734375,0,0,0,0,0,0.613967895507812,0,0,0,0,0,0,-10.0529708862305,0,0,0,0,0,0.59893798828125,0,0,0,0,0,0,0,0,0.745010375976562,0,0,0,0,0,0.873321533203125,0,0.697540283203125,0,0,0,0,0,0,0,0.717582702636719,0,0,0,0,0,0,0.927055358886719,0,0,0,0,0,1.20508575439453,0,0,0,0,0,0,0,1.20692443847656,0,1.19841766357422,0,0.746124267578125,0,1.04907989501953,0,0,0,0,0,0,0,-9.48115539550781,0,0,0,0,0,0,0,0,1.08002471923828,0,0,0,0,0,0,0.745262145996094,0,0,0,0,0,0,1.03822326660156,0,0,0,0,0,0,0,0.891754150390625,0,0,0,0,0,0,0.595008850097656,0,0,0,0,0,0,0,0,0.7259521484375,0,0,0,0,0,0,0.717864990234375,0,0,0,0,0,0,0,0.623268127441406,0,0,0,0,0,0,0,1.20824432373047,0,0.612754821777344,0,0,0,1.18220520019531,0,0,0,0,0,0,0,-9.62639617919922,0,0,0,0,0,0.731575012207031,0,0,0,0,0.688804626464844,0,0,0,0,0,1.14463043212891,0,0,0,0,0,0,0.611534118652344,0,0,0,0,0,0,0,0,0,0,1.22158050537109,0,0,0,0,0,0.904960632324219,0,0,0,0,0,0,0,0,0.901313781738281,0,0,0,0,0,0,0,0,0,0,0,0,0.690414428710938,0,0,0,0,0,0,0.584274291992188,0,0,0,0,0,0,0,0,0,0.656303405761719,0,0,0,0,0,0,0,0,0,0,0.608932495117188,0,0,0,0,0,0,0,0,0,0.604148864746094,0,0,0,0,-9.48622894287109,0,0,0,0,0,0.74713134765625,0,0,0,0,0,0,1.07229614257812,0,0.760780334472656,0,0,0,0,0.601150512695312,0,0,0,0,0,0],\"filename\":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null]},\"interval\":10,\"files\":[],\"prof_output\":\"/var/folders/f3/t51slq3x2dlfgk3ksp7vddzm0000gq/T//Rtmpc50nMf/filea0242a7e129.prof\",\"highlight\":{\"output\":[\"^output\\\\$\"],\"gc\":[\"^$\"],\"stacktrace\":[\"^\\\\.\\\\.stacktraceo(n|ff)\\\\.\\\\.$\"]},\"split\":\"h\"}},\"evals\":[],\"jsHooks\":[]}  {\"x\":{\"message\":{\"prof\":{\"time\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,11,11,11,11,11,11,11,11,11],\"depth\":[16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1],\"label\":[\"%in%\",\"getFoldFun\",\"constantFoldCall\",\"constantFold\",\"cmp\",\"h\",\"tryInline\",\"cmpCall\",\"cmp\",\"genCode\",\"cmpfun\",\"compiler:::tryCmpfun\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"sum\",\"fastSimNullDistR_work\",\"fastSimNullDistRProp\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"sample.int\",\"base::sample\",\"fastSimNullDistR_work\",\"fastSimNullDistRProp\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"getOption\",\"sum\",\"fastSimNullDistR_work\",\"fastSimNullDistRProp\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"sum\",\"fastSimNullDistR_work\",\"fastSimNullDistRProp\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"\",\"base::sample\",\"fastSimNullDistR_work\",\"fastSimNullDistRProp\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"sample.int\",\"base::sample\",\"fastSimNullDistR_work\",\"fastSimNullDistRProp\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"sample.int\",\"base::sample\",\"fastSimNullDistR_work\",\"fastSimNullDistRProp\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"sample.int\",\"base::sample\",\"fastSimNullDistR_work\",\"fastSimNullDistRProp\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"sum\",\"fastSimNullDistR_work\",\"fastSimNullDistRProp\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"\",\"sample.int\",\"base::sample\",\"fastSimNullDistR_work\",\"fastSimNullDistRProp\",\"eval\",\"eval\",\"eval.parent\",\"local\"],\"filenum\":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null],\"linenum\":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,3,null,null,null,null,null,null,null,3,null,null,null,null,null,null,null,3,null,null,null,null,null,null,3,null,null,null,null,null,null,null,3,null,null,null,null,null,null,null,3,null,null,null,null,null,null,null,3,null,null,null,null,null,null,null,3,null,null,null,null,null,null,3,null,null,null,null,null,null,null,null,3,null,null,null,null],\"memalloc\":[28.4371337890625,28.4371337890625,28.4371337890625,28.4371337890625,28.4371337890625,28.4371337890625,28.4371337890625,28.4371337890625,28.4371337890625,28.4371337890625,28.4371337890625,28.4371337890625,28.4371337890625,28.4371337890625,28.4371337890625,28.4371337890625,31.9986114501953,31.9986114501953,31.9986114501953,31.9986114501953,31.9986114501953,31.9986114501953,31.9986114501953,43.547966003418,43.547966003418,43.547966003418,43.547966003418,43.547966003418,43.547966003418,43.547966003418,43.547966003418,50.6626663208008,50.6626663208008,50.6626663208008,50.6626663208008,50.6626663208008,50.6626663208008,50.6626663208008,50.6626663208008,60.4793243408203,60.4793243408203,60.4793243408203,60.4793243408203,60.4793243408203,60.4793243408203,60.4793243408203,49.5292510986328,49.5292510986328,49.5292510986328,49.5292510986328,49.5292510986328,49.5292510986328,49.5292510986328,49.5292510986328,35.8811111450195,35.8811111450195,35.8811111450195,35.8811111450195,35.8811111450195,35.8811111450195,35.8811111450195,35.8811111450195,44.1074447631836,44.1074447631836,44.1074447631836,44.1074447631836,44.1074447631836,44.1074447631836,44.1074447631836,44.1074447631836,54.609489440918,54.609489440918,54.609489440918,54.609489440918,54.609489440918,54.609489440918,54.609489440918,54.609489440918,60.3472442626953,60.3472442626953,60.3472442626953,60.3472442626953,60.3472442626953,60.3472442626953,60.3472442626953,44.938232421875,44.938232421875,44.938232421875,44.938232421875,44.938232421875,44.938232421875,44.938232421875,44.938232421875,44.938232421875],\"meminc\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.56147766113281,0,0,0,0,0,0,11.5493545532227,0,0,0,0,0,0,0,7.11470031738281,0,0,0,0,0,0,0,9.81665802001953,0,0,0,0,0,0,-10.9500732421875,0,0,0,0,0,0,0,-13.6481399536133,0,0,0,0,0,0,0,8.22633361816406,0,0,0,0,0,0,0,10.5020446777344,0,0,0,0,0,0,0,5.73775482177734,0,0,0,0,0,0,-15.4090118408203,0,0,0,0,0,0,0,0],\"filename\":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,null,\"\",null,null,null,null]},\"interval\":10,\"files\":[{\"filename\":\"\",\"content\":\"set.seed(2009)\\nprofvis({\\n NullDistFSNDR_aw \"}],\"prof_output\":\"/var/folders/f3/t51slq3x2dlfgk3ksp7vddzm0000gq/T//Rtmpc50nMf/filea021a18cd1c.prof\",\"highlight\":{\"output\":[\"^output\\\\$\"],\"gc\":[\"^$\"],\"stacktrace\":[\"^\\\\.\\\\.stacktraceo(n|ff)\\\\.\\\\.$\"]},\"split\":\"h\"}},\"evals\":[],\"jsHooks\":[]}  {\"x\":{\"message\":{\"prof\":{\"time\":[1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,3,3,4,4,4,4,4,4,4,5,5,5,6,6,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,11,11,11,12,12,12,12,12,12,12,12,12,13,13,13,13,13,14,14,14,14,14,14,14,15,15,15,15,15,16,16,16,16,16,16,16,17,17,18,18,18,18,18,19,19,19,19,19,20,20,20,20,20,20,20,21,21,21,21,22,22,22,22,22,22,22,22,23,23,23,24,24,24,24,24,25,25,25,25,25,25,25,25,25,26,26,26,26,26,26,26,27,27,27,27,27,27,27,27,27,28,28,28,28,28,28,28,29,29,29,29,29,29,29,30,30,30,30,30,30,30,31,31,31,31,31,32,32,33,33,33,33,33,33,33,33,33,34,34,34,34,34,34,34,34,34,35,35,35,35,35,35,35,35,36,36,36,36,36,36,36,36,36,37,37,37,37,37,37,37,37,38,38,38,38,38,38,38,38,39,39,39,39,39,40,40,41,41,41,41,41,41,41,42,42,42,42,42,42,42,42,42,43,43,43,43,43,43,43,44,44,44,44,44,44,45,45,45,45,45,45,45,45,46,46,46,46,46,47,47,47,47,47,47,47,47,47,47,47,47,48,48,48,48,48,48,48,48,48,48,49,49,49,49,49,49,49,49,50,50,50,50,50,50,50,50,50,51,51,51,51,51,51,51,52,52,52,52,53,53,53,53,53,53,53,53,53,53,53,54,54,54,54,54,55,55,55,55,55,55,55,55,55,55,55,56,56,56,56,56,56,56,56,57,57,57,57,57,57,57,57,57,57,57,58,58,58,58,58,58,58,58,59,59,59,59,59,59,59,59,59,59,59,59,59,59,60,60,60,60,60,60,60,61,61,61,61,61,61,61,62,62,62,62,62,62,62,63,63,63,63,63,63,63,63,63,63,64,64,64,64,64,64,64,65,65,66,66,66,66,66,66,66,66,67,67,67,67,68,68,69,69,69,69,69,69,69,69,69,70,70,70,70,71,71,71,71,71,71,71,71,71,71,71,72,72,72,72,72,72,72,72,72,72,73,73,73,73,73,73,73,73,73,73,73,73,74,74,74,74,74,74,74,74,74,74,74,74,74,74,75,75,75,75,75,75,75,75,75,76,76,76,76,76,76,76,76,76,77,77,77,77,77,77,77,78,78,78,78,78,78,78,78,78,78,78,79,79,79,79,79,79,79,79,80,80,80,80,80,80,80,81,81,81,81,81,81,81,82,82,82,82,82,83,83,83,83,83,84,84,84,84,84,84,84,84,85,85,86,86,86,86,86,86,86,87,87,87,87,87,87,87,87,88,88,88,88,88,88,89,89,90,90,90,90,90,91,91,91,91,91,91,91,91,91,91,91,92,92,92,92,92,93,93,93,93,93,94,94,94,94,94,94,94,94,95,95,96,96,96,96,96,97,97,97,97,97,98,98,98,98,99,99,100,100,100,100,100,100,100,101,101,101,102,102,102,102,102,102,102,103,103,103,103,103,103,103,104,104,104,104,105,105,105,105,105,105,105,105,105,106,106,106,106,106,107,107,107,107,107,107,107,108,108,108,108,108,108,108,108,108,109,109,109,109,109,109,110,110,110,110,110,110,110,111,111,111,111,111,112,112,112,112,112,112,112,113,113,113,113,113,114,114,114,114,115,115,116,116,116,116,116,116,116,116,116,117,117,117,117,117,118,118,119,119,119,119,119,120,120,120,120,120,120,120,120,120,121,121,121,121,121,121,121,121,121,121,121,122,122,122,122,122,122,122,122,123,123,123,123,123,123,123,123,123,123,123,124,124,124,124,124,124,124,124,125,125,125,125,125,125,125,125,126,126,126,126,126,127,127,127,127,127,127,127,127,128,128,128,128,128,129,129,129,129,129,129,129,129,130,130,130,130,130,130,130,130,131,131,131,131,131,131,131,132,132,132,132,132,132,133,133,133,133,133,133,133,133,134,134,135,135,135,135,135,135,135,135,135,136,136,136,136,136,137,137,137,137,137,137,137,138,138,138,138,138,138,139,139,139,139,139,140,140,140,140,140,140,140,140,140,140,141,141,141,142,142,142,142,142,142,142,142,143,143,143,144,144,144,144,144,144,144,144,144,144,145,145,145,145,145,145,145,146,146,146,146,146,146,146,147,147,147,147,147,147,147,147,147,147,147,147,147,148,148,148,148,148,148,148,148,148,148,149,149,149,149,149,149,149,149,149,149,150,150,150,150,150,150,150,150,150,151,151,151,151,151,152,152,152,152,152,152,152,152,152,153,153,153,153,153,153,153,154,154,154,154,154,154,155,155,155,155,155,155,155,155,155,155,156,156,156,156,156,156,156,157,157,157,157,157,157,157,157,157,157,157,157,158,158,158,158,158,158,158,159,159,159,159,160,160,161,161,161,161,161,161,161,161,162,162,162,162,162,162,163,163,163,163,163,163,164,164,164,164,164,164,165,165,165,165,165,165,166,166,166,166,166,166,166,166,166,166,167,167,167,167,167,167,168,168,168,168,169,169,169,169,169,169,169,169,170,170,170,170,170,170,170,170,170,170,170,171,171,171,172,172,172,172,172,172,172,173,173,174,174,174,174,174,174,174,175,175,175,175,175,175,176,176,176,176,176,176,176,176,177,177,177,177,177,177,177,177,177,178,178,178,178,179,179,180,180,180,180,180,180,180,180,180,180,181,181,181,181,181,181,181,181,181,182,182,183,183,183,183,183,183,183,183,183,183,184,184,184,184,184,184,184,185,185,185,185,185,185,186,186,186,186,186,186,186,186,186,187,187,187,187,187,187,187,187,187,188,188,188,188,188,188,188,188,189,189,189,189,189,189,189,189,189,189,190,190,190,190,190,190,191,191,191,191,192,192,192,192,192,193,193,193,193,193,193,193,193,194,194,194,194,194,194,194,195,195,195,195,195,195,195,195,196,196,196,196,196,196,197,197,197,197,197,197,197,197,197,197,198,198,198,198,198,198,198,198,199,199,199,199,199,199,199,199,200,200,200,200,201,201,201,201,201,201,201,201,201,202,202,202,202,202,202,202,202,202,202,202,202,202,203,203,203,203,203,203,203,203,203,203,204,204,204,204,205,205,205,205,205,206,206,206,206,206,206,206,207,207,207,207,207,207,207,208,208,208,208,208,208,208,208,208,208,209,209,209,209,209,210,210,210,210,210,210,210,211,211,211,211,211,211,211,211,211,211,211,211,211,212,212,212,212,212,213,213,213,213,213,213,213,214,214,214,214,215,215,216,216,216,216,216,216,216,216,217,217,218,218,218,218,218,219,219,219,219,219,219,220,220,220,220,220,220,220,220,220,220,221,221,221,221,221,222,222,222,222,222,222,222,223,223,223,223,223,223,223,223,223,224,224,224,224,224,224,225,225,225,225,225,225,225,226,226,226,226,226,227,227,227,227,227,227,227,227,227,227,228,228,228,228,228,228,228,228,228,228,229,229,230,230,230,230,230,230,230,230,231,231,231,231,231,231,231,231,231,231,231,232,232,232,232,232,232,232,233,233,233,233,233,233,234,234,234,234,234,234,234,235,235,235,235,235,235,235,236,236,236,236,236,236,237,237,237,237,237,237,237,237,238,238,238,238,238,238,238,238,238,239,239,239,239,239,239,240,240,240,240,240,240,240,240,241,241,241,241,241,242,242,242,242,242,242,242,243,243,244,244,244,244,244,244,244,245,245,246,246,246,246,246,246,246,247,247,247,247,247,247,247,248,248,248,248,248,248,248,248,248,249,249,249,249,249,249,249,250,250,250,250,250,250,250,250,250,251,251,252,252,252,252,252,252,252,253,253,253,253,253,254,254,255,255,255,255,255,256,256,256,256,256,257,257,257,257,257,257,257,257,258,258,258,258,258,258,258,258,258,258,258,259,259,259,259,259,259,259,260,260,260,260,260,260,260,260,260,260,260,260,261,261,261,261,261,261,261,262,262,262,262,262,262,262,262,262,262,262,263,263,263,263,263,263,263,264,264,264,264,264,264,264,264,264,265,265,265,265,265,265,266,266,266,266,266,266,266,266,267,267,267,267,267,267,267,268,268,269,269,269,269,269,269,269,269,270,270,270,270,270,270,270,270,270,270,271,271,271,271,271,271,271,272,272,272,272,272,272,273,273,273,273,273,273,273,274,274,274,274,274,274,274,274,275,275,275,275,275,275,275,276,276,276,277,277,278,278,278,278,278,278,278,279,279,279,279,279,280,280,280,280,280,280,280,280,280,281,281,281,281,281,281,281,281,281,281,282,282,282,282,282,283,283,283,283,284,284,284,284,284,285,285,285,285,285,285,286,286,286,286,286,286,286,287,287,288,288,288,288,288,288,288,289,289,289,289,289,289,289,289,289,289,289,290,290,290,290,290,290,290,290,290,290,290,290,290,290,291,291,291,291,291,291,292,292,292,292,292,292,293,293,294,294,294,294,294,294,295,295,296,296,296,296,296,296,296,297,297,297,297,297,297,298,298,298,298,298,299,299,299,299,299,299,299,300,300,300,300,300,301,301,301,301,301,301,301,302,302,302,303,303,303,303,303,303,303,304,304,304,304,304,304,305,305,305,305,305,305,305,306,306,306,306,306,307,307,307,307,307,307,307,308,308,308,308,308,308,308,308,308,309,309,309,309,309,309,309,309,309,309,310,310,310,310,310,310,311,311,311,311,311,311,311,312,312,312,312,312,313,313,313,313,313,314,314,314,314,314,314,314,314,315,315,315,315,315,315,315,315,316,316,316,316,316,316,316,316,317,317,317,317,317,317,317,317,318,318,318,318,318,318,318,318,319,319,319,319,319,319,319,319,320,320,320,320,320,320,320,320,321,321,321,321,321,321,321,321,322,322,322,322,322,322,323,323,323,323,323,323,323,323,323,323,323,324,324,324,324,324,324,324,324,325,325,325,325,325,325,325,326,326,326,326,326,326,326,326,327,327,327,327,327,327,327,327,328,328,328,328,328,328,328,329,329,329,329,329,329,329,330,330,330,331,331,331,331,332,332,332,332,332,332,332,332,332,333,333,333,333,333,333,333,333,333,333,333,334,334,335,335,335,335,335,335,335,335,336,336,337,337,337,337,337,337,337,337,337,338,338,338,338,338,338,339,339,339,339,339,339,340,340,340,340,340,340,340,340,340,340,340,341,341,341,341,341,341,341,341,341,342,342,342,342,342,342,342,343,343,343,343,343,343,343,343,343,343,344,344,344,344,345,345,345,345,345,346,346,346,346,346,346,347,347,347,347,347,347,347,348,348,348,348,348,348,348,348,348,349,349,349,349,349,349,350,350,350,350,351,351,351,351,351,351,351,351,351,351,351,351,351,351,352,352,352,352,352,352,352,352,352,352,352,352,353,353,353,353,353,353,353,353,353,353,353,354,354,354,354,354,354,354,355,355,355,355,355,355,355,356,356,356,356,356,356,357,357,357,357,357,358,358,358,358,358,358,358,358,358,359,359,359,359,359,359,359,359,360,360,360,360,360,360,360,360,361,361,361,361,361,361,361,361,362,362,362,362,362,362,362,362,363,363,363,363,363,363,363,363,364,364,364,364,364,364,365,365,365,365,365,366,366,366,366,366,366,366,367,367,367,367,367,367,367,368,368,369,369,369,369,369,369,369,369,370,370,370,370,370,371,371,371,371,371,371,371,371,372,372,372,372,372,372,372,373,373,373,373,373,373,373,374,374,374,374,374,374,374,375,375,375,375,375,375,375,375,376,376,376,376,376,376,376,376,376,377,377,377,377,377,377,377,377,378,378,378,378,378,378,378,378,378,378,379,379,380,380,381,381,381,381,381,381,381,382,382,382,383,383,383,384,384,384,384,384,384,384,384,384,384,385,385,385,385,385,385,386,386,386,386,386,386,386,387,387,387,387,387,387,388,388,388,388,388,388,389,389,389,389,389,390,390,390,390,390,391,391,391,391,391,391,391,391,392,392,392,392,392,393,393,393,394,394,394,394,395,395,395,395,395,395,395,395,395,395,395,395,395,396,396,396,396,396,396,396,396,396,396,396,396,396,396,397,397,398,398,398,398,398,398,398,398,398,399,399,399,399,399,400,400,400,400,400,400,400,400,401,401,401,401,401,402,402,402,402,402,402,402,402,402,403,403,403,403,403,403,403,404,404,405,405,405,405,405,405,405,405,405,406,406,406,406,406,406,406,406,406,406,407,407,407,407,407,407,407,407,408,408,408,408,408,408,408,408,409,409,409,409,409,409,409,409,409,409,410,410,410,410,410,410,410,410,411,411,411,411,411,411,412,412,412,412,412,412,412,412,412,412,413,413,414,414,414,414,414,414,414,415,415,415,415,415,415,415,415,415,415,415,416,416,416,416,416,417,417,417,417,417,417,417,417,417,417,418,418,418,418,418,419,419,419,419,419,420,420,420,420,421,421,422,422,422,422,422,422,422,422,423,423,423,423,423,423,423,424,424,424,424,424,424,425,425,425,425,425,425,425,426,426,426,426,426,427,427,427,427,427,427,427,428,428,428,428,428,428,428,429,429,429,429,429,429,429,429,429,429,429,429,430,430,430,430,430,430,430,431,431,432,432,432,432,433,433,433,433,433,433,433,433,433,433,434,434,434,434,434,434,434,434,435,435,435,435,435,435,435,435,435,435,435,436,436,436,436,436,436,436,436,437,437,437,437,437,437,437,437,438,438,438,438,438,438,438,439,439,439,439,439,439,439,439,439,439,439,440,440,440,440,440,440,441,441,441,441,441,441,441,442,442,442,442,442,442,442,443,443,443,443,443,443,443,443,444,444,444,444,444,445,445,445,445,445,445,445,445,446,446,446,446,447,447,447,447,447,447,447,448,448,448,448,448,448,448,449,449,449,449,449,449,449,449,450,450,450,450,450,450,450,450,451,451,451,451,451,451,452,452,452,453,453,453,453,453,453,453,454,454,454,454,454,454,454,454,454,454,455,455,455,456,456,456,456,456,456,456,456,456,456,457,457,457,457,457,457,458,458,458,458,458,458,458,459,459,459,459,459,460,460,461,461,461,461,461,462,462,462,462,462,462,462,462,462,462,462,462,462,463,463,463,463,463,463,464,464,464,464,464,465,465,465,465,465,465,465,465,466,466,466,466,466,467,467,468,468,469,469,469,469,469,470,470,470,470,470,470,470,471,471,471,471,471,471,471,471,472,472,472,472,472,472,472,473,473,473,473,473,473,474,474,474,474,474,474,474,474,474,475,475,475,475,475,475,475,476,476,476,476,476,477,477,477,477,477,478,478,478,478,479,479,479,479,480,480,480,480,480,481,481,481,481,481,481,481,482,482,483,483,483,483,483,483,483,484,484,485,485,485,485,485,485,486,486,486,486,486,486,486,487,487,488,488,488,488,488,489,489,489,489,489,490,490,491,491,491,491,491,491,491,492,492,492,492,492,492,492,492,493,493,493,493,493,493,493,493,494,494,494,495,495,495,495,495,495,495,496,496,496,496,496,496,496,496,497,497,497,497,497,497,497,498,498,498,498,498,498,498,498,499,499,499,499,499,499,499,500,500,500,500,500,501,501,501,501,501,502,502,502,502,502,503,503,503,503,503,503,503,503,503],\"depth\":[9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,3,2,1,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,3,2,1,9,8,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,2,1,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,4,3,2,1,8,7,6,5,4,3,2,1,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,4,3,2,1,2,1,9,8,7,6,5,4,3,2,1,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,2,1,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,5,4,3,2,1,5,4,3,2,1,4,3,2,1,2,1,7,6,5,4,3,2,1,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,4,3,2,1,9,8,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,4,3,2,1,2,1,9,8,7,6,5,4,3,2,1,5,4,3,2,1,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,9,8,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,3,2,1,8,7,6,5,4,3,2,1,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,4,3,2,1,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,3,2,1,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,4,3,2,1,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,4,3,2,1,2,1,8,7,6,5,4,3,2,1,2,1,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,5,4,3,2,1,2,1,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,3,2,1,2,1,7,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,2,1,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,3,2,1,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,2,1,2,1,7,6,5,4,3,2,1,3,2,1,3,2,1,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,3,2,1,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,2,1,9,8,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,4,3,2,1,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,3,2,1,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,2,1,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,2,1,2,1,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,4,3,2,1,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,2,1,7,6,5,4,3,2,1,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,5,4,3,2,1,5,4,3,2,1,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1],\"label\":[\"structure\",\"as.data.frame.numeric\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"[\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"shuffle\",\"local\",\"\",\"FUN\",\"lapply\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"sample.default\",\"shuffle\",\"local\",\"~\",\"local\",\"%in%\",\"[[.data.frame\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"unique\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"lhs\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"sample.default\",\"shuffle\",\"local\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"parse.formula\",\"lhs.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"length\",\"parse.formula\",\"condition.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"c\",\"local\",\"rhs_or_expr\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"alist\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"rlang::is_formula\",\"mean_\",\"diffmean\",\"local\",\"rlang::eval_tidy\",\"FUN\",\"lapply\",\"*\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"diff.default\",\"diffmean\",\"local\",\"length\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"do.call\",\"FUN\",\"lapply\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\".deparseOpts\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"NextMethod\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"parse.formula\",\"condition.formula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"length\",\"parse.formula\",\"rhs.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"[[\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.numeric\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"levels\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"[[.data.frame\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"deparse\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"condition\",\"condition.formula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"is.call\",\"local\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\".External2\",\"local\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"match.arg\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"eval\",\"eval\",\"match.arg\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"pmatch\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.numeric\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"unique.default\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"rlang::is_formula\",\"rhs_or_expr\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"sys.function\",\"match.arg\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"$\",\"lhs.parsedFormula\",\"lhs.formula\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"parse.formula\",\"lhs.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"gsub\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"[.data.frame\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"$\",\"condition.parsedFormula\",\"condition.formula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"sum\",\"local\",\"deparse\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"parse.formula\",\"lhs.formula\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"length\",\"local\",\"gsub\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"sys.function\",\"match.arg\",\"order\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"row.names\",\"rhs.parsedFormula\",\"rhs.formula\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\".External2\",\"local\",\"force\",\"as.data.frame.numeric\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"enexpr\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"parse.formula\",\"rhs.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"lhs\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"diff.default\",\"diffmean\",\"local\",\"$\",\"lhs.parsedFormula\",\"lhs.formula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"mean_\",\"diffmean\",\"local\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.numeric\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"deparse\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"deparse\",\"deparse1\",\"as.data.frame.numeric\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"as.data.frame.numeric\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"%in%\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"\",\"FUN\",\"lapply\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"parse.formula\",\"rhs.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"match.arg\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"parse.formula\",\"lhs.formula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"deparse\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"sum\",\"local\",\"structure\",\"parse.formula\",\"rhs.formula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"lapply\",\"*\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"FUN\",\"lapply\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"enexpr\",\"rhs_or_expr\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"vapply\",\"order\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"lhs\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"eval\",\"match.arg\",\"order\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"parse.formula\",\"lhs.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\".External2\",\"local\",\"NextMethod\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"make.unique\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\".External2\",\"local\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"as.data.frame.vector\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"diffmean\",\"local\",\"paste\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"enexpr\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"unique\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"paste\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"rhs\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"sys.call\",\"alist\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"alist\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"pmatch\",\".deparseOpts\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"parse.formula\",\"condition.formula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"NextMethod\",\"[.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"rlang::is_formula\",\"mean_\",\"diffmean\",\"local\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.numeric\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"vapply\",\"order\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"rlang::is_formula\",\"mean_\",\"diffmean\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"deparse\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"parse.formula\",\"lhs.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"vapply\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"gsub\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.numeric\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"paste0\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"getOption\",\"mean_\",\"diffmean\",\"local\",\"...names\",\"local\",\"length\",\"parse.formula\",\"condition.formula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"any\",\"local\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"deparse\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"gsub\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"length\",\"parse.formula\",\"condition.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"as.list.default\",\"alist\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"deparse\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"rhs_or_expr\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"pmatch\",\".deparseOpts\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"vapply\",\"order\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"c\",\"local\",\"%in%\",\"deparse\",\"mode\",\"deparse\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"sys.function\",\"match.arg\",\"order\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"eval\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"\",\"mosaicCore::joinFrames\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"condition\",\"condition.formula\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"condition.parsedFormula\",\"condition.formula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"mode\",\"deparse\",\"mode\",\"deparse\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"dim.data.frame\",\"ncol\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"[[\",\"parse.formula\",\"lhs.formula\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"lhs\",\"lhs.formula\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"as.integer\",\"local\",\"\",\"%in%\",\"[.data.frame\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"attributes\",\"local\",\"structure\",\"parse.formula\",\"condition.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"names\",\"FUN\",\"lapply\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"any\",\"local\",\"structure\",\"parse.formula\",\"rhs.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.numeric\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"%in%\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.numeric\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"rhs_or_expr\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"names\",\"[.data.frame\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"any\",\"local\",\"is.factor\",\"gsub\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"shuffle\",\"local\",\"structure\",\"parse.formula\",\"lhs.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"condition.formula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"lapply\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"deparse\",\"mode\",\"deparse\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"mean_\",\"diffmean\",\"local\",\"enexpr\",\"rhs_or_expr\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"parse.formula\",\"rhs.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"lhs\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"eval\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"order\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"deparse\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"match.arg\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\".deparseOpts\",\"deparse\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"make.names\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"eval\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"deparse\",\"mode\",\"deparse\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"diff.default\",\"diffmean\",\"local\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"pmatch\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"match.fun\",\"vapply\",\"order\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"(\",\"local\",\"$\",\"lhs.parsedFormula\",\"lhs.formula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"list\",\"local\",\".deparseOpts\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"condition\",\"condition.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"dim\",\"ncol\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"%in%\",\"deparse\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\".deparseOpts\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"deparse\",\"deparse1\",\"as.data.frame.numeric\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"parse.formula\",\"condition.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"order\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"is.factor\",\"grep\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"%in%\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.character\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"pmatch\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"mode\",\"deparse\",\"deparse1\",\"as.data.frame.numeric\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"length\",\"parse.formula\",\"rhs.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"as.list\",\"alist\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"lapply\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"order\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"as.list.default\",\"alist\",\"FUN\",\"lapply\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"startsWith\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"\",\"structure\",\"parse.formula\",\"rhs.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"as.list\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"character\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"gsub\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"is.data.frame\",\"rownames\",\"structure\",\"parse.formula\",\"condition.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"rlang::enexpr\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"length\",\"local\",\"shuffle\",\"local\",\"deparse\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"length\",\"parse.formula\",\"condition.formula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"parse.formula\",\"rhs.formula\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"pmatch\",\".deparseOpts\",\"deparse\",\"mode\",\"deparse\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"data.frame\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"deparse\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"environment\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"sample.int\",\"base::sample\",\"sample.default\",\"shuffle\",\"local\",\"condition.parsedFormula\",\"condition.formula\",\"mosaicCore::mosaic_formula_q\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"length\",\"local\",\"structure\",\"parse.formula\",\"condition.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\".External2\",\"local\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"[[\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"attributes\",\"local\",\"grep\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"grep\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"names\",\"local\",\"structure\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"factor\",\"as.factor\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"sample\",\"shuffle\",\"local\",\"structure\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"parse.formula\",\"condition.formula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"interaction\",\"split.default\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"parse.formula\",\"rhs.formula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"structure\",\"parse.formula\",\"condition.formula\",\"mosaicCore::mosaic_formula_q\",\"mean_\",\"diffmean\",\"local\",\"deparse\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"gsub\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"grep\",\"maggregate\",\"mean_\",\"diffmean\",\"local\",\"%in%\",\".deparseOpts\",\"deparse\",\"evalSubFormula\",\"mosaicCore::evalFormula\",\"maggregate\",\"mean_\",\"diffmean\",\"local\"],\"filenum\":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],\"linenum\":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],\"memalloc\":[28.6987686157227,28.6987686157227,28.6987686157227,28.6987686157227,28.6987686157227,28.6987686157227,28.6987686157227,28.6987686157227,28.6987686157227,29.1891784667969,29.1891784667969,29.1891784667969,29.1891784667969,29.1891784667969,29.1891784667969,29.1891784667969,29.6740570068359,29.6740570068359,30.6693878173828,30.6693878173828,30.6693878173828,30.6693878173828,30.6693878173828,30.6693878173828,30.6693878173828,31.1491317749023,31.1491317749023,31.1491317749023,32.0458068847656,32.0458068847656,32.9710388183594,32.9710388183594,32.9710388183594,32.9710388183594,32.9710388183594,32.9710388183594,32.9710388183594,32.9710388183594,33.4404067993164,33.4404067993164,33.4404067993164,33.4404067993164,33.4404067993164,33.4404067993164,33.4404067993164,33.4404067993164,33.4404067993164,34.3660888671875,34.3660888671875,34.3660888671875,34.3660888671875,34.3660888671875,34.3660888671875,34.3660888671875,34.3660888671875,34.3660888671875,35.2992858886719,35.2992858886719,35.2992858886719,35.2992858886719,35.2992858886719,35.2992858886719,35.7754287719727,35.7754287719727,35.7754287719727,36.2434463500977,36.2434463500977,36.2434463500977,36.2434463500977,36.2434463500977,36.2434463500977,36.2434463500977,36.2434463500977,36.2434463500977,28.2878799438477,28.2878799438477,28.2878799438477,28.2878799438477,28.2878799438477,28.8211822509766,28.8211822509766,28.8211822509766,28.8211822509766,28.8211822509766,28.8211822509766,28.8211822509766,29.4347534179688,29.4347534179688,29.4347534179688,29.4347534179688,29.4347534179688,30.0958862304688,30.0958862304688,30.0958862304688,30.0958862304688,30.0958862304688,30.0958862304688,30.0958862304688,30.8663101196289,30.8663101196289,31.5713348388672,31.5713348388672,31.5713348388672,31.5713348388672,31.5713348388672,32.0727691650391,32.0727691650391,32.0727691650391,32.0727691650391,32.0727691650391,32.6383819580078,32.6383819580078,32.6383819580078,32.6383819580078,32.6383819580078,32.6383819580078,32.6383819580078,33.2125015258789,33.2125015258789,33.2125015258789,33.2125015258789,34.1846389770508,34.1846389770508,34.1846389770508,34.1846389770508,34.1846389770508,34.1846389770508,34.1846389770508,34.1846389770508,35.1567306518555,35.1567306518555,35.1567306518555,35.9982070922852,35.9982070922852,35.9982070922852,35.9982070922852,35.9982070922852,36.5636825561523,36.5636825561523,36.5636825561523,36.5636825561523,36.5636825561523,36.5636825561523,36.5636825561523,36.5636825561523,36.5636825561523,29.0336074829102,29.0336074829102,29.0336074829102,29.0336074829102,29.0336074829102,29.0336074829102,29.0336074829102,29.6117095947266,29.6117095947266,29.6117095947266,29.6117095947266,29.6117095947266,29.6117095947266,29.6117095947266,29.6117095947266,29.6117095947266,30.6039962768555,30.6039962768555,30.6039962768555,30.6039962768555,30.6039962768555,30.6039962768555,30.6039962768555,31.3678970336914,31.3678970336914,31.3678970336914,31.3678970336914,31.3678970336914,31.3678970336914,31.3678970336914,32.3549423217773,32.3549423217773,32.3549423217773,32.3549423217773,32.3549423217773,32.3549423217773,32.3549423217773,33.3281402587891,33.3281402587891,33.3281402587891,33.3281402587891,33.3281402587891,33.933349609375,33.933349609375,34.7912521362305,34.7912521362305,34.7912521362305,34.7912521362305,34.7912521362305,34.7912521362305,34.7912521362305,34.7912521362305,34.7912521362305,35.7634887695312,35.7634887695312,35.7634887695312,35.7634887695312,35.7634887695312,35.7634887695312,35.7634887695312,35.7634887695312,35.7634887695312,36.4136810302734,36.4136810302734,36.4136810302734,36.4136810302734,36.4136810302734,36.4136810302734,36.4136810302734,36.4136810302734,28.4593124389648,28.4593124389648,28.4593124389648,28.4593124389648,28.4593124389648,28.4593124389648,28.4593124389648,28.4593124389648,28.4593124389648,29.464111328125,29.464111328125,29.464111328125,29.464111328125,29.464111328125,29.464111328125,29.464111328125,29.464111328125,30.4088439941406,30.4088439941406,30.4088439941406,30.4088439941406,30.4088439941406,30.4088439941406,30.4088439941406,30.4088439941406,30.9611053466797,30.9611053466797,30.9611053466797,30.9611053466797,30.9611053466797,31.9653701782227,31.9653701782227,32.9557723999023,32.9557723999023,32.9557723999023,32.9557723999023,32.9557723999023,32.9557723999023,32.9557723999023,33.9424514770508,33.9424514770508,33.9424514770508,33.9424514770508,33.9424514770508,33.9424514770508,33.9424514770508,33.9424514770508,33.9424514770508,34.4343948364258,34.4343948364258,34.4343948364258,34.4343948364258,34.4343948364258,34.4343948364258,34.4343948364258,35.3689804077148,35.3689804077148,35.3689804077148,35.3689804077148,35.3689804077148,35.3689804077148,35.9109115600586,35.9109115600586,35.9109115600586,35.9109115600586,35.9109115600586,35.9109115600586,35.9109115600586,35.9109115600586,28.3206558227539,28.3206558227539,28.3206558227539,28.3206558227539,28.3206558227539,29.3217697143555,29.3217697143555,29.3217697143555,29.3217697143555,29.3217697143555,29.3217697143555,29.3217697143555,29.3217697143555,29.3217697143555,29.3217697143555,29.3217697143555,29.3217697143555,29.822883605957,29.822883605957,29.822883605957,29.822883605957,29.822883605957,29.822883605957,29.822883605957,29.822883605957,29.822883605957,29.822883605957,30.5635528564453,30.5635528564453,30.5635528564453,30.5635528564453,30.5635528564453,30.5635528564453,30.5635528564453,30.5635528564453,31.0490036010742,31.0490036010742,31.0490036010742,31.0490036010742,31.0490036010742,31.0490036010742,31.0490036010742,31.0490036010742,31.0490036010742,31.5369262695312,31.5369262695312,31.5369262695312,31.5369262695312,31.5369262695312,31.5369262695312,31.5369262695312,32.031364440918,32.031364440918,32.031364440918,32.031364440918,32.7195053100586,32.7195053100586,32.7195053100586,32.7195053100586,32.7195053100586,32.7195053100586,32.7195053100586,32.7195053100586,32.7195053100586,32.7195053100586,32.7195053100586,33.204216003418,33.204216003418,33.204216003418,33.204216003418,33.204216003418,33.7797317504883,33.7797317504883,33.7797317504883,33.7797317504883,33.7797317504883,33.7797317504883,33.7797317504883,33.7797317504883,33.7797317504883,33.7797317504883,33.7797317504883,34.5375213623047,34.5375213623047,34.5375213623047,34.5375213623047,34.5375213623047,34.5375213623047,34.5375213623047,34.5375213623047,35.2329559326172,35.2329559326172,35.2329559326172,35.2329559326172,35.2329559326172,35.2329559326172,35.2329559326172,35.2329559326172,35.2329559326172,35.2329559326172,35.2329559326172,36.1960372924805,36.1960372924805,36.1960372924805,36.1960372924805,36.1960372924805,36.1960372924805,36.1960372924805,36.1960372924805,28.2281036376953,28.2281036376953,28.2281036376953,28.2281036376953,28.2281036376953,28.2281036376953,28.2281036376953,28.2281036376953,28.2281036376953,28.2281036376953,28.2281036376953,28.2281036376953,28.2281036376953,28.2281036376953,28.5526580810547,28.5526580810547,28.5526580810547,28.5526580810547,28.5526580810547,28.5526580810547,28.5526580810547,29.0365600585938,29.0365600585938,29.0365600585938,29.0365600585938,29.0365600585938,29.0365600585938,29.0365600585938,30.0204162597656,30.0204162597656,30.0204162597656,30.0204162597656,30.0204162597656,30.0204162597656,30.0204162597656,31.0116500854492,31.0116500854492,31.0116500854492,31.0116500854492,31.0116500854492,31.0116500854492,31.0116500854492,31.0116500854492,31.0116500854492,31.0116500854492,31.9977951049805,31.9977951049805,31.9977951049805,31.9977951049805,31.9977951049805,31.9977951049805,31.9977951049805,32.6970901489258,32.6970901489258,33.4265823364258,33.4265823364258,33.4265823364258,33.4265823364258,33.4265823364258,33.4265823364258,33.4265823364258,33.4265823364258,34.3617172241211,34.3617172241211,34.3617172241211,34.3617172241211,35.2970428466797,35.2970428466797,35.7507705688477,35.7507705688477,35.7507705688477,35.7507705688477,35.7507705688477,35.7507705688477,35.7507705688477,35.7507705688477,35.7507705688477,36.2373046875,36.2373046875,36.2373046875,36.2373046875,28.3667068481445,28.3667068481445,28.3667068481445,28.3667068481445,28.3667068481445,28.3667068481445,28.3667068481445,28.3667068481445,28.3667068481445,28.3667068481445,28.3667068481445,28.8617553710938,28.8617553710938,28.8617553710938,28.8617553710938,28.8617553710938,28.8617553710938,28.8617553710938,28.8617553710938,28.8617553710938,28.8617553710938,29.5321655273438,29.5321655273438,29.5321655273438,29.5321655273438,29.5321655273438,29.5321655273438,29.5321655273438,29.5321655273438,29.5321655273438,29.5321655273438,29.5321655273438,29.5321655273438,30.0658569335938,30.0658569335938,30.0658569335938,30.0658569335938,30.0658569335938,30.0658569335938,30.0658569335938,30.0658569335938,30.0658569335938,30.0658569335938,30.0658569335938,30.0658569335938,30.0658569335938,30.0658569335938,30.5874710083008,30.5874710083008,30.5874710083008,30.5874710083008,30.5874710083008,30.5874710083008,30.5874710083008,30.5874710083008,30.5874710083008,31.5544738769531,31.5544738769531,31.5544738769531,31.5544738769531,31.5544738769531,31.5544738769531,31.5544738769531,31.5544738769531,31.5544738769531,32.5454864501953,32.5454864501953,32.5454864501953,32.5454864501953,32.5454864501953,32.5454864501953,32.5454864501953,33.1526565551758,33.1526565551758,33.1526565551758,33.1526565551758,33.1526565551758,33.1526565551758,33.1526565551758,33.1526565551758,33.1526565551758,33.1526565551758,33.1526565551758,33.7848663330078,33.7848663330078,33.7848663330078,33.7848663330078,33.7848663330078,33.7848663330078,33.7848663330078,33.7848663330078,34.5204849243164,34.5204849243164,34.5204849243164,34.5204849243164,34.5204849243164,34.5204849243164,34.5204849243164,35.5071029663086,35.5071029663086,35.5071029663086,35.5071029663086,35.5071029663086,35.5071029663086,35.5071029663086,36.4976348876953,36.4976348876953,36.4976348876953,36.4976348876953,36.4976348876953,28.5292816162109,28.5292816162109,28.5292816162109,28.5292816162109,28.5292816162109,29.5347061157227,29.5347061157227,29.5347061157227,29.5347061157227,29.5347061157227,29.5347061157227,29.5347061157227,29.5347061157227,30.2923049926758,30.2923049926758,31.0427856445312,31.0427856445312,31.0427856445312,31.0427856445312,31.0427856445312,31.0427856445312,31.0427856445312,32.0313491821289,32.0313491821289,32.0313491821289,32.0313491821289,32.0313491821289,32.0313491821289,32.0313491821289,32.0313491821289,33.0093383789062,33.0093383789062,33.0093383789062,33.0093383789062,33.0093383789062,33.0093383789062,33.640266418457,33.640266418457,34.2414627075195,34.2414627075195,34.2414627075195,34.2414627075195,34.2414627075195,34.8349227905273,34.8349227905273,34.8349227905273,34.8349227905273,34.8349227905273,34.8349227905273,34.8349227905273,34.8349227905273,34.8349227905273,34.8349227905273,34.8349227905273,35.5825271606445,35.5825271606445,35.5825271606445,35.5825271606445,35.5825271606445,36.118782043457,36.118782043457,36.118782043457,36.118782043457,36.118782043457,28.3472518920898,28.3472518920898,28.3472518920898,28.3472518920898,28.3472518920898,28.3472518920898,28.3472518920898,28.3472518920898,29.3491363525391,29.3491363525391,30.3543701171875,30.3543701171875,30.3543701171875,30.3543701171875,30.3543701171875,31.3584365844727,31.3584365844727,31.3584365844727,31.3584365844727,31.3584365844727,31.861686706543,31.861686706543,31.861686706543,31.861686706543,32.3647003173828,32.3647003173828,32.9048004150391,32.9048004150391,32.9048004150391,32.9048004150391,32.9048004150391,32.9048004150391,32.9048004150391,33.7407302856445,33.7407302856445,33.7407302856445,34.6439514160156,34.6439514160156,34.6439514160156,34.6439514160156,34.6439514160156,34.6439514160156,34.6439514160156,35.3845901489258,35.3845901489258,35.3845901489258,35.3845901489258,35.3845901489258,35.3845901489258,35.3845901489258,35.8655090332031,35.8655090332031,35.8655090332031,35.8655090332031,36.4751510620117,36.4751510620117,36.4751510620117,36.4751510620117,36.4751510620117,36.4751510620117,36.4751510620117,36.4751510620117,36.4751510620117,28.6486968994141,28.6486968994141,28.6486968994141,28.6486968994141,28.6486968994141,29.2059860229492,29.2059860229492,29.2059860229492,29.2059860229492,29.2059860229492,29.2059860229492,29.2059860229492,29.7463607788086,29.7463607788086,29.7463607788086,29.7463607788086,29.7463607788086,29.7463607788086,29.7463607788086,29.7463607788086,29.7463607788086,30.3280029296875,30.3280029296875,30.3280029296875,30.3280029296875,30.3280029296875,30.3280029296875,30.8473663330078,30.8473663330078,30.8473663330078,30.8473663330078,30.8473663330078,30.8473663330078,30.8473663330078,31.4646759033203,31.4646759033203,31.4646759033203,31.4646759033203,31.4646759033203,31.9639892578125,31.9639892578125,31.9639892578125,31.9639892578125,31.9639892578125,31.9639892578125,31.9639892578125,32.4380264282227,32.4380264282227,32.4380264282227,32.4380264282227,32.4380264282227,32.9748764038086,32.9748764038086,32.9748764038086,32.9748764038086,33.5792617797852,33.5792617797852,34.0654296875,34.0654296875,34.0654296875,34.0654296875,34.0654296875,34.0654296875,34.0654296875,34.0654296875,34.0654296875,34.5488433837891,34.5488433837891,34.5488433837891,34.5488433837891,34.5488433837891,35.0136642456055,35.0136642456055,35.4886703491211,35.4886703491211,35.4886703491211,35.4886703491211,35.4886703491211,36.0753936767578,36.0753936767578,36.0753936767578,36.0753936767578,36.0753936767578,36.0753936767578,36.0753936767578,36.0753936767578,36.0753936767578,28.52783203125,28.52783203125,28.52783203125,28.52783203125,28.52783203125,28.52783203125,28.52783203125,28.52783203125,28.52783203125,28.52783203125,28.52783203125,29.3633193969727,29.3633193969727,29.3633193969727,29.3633193969727,29.3633193969727,29.3633193969727,29.3633193969727,29.3633193969727,30.0372695922852,30.0372695922852,30.0372695922852,30.0372695922852,30.0372695922852,30.0372695922852,30.0372695922852,30.0372695922852,30.0372695922852,30.0372695922852,30.0372695922852,31.0450744628906,31.0450744628906,31.0450744628906,31.0450744628906,31.0450744628906,31.0450744628906,31.0450744628906,31.0450744628906,32.0503692626953,32.0503692626953,32.0503692626953,32.0503692626953,32.0503692626953,32.0503692626953,32.0503692626953,32.0503692626953,32.8455429077148,32.8455429077148,32.8455429077148,32.8455429077148,32.8455429077148,33.3578796386719,33.3578796386719,33.3578796386719,33.3578796386719,33.3578796386719,33.3578796386719,33.3578796386719,33.3578796386719,33.8525390625,33.8525390625,33.8525390625,33.8525390625,33.8525390625,34.3610153198242,34.3610153198242,34.3610153198242,34.3610153198242,34.3610153198242,34.3610153198242,34.3610153198242,34.3610153198242,35.1442413330078,35.1442413330078,35.1442413330078,35.1442413330078,35.1442413330078,35.1442413330078,35.1442413330078,35.1442413330078,35.6157073974609,35.6157073974609,35.6157073974609,35.6157073974609,35.6157073974609,35.6157073974609,35.6157073974609,36.1185607910156,36.1185607910156,36.1185607910156,36.1185607910156,36.1185607910156,36.1185607910156,36.5709381103516,36.5709381103516,36.5709381103516,36.5709381103516,36.5709381103516,36.5709381103516,36.5709381103516,36.5709381103516,28.8320388793945,28.8320388793945,29.3390197753906,29.3390197753906,29.3390197753906,29.3390197753906,29.3390197753906,29.3390197753906,29.3390197753906,29.3390197753906,29.3390197753906,29.973388671875,29.973388671875,29.973388671875,29.973388671875,29.973388671875,30.6767044067383,30.6767044067383,30.6767044067383,30.6767044067383,30.6767044067383,30.6767044067383,30.6767044067383,31.4211730957031,31.4211730957031,31.4211730957031,31.4211730957031,31.4211730957031,31.4211730957031,31.9833679199219,31.9833679199219,31.9833679199219,31.9833679199219,31.9833679199219,32.5009689331055,32.5009689331055,32.5009689331055,32.5009689331055,32.5009689331055,32.5009689331055,32.5009689331055,32.5009689331055,32.5009689331055,32.5009689331055,32.9887313842773,32.9887313842773,32.9887313842773,33.5962448120117,33.5962448120117,33.5962448120117,33.5962448120117,33.5962448120117,33.5962448120117,33.5962448120117,33.5962448120117,34.0952835083008,34.0952835083008,34.0952835083008,35.0745620727539,35.0745620727539,35.0745620727539,35.0745620727539,35.0745620727539,35.0745620727539,35.0745620727539,35.0745620727539,35.0745620727539,35.0745620727539,35.6079025268555,35.6079025268555,35.6079025268555,35.6079025268555,35.6079025268555,35.6079025268555,35.6079025268555,36.5509262084961,36.5509262084961,36.5509262084961,36.5509262084961,36.5509262084961,36.5509262084961,36.5509262084961,29.1143341064453,29.1143341064453,29.1143341064453,29.1143341064453,29.1143341064453,29.1143341064453,29.1143341064453,29.1143341064453,29.1143341064453,29.1143341064453,29.1143341064453,29.1143341064453,29.1143341064453,29.6186981201172,29.6186981201172,29.6186981201172,29.6186981201172,29.6186981201172,29.6186981201172,29.6186981201172,29.6186981201172,29.6186981201172,29.6186981201172,30.1205978393555,30.1205978393555,30.1205978393555,30.1205978393555,30.1205978393555,30.1205978393555,30.1205978393555,30.1205978393555,30.1205978393555,30.1205978393555,31.1264038085938,31.1264038085938,31.1264038085938,31.1264038085938,31.1264038085938,31.1264038085938,31.1264038085938,31.1264038085938,31.1264038085938,32.0031204223633,32.0031204223633,32.0031204223633,32.0031204223633,32.0031204223633,32.6285247802734,32.6285247802734,32.6285247802734,32.6285247802734,32.6285247802734,32.6285247802734,32.6285247802734,32.6285247802734,32.6285247802734,33.6334075927734,33.6334075927734,33.6334075927734,33.6334075927734,33.6334075927734,33.6334075927734,33.6334075927734,34.2729110717773,34.2729110717773,34.2729110717773,34.2729110717773,34.2729110717773,34.2729110717773,34.7821807861328,34.7821807861328,34.7821807861328,34.7821807861328,34.7821807861328,34.7821807861328,34.7821807861328,34.7821807861328,34.7821807861328,34.7821807861328,35.2479476928711,35.2479476928711,35.2479476928711,35.2479476928711,35.2479476928711,35.2479476928711,35.2479476928711,35.7292251586914,35.7292251586914,35.7292251586914,35.7292251586914,35.7292251586914,35.7292251586914,35.7292251586914,35.7292251586914,35.7292251586914,35.7292251586914,35.7292251586914,35.7292251586914,36.3250122070312,36.3250122070312,36.3250122070312,36.3250122070312,36.3250122070312,36.3250122070312,36.3250122070312,28.5292434692383,28.5292434692383,28.5292434692383,28.5292434692383,29.1479568481445,29.1479568481445,29.8431854248047,29.8431854248047,29.8431854248047,29.8431854248047,29.8431854248047,29.8431854248047,29.8431854248047,29.8431854248047,30.5132904052734,30.5132904052734,30.5132904052734,30.5132904052734,30.5132904052734,30.5132904052734,31.0083160400391,31.0083160400391,31.0083160400391,31.0083160400391,31.0083160400391,31.0083160400391,31.5091400146484,31.5091400146484,31.5091400146484,31.5091400146484,31.5091400146484,31.5091400146484,32.4848937988281,32.4848937988281,32.4848937988281,32.4848937988281,32.4848937988281,32.4848937988281,32.9759750366211,32.9759750366211,32.9759750366211,32.9759750366211,32.9759750366211,32.9759750366211,32.9759750366211,32.9759750366211,32.9759750366211,32.9759750366211,33.861213684082,33.861213684082,33.861213684082,33.861213684082,33.861213684082,33.861213684082,34.3622131347656,34.3622131347656,34.3622131347656,34.3622131347656,34.9374847412109,34.9374847412109,34.9374847412109,34.9374847412109,34.9374847412109,34.9374847412109,34.9374847412109,34.9374847412109,35.925895690918,35.925895690918,35.925895690918,35.925895690918,35.925895690918,35.925895690918,35.925895690918,35.925895690918,35.925895690918,35.925895690918,35.925895690918,28.4809417724609,28.4809417724609,28.4809417724609,29.4855117797852,29.4855117797852,29.4855117797852,29.4855117797852,29.4855117797852,29.4855117797852,29.4855117797852,30.4907302856445,30.4907302856445,31.0535125732422,31.0535125732422,31.0535125732422,31.0535125732422,31.0535125732422,31.0535125732422,31.0535125732422,31.5382843017578,31.5382843017578,31.5382843017578,31.5382843017578,31.5382843017578,31.5382843017578,32.0396728515625,32.0396728515625,32.0396728515625,32.0396728515625,32.0396728515625,32.0396728515625,32.0396728515625,32.0396728515625,32.6099166870117,32.6099166870117,32.6099166870117,32.6099166870117,32.6099166870117,32.6099166870117,32.6099166870117,32.6099166870117,32.6099166870117,33.3263626098633,33.3263626098633,33.3263626098633,33.3263626098633,33.8430862426758,33.8430862426758,34.3637542724609,34.3637542724609,34.3637542724609,34.3637542724609,34.3637542724609,34.3637542724609,34.3637542724609,34.3637542724609,34.3637542724609,34.3637542724609,35.0324554443359,35.0324554443359,35.0324554443359,35.0324554443359,35.0324554443359,35.0324554443359,35.0324554443359,35.0324554443359,35.0324554443359,35.5192642211914,35.5192642211914,36.0045547485352,36.0045547485352,36.0045547485352,36.0045547485352,36.0045547485352,36.0045547485352,36.0045547485352,36.0045547485352,36.0045547485352,36.0045547485352,36.4758987426758,36.4758987426758,36.4758987426758,36.4758987426758,36.4758987426758,36.4758987426758,36.4758987426758,28.8071746826172,28.8071746826172,28.8071746826172,28.8071746826172,28.8071746826172,28.8071746826172,29.4978713989258,29.4978713989258,29.4978713989258,29.4978713989258,29.4978713989258,29.4978713989258,29.4978713989258,29.4978713989258,29.4978713989258,29.9949798583984,29.9949798583984,29.9949798583984,29.9949798583984,29.9949798583984,29.9949798583984,29.9949798583984,29.9949798583984,29.9949798583984,30.5876388549805,30.5876388549805,30.5876388549805,30.5876388549805,30.5876388549805,30.5876388549805,30.5876388549805,30.5876388549805,31.2001647949219,31.2001647949219,31.2001647949219,31.2001647949219,31.2001647949219,31.2001647949219,31.2001647949219,31.2001647949219,31.2001647949219,31.2001647949219,31.8624496459961,31.8624496459961,31.8624496459961,31.8624496459961,31.8624496459961,31.8624496459961,32.359016418457,32.359016418457,32.359016418457,32.359016418457,33.3401260375977,33.3401260375977,33.3401260375977,33.3401260375977,33.3401260375977,34.3273010253906,34.3273010253906,34.3273010253906,34.3273010253906,34.3273010253906,34.3273010253906,34.3273010253906,34.3273010253906,35.3092346191406,35.3092346191406,35.3092346191406,35.3092346191406,35.3092346191406,35.3092346191406,35.3092346191406,35.8033142089844,35.8033142089844,35.8033142089844,35.8033142089844,35.8033142089844,35.8033142089844,35.8033142089844,35.8033142089844,36.5496673583984,36.5496673583984,36.5496673583984,36.5496673583984,36.5496673583984,36.5496673583984,28.7374877929688,28.7374877929688,28.7374877929688,28.7374877929688,28.7374877929688,28.7374877929688,28.7374877929688,28.7374877929688,28.7374877929688,28.7374877929688,29.621940612793,29.621940612793,29.621940612793,29.621940612793,29.621940612793,29.621940612793,29.621940612793,29.621940612793,30.6346435546875,30.6346435546875,30.6346435546875,30.6346435546875,30.6346435546875,30.6346435546875,30.6346435546875,30.6346435546875,31.6474838256836,31.6474838256836,31.6474838256836,31.6474838256836,32.6037750244141,32.6037750244141,32.6037750244141,32.6037750244141,32.6037750244141,32.6037750244141,32.6037750244141,32.6037750244141,32.6037750244141,33.0935211181641,33.0935211181641,33.0935211181641,33.0935211181641,33.0935211181641,33.0935211181641,33.0935211181641,33.0935211181641,33.0935211181641,33.0935211181641,33.0935211181641,33.0935211181641,33.0935211181641,33.7826766967773,33.7826766967773,33.7826766967773,33.7826766967773,33.7826766967773,33.7826766967773,33.7826766967773,33.7826766967773,33.7826766967773,33.7826766967773,34.262321472168,34.262321472168,34.262321472168,34.262321472168,34.742057800293,34.742057800293,34.742057800293,34.742057800293,34.742057800293,35.4212265014648,35.4212265014648,35.4212265014648,35.4212265014648,35.4212265014648,35.4212265014648,35.4212265014648,35.905403137207,35.905403137207,35.905403137207,35.905403137207,35.905403137207,35.905403137207,35.905403137207,28.4373550415039,28.4373550415039,28.4373550415039,28.4373550415039,28.4373550415039,28.4373550415039,28.4373550415039,28.4373550415039,28.4373550415039,28.4373550415039,28.9997329711914,28.9997329711914,28.9997329711914,28.9997329711914,28.9997329711914,29.4842910766602,29.4842910766602,29.4842910766602,29.4842910766602,29.4842910766602,29.4842910766602,29.4842910766602,30.0774917602539,30.0774917602539,30.0774917602539,30.0774917602539,30.0774917602539,30.0774917602539,30.0774917602539,30.0774917602539,30.0774917602539,30.0774917602539,30.0774917602539,30.0774917602539,30.0774917602539,30.5741882324219,30.5741882324219,30.5741882324219,30.5741882324219,30.5741882324219,31.163215637207,31.163215637207,31.163215637207,31.163215637207,31.163215637207,31.163215637207,31.163215637207,31.6518325805664,31.6518325805664,31.6518325805664,31.6518325805664,32.2009201049805,32.2009201049805,32.7740249633789,32.7740249633789,32.7740249633789,32.7740249633789,32.7740249633789,32.7740249633789,32.7740249633789,32.7740249633789,33.30859375,33.30859375,33.7962417602539,33.7962417602539,33.7962417602539,33.7962417602539,33.7962417602539,34.3376846313477,34.3376846313477,34.3376846313477,34.3376846313477,34.3376846313477,34.3376846313477,34.9065017700195,34.9065017700195,34.9065017700195,34.9065017700195,34.9065017700195,34.9065017700195,34.9065017700195,34.9065017700195,34.9065017700195,34.9065017700195,35.4697799682617,35.4697799682617,35.4697799682617,35.4697799682617,35.4697799682617,36.4435043334961,36.4435043334961,36.4435043334961,36.4435043334961,36.4435043334961,36.4435043334961,36.4435043334961,29.0057983398438,29.0057983398438,29.0057983398438,29.0057983398438,29.0057983398438,29.0057983398438,29.0057983398438,29.0057983398438,29.0057983398438,30.004508972168,30.004508972168,30.004508972168,30.004508972168,30.004508972168,30.004508972168,30.9957656860352,30.9957656860352,30.9957656860352,30.9957656860352,30.9957656860352,30.9957656860352,30.9957656860352,31.6151351928711,31.6151351928711,31.6151351928711,31.6151351928711,31.6151351928711,32.0999984741211,32.0999984741211,32.0999984741211,32.0999984741211,32.0999984741211,32.0999984741211,32.0999984741211,32.0999984741211,32.0999984741211,32.0999984741211,32.7431106567383,32.7431106567383,32.7431106567383,32.7431106567383,32.7431106567383,32.7431106567383,32.7431106567383,32.7431106567383,32.7431106567383,32.7431106567383,33.2236633300781,33.2236633300781,34.1933441162109,34.1933441162109,34.1933441162109,34.1933441162109,34.1933441162109,34.1933441162109,34.1933441162109,34.1933441162109,35.1566543579102,35.1566543579102,35.1566543579102,35.1566543579102,35.1566543579102,35.1566543579102,35.1566543579102,35.1566543579102,35.1566543579102,35.1566543579102,35.1566543579102,35.9800872802734,35.9800872802734,35.9800872802734,35.9800872802734,35.9800872802734,35.9800872802734,35.9800872802734,36.5949935913086,36.5949935913086,36.5949935913086,36.5949935913086,36.5949935913086,36.5949935913086,28.8724136352539,28.8724136352539,28.8724136352539,28.8724136352539,28.8724136352539,28.8724136352539,28.8724136352539,29.8610763549805,29.8610763549805,29.8610763549805,29.8610763549805,29.8610763549805,29.8610763549805,29.8610763549805,30.8503189086914,30.8503189086914,30.8503189086914,30.8503189086914,30.8503189086914,30.8503189086914,31.3484878540039,31.3484878540039,31.3484878540039,31.3484878540039,31.3484878540039,31.3484878540039,31.3484878540039,31.3484878540039,31.8363037109375,31.8363037109375,31.8363037109375,31.8363037109375,31.8363037109375,31.8363037109375,31.8363037109375,31.8363037109375,31.8363037109375,32.7441864013672,32.7441864013672,32.7441864013672,32.7441864013672,32.7441864013672,32.7441864013672,33.7319259643555,33.7319259643555,33.7319259643555,33.7319259643555,33.7319259643555,33.7319259643555,33.7319259643555,33.7319259643555,34.2290954589844,34.2290954589844,34.2290954589844,34.2290954589844,34.2290954589844,35.2074127197266,35.2074127197266,35.2074127197266,35.2074127197266,35.2074127197266,35.2074127197266,35.2074127197266,35.7019577026367,35.7019577026367,36.5995254516602,36.5995254516602,36.5995254516602,36.5995254516602,36.5995254516602,36.5995254516602,36.5995254516602,29.2799682617188,29.2799682617188,30.2861480712891,30.2861480712891,30.2861480712891,30.2861480712891,30.2861480712891,30.2861480712891,30.2861480712891,31.2089996337891,31.2089996337891,31.2089996337891,31.2089996337891,31.2089996337891,31.2089996337891,31.2089996337891,31.7103729248047,31.7103729248047,31.7103729248047,31.7103729248047,31.7103729248047,31.7103729248047,31.7103729248047,31.7103729248047,31.7103729248047,32.7005844116211,32.7005844116211,32.7005844116211,32.7005844116211,32.7005844116211,32.7005844116211,32.7005844116211,33.3425827026367,33.3425827026367,33.3425827026367,33.3425827026367,33.3425827026367,33.3425827026367,33.3425827026367,33.3425827026367,33.3425827026367,34.0414428710938,34.0414428710938,34.525016784668,34.525016784668,34.525016784668,34.525016784668,34.525016784668,34.525016784668,34.525016784668,35.5139236450195,35.5139236450195,35.5139236450195,35.5139236450195,35.5139236450195,36.4920883178711,36.4920883178711,29.0859756469727,29.0859756469727,29.0859756469727,29.0859756469727,29.0859756469727,29.5887222290039,29.5887222290039,29.5887222290039,29.5887222290039,29.5887222290039,30.0908737182617,30.0908737182617,30.0908737182617,30.0908737182617,30.0908737182617,30.0908737182617,30.0908737182617,30.0908737182617,30.5844573974609,30.5844573974609,30.5844573974609,30.5844573974609,30.5844573974609,30.5844573974609,30.5844573974609,30.5844573974609,30.5844573974609,30.5844573974609,30.5844573974609,31.5845642089844,31.5845642089844,31.5845642089844,31.5845642089844,31.5845642089844,31.5845642089844,31.5845642089844,32.5774688720703,32.5774688720703,32.5774688720703,32.5774688720703,32.5774688720703,32.5774688720703,32.5774688720703,32.5774688720703,32.5774688720703,32.5774688720703,32.5774688720703,32.5774688720703,33.576545715332,33.576545715332,33.576545715332,33.576545715332,33.576545715332,33.576545715332,33.576545715332,34.0660018920898,34.0660018920898,34.0660018920898,34.0660018920898,34.0660018920898,34.0660018920898,34.0660018920898,34.0660018920898,34.0660018920898,34.0660018920898,34.0660018920898,34.8515625,34.8515625,34.8515625,34.8515625,34.8515625,34.8515625,34.8515625,35.5418853759766,35.5418853759766,35.5418853759766,35.5418853759766,35.5418853759766,35.5418853759766,35.5418853759766,35.5418853759766,35.5418853759766,36.5296020507812,36.5296020507812,36.5296020507812,36.5296020507812,36.5296020507812,36.5296020507812,28.6249771118164,28.6249771118164,28.6249771118164,28.6249771118164,28.6249771118164,28.6249771118164,28.6249771118164,28.6249771118164,29.6239624023438,29.6239624023438,29.6239624023438,29.6239624023438,29.6239624023438,29.6239624023438,29.6239624023438,30.6277694702148,30.6277694702148,31.4217987060547,31.4217987060547,31.4217987060547,31.4217987060547,31.4217987060547,31.4217987060547,31.4217987060547,31.4217987060547,32.1233139038086,32.1233139038086,32.1233139038086,32.1233139038086,32.1233139038086,32.1233139038086,32.1233139038086,32.1233139038086,32.1233139038086,32.1233139038086,32.6223068237305,32.6223068237305,32.6223068237305,32.6223068237305,32.6223068237305,32.6223068237305,32.6223068237305,33.6013565063477,33.6013565063477,33.6013565063477,33.6013565063477,33.6013565063477,33.6013565063477,34.5858917236328,34.5858917236328,34.5858917236328,34.5858917236328,34.5858917236328,34.5858917236328,34.5858917236328,35.5741271972656,35.5741271972656,35.5741271972656,35.5741271972656,35.5741271972656,35.5741271972656,35.5741271972656,35.5741271972656,36.0725479125977,36.0725479125977,36.0725479125977,36.0725479125977,36.0725479125977,36.0725479125977,36.0725479125977,36.5521087646484,36.5521087646484,36.5521087646484,29.1489639282227,29.1489639282227,29.7839660644531,29.7839660644531,29.7839660644531,29.7839660644531,29.7839660644531,29.7839660644531,29.7839660644531,30.2849807739258,30.2849807739258,30.2849807739258,30.2849807739258,30.2849807739258,31.2821273803711,31.2821273803711,31.2821273803711,31.2821273803711,31.2821273803711,31.2821273803711,31.2821273803711,31.2821273803711,31.2821273803711,32.1825561523438,32.1825561523438,32.1825561523438,32.1825561523438,32.1825561523438,32.1825561523438,32.1825561523438,32.1825561523438,32.1825561523438,32.1825561523438,32.9010467529297,32.9010467529297,32.9010467529297,32.9010467529297,32.9010467529297,33.6386871337891,33.6386871337891,33.6386871337891,33.6386871337891,34.1769485473633,34.1769485473633,34.1769485473633,34.1769485473633,34.1769485473633,34.6519775390625,34.6519775390625,34.6519775390625,34.6519775390625,34.6519775390625,34.6519775390625,35.2797393798828,35.2797393798828,35.2797393798828,35.2797393798828,35.2797393798828,35.2797393798828,35.2797393798828,35.9525985717773,35.9525985717773,36.4895324707031,36.4895324707031,36.4895324707031,36.4895324707031,36.4895324707031,36.4895324707031,36.4895324707031,28.6053619384766,28.6053619384766,28.6053619384766,28.6053619384766,28.6053619384766,28.6053619384766,28.6053619384766,28.6053619384766,28.6053619384766,28.6053619384766,28.6053619384766,29.2092666625977,29.2092666625977,29.2092666625977,29.2092666625977,29.2092666625977,29.2092666625977,29.2092666625977,29.2092666625977,29.2092666625977,29.2092666625977,29.2092666625977,29.2092666625977,29.2092666625977,29.2092666625977,29.777702331543,29.777702331543,29.777702331543,29.777702331543,29.777702331543,29.777702331543,30.4649200439453,30.4649200439453,30.4649200439453,30.4649200439453,30.4649200439453,30.4649200439453,31.0510177612305,31.0510177612305,31.5497131347656,31.5497131347656,31.5497131347656,31.5497131347656,31.5497131347656,31.5497131347656,32.0961380004883,32.0961380004883,32.5910873413086,32.5910873413086,32.5910873413086,32.5910873413086,32.5910873413086,32.5910873413086,32.5910873413086,33.0794906616211,33.0794906616211,33.0794906616211,33.0794906616211,33.0794906616211,33.0794906616211,33.6614227294922,33.6614227294922,33.6614227294922,33.6614227294922,33.6614227294922,34.2171630859375,34.2171630859375,34.2171630859375,34.2171630859375,34.2171630859375,34.2171630859375,34.2171630859375,35.0146789550781,35.0146789550781,35.0146789550781,35.0146789550781,35.0146789550781,36.0083999633789,36.0083999633789,36.0083999633789,36.0083999633789,36.0083999633789,36.0083999633789,36.0083999633789,36.5463562011719,36.5463562011719,36.5463562011719,29.0607528686523,29.0607528686523,29.0607528686523,29.0607528686523,29.0607528686523,29.0607528686523,29.0607528686523,29.6131362915039,29.6131362915039,29.6131362915039,29.6131362915039,29.6131362915039,29.6131362915039,30.6005249023438,30.6005249023438,30.6005249023438,30.6005249023438,30.6005249023438,30.6005249023438,30.6005249023438,31.1697082519531,31.1697082519531,31.1697082519531,31.1697082519531,31.1697082519531,31.9155654907227,31.9155654907227,31.9155654907227,31.9155654907227,31.9155654907227,31.9155654907227,31.9155654907227,32.5509796142578,32.5509796142578,32.5509796142578,32.5509796142578,32.5509796142578,32.5509796142578,32.5509796142578,32.5509796142578,32.5509796142578,33.5224304199219,33.5224304199219,33.5224304199219,33.5224304199219,33.5224304199219,33.5224304199219,33.5224304199219,33.5224304199219,33.5224304199219,33.5224304199219,34.0139389038086,34.0139389038086,34.0139389038086,34.0139389038086,34.0139389038086,34.0139389038086,34.9872360229492,34.9872360229492,34.9872360229492,34.9872360229492,34.9872360229492,34.9872360229492,34.9872360229492,35.5980529785156,35.5980529785156,35.5980529785156,35.5980529785156,35.5980529785156,36.3426666259766,36.3426666259766,36.3426666259766,36.3426666259766,36.3426666259766,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,36.6179428100586,28.266960144043,28.266960144043,28.266960144043,28.266960144043,28.266960144043,28.266960144043,28.266960144043,28.266960144043,28.7607192993164,28.7607192993164,28.7607192993164,28.7607192993164,28.7607192993164,28.7607192993164,29.3388137817383,29.3388137817383,29.3388137817383,29.3388137817383,29.3388137817383,29.3388137817383,29.3388137817383,29.3388137817383,29.3388137817383,29.3388137817383,29.3388137817383,29.8329010009766,29.8329010009766,29.8329010009766,29.8329010009766,29.8329010009766,29.8329010009766,29.8329010009766,29.8329010009766,30.3790130615234,30.3790130615234,30.3790130615234,30.3790130615234,30.3790130615234,30.3790130615234,30.3790130615234,30.8710784912109,30.8710784912109,30.8710784912109,30.8710784912109,30.8710784912109,30.8710784912109,30.8710784912109,30.8710784912109,31.5085220336914,31.5085220336914,31.5085220336914,31.5085220336914,31.5085220336914,31.5085220336914,31.5085220336914,31.5085220336914,32.0080947875977,32.0080947875977,32.0080947875977,32.0080947875977,32.0080947875977,32.0080947875977,32.0080947875977,32.5060043334961,32.5060043334961,32.5060043334961,32.5060043334961,32.5060043334961,32.5060043334961,32.5060043334961,33.5126495361328,33.5126495361328,33.5126495361328,34.0152282714844,34.0152282714844,34.0152282714844,34.0152282714844,34.5172729492188,34.5172729492188,34.5172729492188,34.5172729492188,34.5172729492188,34.5172729492188,34.5172729492188,34.5172729492188,34.5172729492188,35.5139007568359,35.5139007568359,35.5139007568359,35.5139007568359,35.5139007568359,35.5139007568359,35.5139007568359,35.5139007568359,35.5139007568359,35.5139007568359,35.5139007568359,36.0020294189453,36.0020294189453,28.5976333618164,28.5976333618164,28.5976333618164,28.5976333618164,28.5976333618164,28.5976333618164,28.5976333618164,28.5976333618164,29.0824661254883,29.0824661254883,29.7737884521484,29.7737884521484,29.7737884521484,29.7737884521484,29.7737884521484,29.7737884521484,29.7737884521484,29.7737884521484,29.7737884521484,30.271484375,30.271484375,30.271484375,30.271484375,30.271484375,30.271484375,30.7590026855469,30.7590026855469,30.7590026855469,30.7590026855469,30.7590026855469,30.7590026855469,31.4154205322266,31.4154205322266,31.4154205322266,31.4154205322266,31.4154205322266,31.4154205322266,31.4154205322266,31.4154205322266,31.4154205322266,31.4154205322266,31.4154205322266,31.9193267822266,31.9193267822266,31.9193267822266,31.9193267822266,31.9193267822266,31.9193267822266,31.9193267822266,31.9193267822266,31.9193267822266,32.5228118896484,32.5228118896484,32.5228118896484,32.5228118896484,32.5228118896484,32.5228118896484,32.5228118896484,33.0576553344727,33.0576553344727,33.0576553344727,33.0576553344727,33.0576553344727,33.0576553344727,33.0576553344727,33.0576553344727,33.0576553344727,33.0576553344727,33.5569305419922,33.5569305419922,33.5569305419922,33.5569305419922,34.1451110839844,34.1451110839844,34.1451110839844,34.1451110839844,34.1451110839844,34.6961364746094,34.6961364746094,34.6961364746094,34.6961364746094,34.6961364746094,34.6961364746094,35.1927795410156,35.1927795410156,35.1927795410156,35.1927795410156,35.1927795410156,35.1927795410156,35.1927795410156,35.7238845825195,35.7238845825195,35.7238845825195,35.7238845825195,35.7238845825195,35.7238845825195,35.7238845825195,35.7238845825195,35.7238845825195,36.4366760253906,36.4366760253906,36.4366760253906,36.4366760253906,36.4366760253906,36.4366760253906,28.6419906616211,28.6419906616211,28.6419906616211,28.6419906616211,29.662467956543,29.662467956543,29.662467956543,29.662467956543,29.662467956543,29.662467956543,29.662467956543,29.662467956543,29.662467956543,29.662467956543,29.662467956543,29.662467956543,29.662467956543,29.662467956543,30.1560287475586,30.1560287475586,30.1560287475586,30.1560287475586,30.1560287475586,30.1560287475586,30.1560287475586,30.1560287475586,30.1560287475586,30.1560287475586,30.1560287475586,30.1560287475586,30.6575469970703,30.6575469970703,30.6575469970703,30.6575469970703,30.6575469970703,30.6575469970703,30.6575469970703,30.6575469970703,30.6575469970703,30.6575469970703,30.6575469970703,31.1505355834961,31.1505355834961,31.1505355834961,31.1505355834961,31.1505355834961,31.1505355834961,31.1505355834961,32.0614547729492,32.0614547729492,32.0614547729492,32.0614547729492,32.0614547729492,32.0614547729492,32.0614547729492,32.563720703125,32.563720703125,32.563720703125,32.563720703125,32.563720703125,32.563720703125,33.3997116088867,33.3997116088867,33.3997116088867,33.3997116088867,33.3997116088867,34.0617904663086,34.0617904663086,34.0617904663086,34.0617904663086,34.0617904663086,34.0617904663086,34.0617904663086,34.0617904663086,34.0617904663086,34.7406845092773,34.7406845092773,34.7406845092773,34.7406845092773,34.7406845092773,34.7406845092773,34.7406845092773,34.7406845092773,35.5647811889648,35.5647811889648,35.5647811889648,35.5647811889648,35.5647811889648,35.5647811889648,35.5647811889648,35.5647811889648,36.5537490844727,36.5537490844727,36.5537490844727,36.5537490844727,36.5537490844727,36.5537490844727,36.5537490844727,36.5537490844727,29.1593246459961,29.1593246459961,29.1593246459961,29.1593246459961,29.1593246459961,29.1593246459961,29.1593246459961,29.1593246459961,29.6622772216797,29.6622772216797,29.6622772216797,29.6622772216797,29.6622772216797,29.6622772216797,29.6622772216797,29.6622772216797,30.264892578125,30.264892578125,30.264892578125,30.264892578125,30.264892578125,30.264892578125,30.8311996459961,30.8311996459961,30.8311996459961,30.8311996459961,30.8311996459961,31.3384704589844,31.3384704589844,31.3384704589844,31.3384704589844,31.3384704589844,31.3384704589844,31.3384704589844,31.9711456298828,31.9711456298828,31.9711456298828,31.9711456298828,31.9711456298828,31.9711456298828,31.9711456298828,32.4602813720703,32.4602813720703,33.0805816650391,33.0805816650391,33.0805816650391,33.0805816650391,33.0805816650391,33.0805816650391,33.0805816650391,33.0805816650391,33.5802307128906,33.5802307128906,33.5802307128906,33.5802307128906,33.5802307128906,34.0859069824219,34.0859069824219,34.0859069824219,34.0859069824219,34.0859069824219,34.0859069824219,34.0859069824219,34.0859069824219,34.7735061645508,34.7735061645508,34.7735061645508,34.7735061645508,34.7735061645508,34.7735061645508,34.7735061645508,35.5267105102539,35.5267105102539,35.5267105102539,35.5267105102539,35.5267105102539,35.5267105102539,35.5267105102539,36.0345764160156,36.0345764160156,36.0345764160156,36.0345764160156,36.0345764160156,36.0345764160156,36.0345764160156,36.516716003418,36.516716003418,36.516716003418,36.516716003418,36.516716003418,36.516716003418,36.516716003418,36.516716003418,28.6824722290039,28.6824722290039,28.6824722290039,28.6824722290039,28.6824722290039,28.6824722290039,28.6824722290039,28.6824722290039,28.6824722290039,29.180908203125,29.180908203125,29.180908203125,29.180908203125,29.180908203125,29.180908203125,29.180908203125,29.180908203125,30.0731735229492,30.0731735229492,30.0731735229492,30.0731735229492,30.0731735229492,30.0731735229492,30.0731735229492,30.0731735229492,30.0731735229492,30.0731735229492,30.5760192871094,30.5760192871094,31.2192306518555,31.2192306518555,31.7072219848633,31.7072219848633,31.7072219848633,31.7072219848633,31.7072219848633,31.7072219848633,31.7072219848633,32.2373733520508,32.2373733520508,32.2373733520508,32.8991928100586,32.8991928100586,32.8991928100586,33.3920059204102,33.3920059204102,33.3920059204102,33.3920059204102,33.3920059204102,33.3920059204102,33.3920059204102,33.3920059204102,33.3920059204102,33.3920059204102,33.9046325683594,33.9046325683594,33.9046325683594,33.9046325683594,33.9046325683594,33.9046325683594,34.372428894043,34.372428894043,34.372428894043,34.372428894043,34.372428894043,34.372428894043,34.372428894043,34.9784088134766,34.9784088134766,34.9784088134766,34.9784088134766,34.9784088134766,34.9784088134766,35.4493789672852,35.4493789672852,35.4493789672852,35.4493789672852,35.4493789672852,35.4493789672852,35.9831008911133,35.9831008911133,35.9831008911133,35.9831008911133,35.9831008911133,28.3251266479492,28.3251266479492,28.3251266479492,28.3251266479492,28.3251266479492,28.8284912109375,28.8284912109375,28.8284912109375,28.8284912109375,28.8284912109375,28.8284912109375,28.8284912109375,28.8284912109375,29.3655242919922,29.3655242919922,29.3655242919922,29.3655242919922,29.3655242919922,30.2037658691406,30.2037658691406,30.2037658691406,31.1847915649414,31.1847915649414,31.1847915649414,31.1847915649414,32.1972427368164,32.1972427368164,32.1972427368164,32.1972427368164,32.1972427368164,32.1972427368164,32.1972427368164,32.1972427368164,32.1972427368164,32.1972427368164,32.1972427368164,32.1972427368164,32.1972427368164,32.699836730957,32.699836730957,32.699836730957,32.699836730957,32.699836730957,32.699836730957,32.699836730957,32.699836730957,32.699836730957,32.699836730957,32.699836730957,32.699836730957,32.699836730957,32.699836730957,33.4398498535156,33.4398498535156,33.9304428100586,33.9304428100586,33.9304428100586,33.9304428100586,33.9304428100586,33.9304428100586,33.9304428100586,33.9304428100586,33.9304428100586,34.4313049316406,34.4313049316406,34.4313049316406,34.4313049316406,34.4313049316406,35.1919555664062,35.1919555664062,35.1919555664062,35.1919555664062,35.1919555664062,35.1919555664062,35.1919555664062,35.1919555664062,35.9361419677734,35.9361419677734,35.9361419677734,35.9361419677734,35.9361419677734,36.4292907714844,36.4292907714844,36.4292907714844,36.4292907714844,36.4292907714844,36.4292907714844,36.4292907714844,36.4292907714844,36.4292907714844,28.6974868774414,28.6974868774414,28.6974868774414,28.6974868774414,28.6974868774414,28.6974868774414,28.6974868774414,29.1939315795898,29.1939315795898,30.1885299682617,30.1885299682617,30.1885299682617,30.1885299682617,30.1885299682617,30.1885299682617,30.1885299682617,30.1885299682617,30.1885299682617,30.6842498779297,30.6842498779297,30.6842498779297,30.6842498779297,30.6842498779297,30.6842498779297,30.6842498779297,30.6842498779297,30.6842498779297,30.6842498779297,31.4622573852539,31.4622573852539,31.4622573852539,31.4622573852539,31.4622573852539,31.4622573852539,31.4622573852539,31.4622573852539,31.9488677978516,31.9488677978516,31.9488677978516,31.9488677978516,31.9488677978516,31.9488677978516,31.9488677978516,31.9488677978516,32.6460037231445,32.6460037231445,32.6460037231445,32.6460037231445,32.6460037231445,32.6460037231445,32.6460037231445,32.6460037231445,32.6460037231445,32.6460037231445,33.1470260620117,33.1470260620117,33.1470260620117,33.1470260620117,33.1470260620117,33.1470260620117,33.1470260620117,33.1470260620117,33.6381912231445,33.6381912231445,33.6381912231445,33.6381912231445,33.6381912231445,33.6381912231445,34.1391448974609,34.1391448974609,34.1391448974609,34.1391448974609,34.1391448974609,34.1391448974609,34.1391448974609,34.1391448974609,34.1391448974609,34.1391448974609,34.6415557861328,34.6415557861328,35.6374435424805,35.6374435424805,35.6374435424805,35.6374435424805,35.6374435424805,35.6374435424805,35.6374435424805,36.139404296875,36.139404296875,36.139404296875,36.139404296875,36.139404296875,36.139404296875,36.139404296875,36.139404296875,36.139404296875,36.139404296875,36.139404296875,28.3504486083984,28.3504486083984,28.3504486083984,28.3504486083984,28.3504486083984,29.2560043334961,29.2560043334961,29.2560043334961,29.2560043334961,29.2560043334961,29.2560043334961,29.2560043334961,29.2560043334961,29.2560043334961,29.2560043334961,29.758544921875,29.758544921875,29.758544921875,29.758544921875,29.758544921875,30.2612991333008,30.2612991333008,30.2612991333008,30.2612991333008,30.2612991333008,31.2671203613281,31.2671203613281,31.2671203613281,31.2671203613281,32.2730941772461,32.2730941772461,32.7759552001953,32.7759552001953,32.7759552001953,32.7759552001953,32.7759552001953,32.7759552001953,32.7759552001953,32.7759552001953,33.3795394897461,33.3795394897461,33.3795394897461,33.3795394897461,33.3795394897461,33.3795394897461,33.3795394897461,34.2846298217773,34.2846298217773,34.2846298217773,34.2846298217773,34.2846298217773,34.2846298217773,34.7880554199219,34.7880554199219,34.7880554199219,34.7880554199219,34.7880554199219,34.7880554199219,34.7880554199219,35.2671890258789,35.2671890258789,35.2671890258789,35.2671890258789,35.2671890258789,36.2582473754883,36.2582473754883,36.2582473754883,36.2582473754883,36.2582473754883,36.2582473754883,36.2582473754883,28.860466003418,28.860466003418,28.860466003418,28.860466003418,28.860466003418,28.860466003418,28.860466003418,29.3607559204102,29.3607559204102,29.3607559204102,29.3607559204102,29.3607559204102,29.3607559204102,29.3607559204102,29.3607559204102,29.3607559204102,29.3607559204102,29.3607559204102,29.3607559204102,29.8615570068359,29.8615570068359,29.8615570068359,29.8615570068359,29.8615570068359,29.8615570068359,29.8615570068359,30.8508148193359,30.8508148193359,31.3467788696289,31.3467788696289,31.3467788696289,31.3467788696289,31.842041015625,31.842041015625,31.842041015625,31.842041015625,31.842041015625,31.842041015625,31.842041015625,31.842041015625,31.842041015625,31.842041015625,32.841438293457,32.841438293457,32.841438293457,32.841438293457,32.841438293457,32.841438293457,32.841438293457,32.841438293457,33.3297653198242,33.3297653198242,33.3297653198242,33.3297653198242,33.3297653198242,33.3297653198242,33.3297653198242,33.3297653198242,33.3297653198242,33.3297653198242,33.3297653198242,33.8215789794922,33.8215789794922,33.8215789794922,33.8215789794922,33.8215789794922,33.8215789794922,33.8215789794922,33.8215789794922,34.8183517456055,34.8183517456055,34.8183517456055,34.8183517456055,34.8183517456055,34.8183517456055,34.8183517456055,34.8183517456055,35.7998352050781,35.7998352050781,35.7998352050781,35.7998352050781,35.7998352050781,35.7998352050781,35.7998352050781,36.3012237548828,36.3012237548828,36.3012237548828,36.3012237548828,36.3012237548828,36.3012237548828,36.3012237548828,36.3012237548828,36.3012237548828,36.3012237548828,36.3012237548828,28.9095611572266,28.9095611572266,28.9095611572266,28.9095611572266,28.9095611572266,28.9095611572266,29.5108947753906,29.5108947753906,29.5108947753906,29.5108947753906,29.5108947753906,29.5108947753906,29.5108947753906,30.3335342407227,30.3335342407227,30.3335342407227,30.3335342407227,30.3335342407227,30.3335342407227,30.3335342407227,30.8381729125977,30.8381729125977,30.8381729125977,30.8381729125977,30.8381729125977,30.8381729125977,30.8381729125977,30.8381729125977,31.5042724609375,31.5042724609375,31.5042724609375,31.5042724609375,31.5042724609375,32.3447570800781,32.3447570800781,32.3447570800781,32.3447570800781,32.3447570800781,32.3447570800781,32.3447570800781,32.3447570800781,32.8472595214844,32.8472595214844,32.8472595214844,32.8472595214844,33.8503036499023,33.8503036499023,33.8503036499023,33.8503036499023,33.8503036499023,33.8503036499023,33.8503036499023,34.6874771118164,34.6874771118164,34.6874771118164,34.6874771118164,34.6874771118164,34.6874771118164,34.6874771118164,35.3578186035156,35.3578186035156,35.3578186035156,35.3578186035156,35.3578186035156,35.3578186035156,35.3578186035156,35.3578186035156,35.9331588745117,35.9331588745117,35.9331588745117,35.9331588745117,35.9331588745117,35.9331588745117,35.9331588745117,35.9331588745117,36.4341430664062,36.4341430664062,36.4341430664062,36.4341430664062,36.4341430664062,36.4341430664062,28.6276016235352,28.6276016235352,28.6276016235352,29.2173919677734,29.2173919677734,29.2173919677734,29.2173919677734,29.2173919677734,29.2173919677734,29.2173919677734,29.7188491821289,29.7188491821289,29.7188491821289,29.7188491821289,29.7188491821289,29.7188491821289,29.7188491821289,29.7188491821289,29.7188491821289,29.7188491821289,30.2119674682617,30.2119674682617,30.2119674682617,30.7244338989258,30.7244338989258,30.7244338989258,30.7244338989258,30.7244338989258,30.7244338989258,30.7244338989258,30.7244338989258,30.7244338989258,30.7244338989258,31.2417297363281,31.2417297363281,31.2417297363281,31.2417297363281,31.2417297363281,31.2417297363281,32.251579284668,32.251579284668,32.251579284668,32.251579284668,32.251579284668,32.251579284668,32.251579284668,32.7843704223633,32.7843704223633,32.7843704223633,32.7843704223633,32.7843704223633,33.3577575683594,33.3577575683594,33.8591156005859,33.8591156005859,33.8591156005859,33.8591156005859,33.8591156005859,34.4656753540039,34.4656753540039,34.4656753540039,34.4656753540039,34.4656753540039,34.4656753540039,34.4656753540039,34.4656753540039,34.4656753540039,34.4656753540039,34.4656753540039,34.4656753540039,34.4656753540039,35.1512145996094,35.1512145996094,35.1512145996094,35.1512145996094,35.1512145996094,35.1512145996094,35.6471481323242,35.6471481323242,35.6471481323242,35.6471481323242,35.6471481323242,36.5743865966797,36.5743865966797,36.5743865966797,36.5743865966797,36.5743865966797,36.5743865966797,36.5743865966797,36.5743865966797,29.2756958007812,29.2756958007812,29.2756958007812,29.2756958007812,29.2756958007812,30.2811584472656,30.2811584472656,31.124755859375,31.124755859375,31.652214050293,31.652214050293,31.652214050293,31.652214050293,31.652214050293,32.206657409668,32.206657409668,32.206657409668,32.206657409668,32.206657409668,32.206657409668,32.206657409668,32.7111206054688,32.7111206054688,32.7111206054688,32.7111206054688,32.7111206054688,32.7111206054688,32.7111206054688,32.7111206054688,33.3981704711914,33.3981704711914,33.3981704711914,33.3981704711914,33.3981704711914,33.3981704711914,33.3981704711914,33.9606475830078,33.9606475830078,33.9606475830078,33.9606475830078,33.9606475830078,33.9606475830078,34.5680847167969,34.5680847167969,34.5680847167969,34.5680847167969,34.5680847167969,34.5680847167969,34.5680847167969,34.5680847167969,34.5680847167969,35.0565643310547,35.0565643310547,35.0565643310547,35.0565643310547,35.0565643310547,35.0565643310547,35.0565643310547,35.9767608642578,35.9767608642578,35.9767608642578,35.9767608642578,35.9767608642578,28.5817108154297,28.5817108154297,28.5817108154297,28.5817108154297,28.5817108154297,29.0813140869141,29.0813140869141,29.0813140869141,29.0813140869141,29.5720291137695,29.5720291137695,29.5720291137695,29.5720291137695,30.0655364990234,30.0655364990234,30.0655364990234,30.0655364990234,30.0655364990234,30.6616592407227,30.6616592407227,30.6616592407227,30.6616592407227,30.6616592407227,30.6616592407227,30.6616592407227,31.1615219116211,31.1615219116211,32.1684875488281,32.1684875488281,32.1684875488281,32.1684875488281,32.1684875488281,32.1684875488281,32.1684875488281,33.1728134155273,33.1728134155273,34.1723556518555,34.1723556518555,34.1723556518555,34.1723556518555,34.1723556518555,34.1723556518555,35.1707458496094,35.1707458496094,35.1707458496094,35.1707458496094,35.1707458496094,35.1707458496094,35.1707458496094,36.1266174316406,36.1266174316406,28.5324401855469,28.5324401855469,28.5324401855469,28.5324401855469,28.5324401855469,29.5381317138672,29.5381317138672,29.5381317138672,29.5381317138672,29.5381317138672,30.5437774658203,30.5437774658203,31.0442962646484,31.0442962646484,31.0442962646484,31.0442962646484,31.0442962646484,31.0442962646484,31.0442962646484,31.5411148071289,31.5411148071289,31.5411148071289,31.5411148071289,31.5411148071289,31.5411148071289,31.5411148071289,31.5411148071289,32.0438842773438,32.0438842773438,32.0438842773438,32.0438842773438,32.0438842773438,32.0438842773438,32.0438842773438,32.0438842773438,32.766487121582,32.766487121582,32.766487121582,33.5248413085938,33.5248413085938,33.5248413085938,33.5248413085938,33.5248413085938,33.5248413085938,33.5248413085938,34.017822265625,34.017822265625,34.017822265625,34.017822265625,34.017822265625,34.017822265625,34.017822265625,34.017822265625,34.7319030761719,34.7319030761719,34.7319030761719,34.7319030761719,34.7319030761719,34.7319030761719,34.7319030761719,35.5150375366211,35.5150375366211,35.5150375366211,35.5150375366211,35.5150375366211,35.5150375366211,35.5150375366211,35.5150375366211,36.5141220092773,36.5141220092773,36.5141220092773,36.5141220092773,36.5141220092773,36.5141220092773,36.5141220092773,29.1552658081055,29.1552658081055,29.1552658081055,29.1552658081055,29.1552658081055,30.1609954833984,30.1609954833984,30.1609954833984,30.1609954833984,30.1609954833984,31.1655349731445,31.1655349731445,31.1655349731445,31.1655349731445,31.1655349731445,31.7896194458008,31.7896194458008,31.7896194458008,31.7896194458008,31.7896194458008,31.7896194458008,31.7896194458008,31.7896194458008,31.7896194458008],\"meminc\":[0,0,0,0,0,0,0,0,0,0.490409851074219,0,0,0,0,0,0,0.484878540039062,0,0.995330810546875,0,0,0,0,0,0,0.479743957519531,0,0,0.896675109863281,0,0.92523193359375,0,0,0,0,0,0,0,0.469367980957031,0,0,0,0,0,0,0,0,0.925682067871094,0,0,0,0,0,0,0,0,0.933197021484375,0,0,0,0,0,0.476142883300781,0,0,0.468017578125,0,0,0,0,0,0,0,0,-7.95556640625,0,0,0,0,0.533302307128906,0,0,0,0,0,0,0.613571166992188,0,0,0,0,0.6611328125,0,0,0,0,0,0,0.770423889160156,0,0.705024719238281,0,0,0,0,0.501434326171875,0,0,0,0,0.56561279296875,0,0,0,0,0,0,0.574119567871094,0,0,0,0.972137451171875,0,0,0,0,0,0,0,0.972091674804688,0,0,0.841476440429688,0,0,0,0,0.565475463867188,0,0,0,0,0,0,0,0,-7.53007507324219,0,0,0,0,0,0,0.578102111816406,0,0,0,0,0,0,0,0,0.992286682128906,0,0,0,0,0,0,0.763900756835938,0,0,0,0,0,0,0.987045288085938,0,0,0,0,0,0,0.973197937011719,0,0,0,0,0.605209350585938,0,0.857902526855469,0,0,0,0,0,0,0,0,0.972236633300781,0,0,0,0,0,0,0,0,0.650192260742188,0,0,0,0,0,0,0,-7.95436859130859,0,0,0,0,0,0,0,0,1.00479888916016,0,0,0,0,0,0,0,0.944732666015625,0,0,0,0,0,0,0,0.552261352539062,0,0,0,0,1.00426483154297,0,0.990402221679688,0,0,0,0,0,0,0.986679077148438,0,0,0,0,0,0,0,0,0.491943359375,0,0,0,0,0,0,0.934585571289062,0,0,0,0,0,0.54193115234375,0,0,0,0,0,0,0,-7.59025573730469,0,0,0,0,1.00111389160156,0,0,0,0,0,0,0,0,0,0,0,0.501113891601562,0,0,0,0,0,0,0,0,0,0.740669250488281,0,0,0,0,0,0,0,0.485450744628906,0,0,0,0,0,0,0,0,0.487922668457031,0,0,0,0,0,0,0.494438171386719,0,0,0,0.688140869140625,0,0,0,0,0,0,0,0,0,0,0.484710693359375,0,0,0,0,0.575515747070312,0,0,0,0,0,0,0,0,0,0,0.757789611816406,0,0,0,0,0,0,0,0.6954345703125,0,0,0,0,0,0,0,0,0,0,0.963081359863281,0,0,0,0,0,0,0,-7.96793365478516,0,0,0,0,0,0,0,0,0,0,0,0,0,0.324554443359375,0,0,0,0,0,0,0.483901977539062,0,0,0,0,0,0,0.983856201171875,0,0,0,0,0,0,0.991233825683594,0,0,0,0,0,0,0,0,0,0.98614501953125,0,0,0,0,0,0,0.699295043945312,0,0.7294921875,0,0,0,0,0,0,0,0.935134887695312,0,0,0,0.935325622558594,0,0.453727722167969,0,0,0,0,0,0,0,0,0.486534118652344,0,0,0,-7.87059783935547,0,0,0,0,0,0,0,0,0,0,0.495048522949219,0,0,0,0,0,0,0,0,0,0.67041015625,0,0,0,0,0,0,0,0,0,0,0,0.53369140625,0,0,0,0,0,0,0,0,0,0,0,0,0,0.521614074707031,0,0,0,0,0,0,0,0,0.967002868652344,0,0,0,0,0,0,0,0,0.991012573242188,0,0,0,0,0,0,0.607170104980469,0,0,0,0,0,0,0,0,0,0,0.632209777832031,0,0,0,0,0,0,0,0.735618591308594,0,0,0,0,0,0,0.986618041992188,0,0,0,0,0,0,0.990531921386719,0,0,0,0,-7.96835327148438,0,0,0,0,1.00542449951172,0,0,0,0,0,0,0,0.757598876953125,0,0.750480651855469,0,0,0,0,0,0,0.988563537597656,0,0,0,0,0,0,0,0.977989196777344,0,0,0,0,0,0.630928039550781,0,0.6011962890625,0,0,0,0,0.593460083007812,0,0,0,0,0,0,0,0,0,0,0.747604370117188,0,0,0,0,0.5362548828125,0,0,0,0,-7.77153015136719,0,0,0,0,0,0,0,1.00188446044922,0,1.00523376464844,0,0,0,0,1.00406646728516,0,0,0,0,0.503250122070312,0,0,0,0.503013610839844,0,0.54010009765625,0,0,0,0,0,0,0.835929870605469,0,0,0.903221130371094,0,0,0,0,0,0,0.740638732910156,0,0,0,0,0,0,0.480918884277344,0,0,0,0.609642028808594,0,0,0,0,0,0,0,0,-7.82645416259766,0,0,0,0,0.557289123535156,0,0,0,0,0,0,0.540374755859375,0,0,0,0,0,0,0,0,0.581642150878906,0,0,0,0,0,0.519363403320312,0,0,0,0,0,0,0.6173095703125,0,0,0,0,0.499313354492188,0,0,0,0,0,0,0.474037170410156,0,0,0,0,0.536849975585938,0,0,0,0.604385375976562,0,0.486167907714844,0,0,0,0,0,0,0,0,0.483413696289062,0,0,0,0,0.464820861816406,0,0.475006103515625,0,0,0,0,0.586723327636719,0,0,0,0,0,0,0,0,-7.54756164550781,0,0,0,0,0,0,0,0,0,0,0.835487365722656,0,0,0,0,0,0,0,0.6739501953125,0,0,0,0,0,0,0,0,0,0,1.00780487060547,0,0,0,0,0,0,0,1.00529479980469,0,0,0,0,0,0,0,0.795173645019531,0,0,0,0,0.512336730957031,0,0,0,0,0,0,0,0.494659423828125,0,0,0,0,0.508476257324219,0,0,0,0,0,0,0,0.783226013183594,0,0,0,0,0,0,0,0.471466064453125,0,0,0,0,0,0,0.502853393554688,0,0,0,0,0,0.452377319335938,0,0,0,0,0,0,0,-7.73889923095703,0,0.506980895996094,0,0,0,0,0,0,0,0,0.634368896484375,0,0,0,0,0.703315734863281,0,0,0,0,0,0,0.744468688964844,0,0,0,0,0,0.56219482421875,0,0,0,0,0.517601013183594,0,0,0,0,0,0,0,0,0,0.487762451171875,0,0,0.607513427734375,0,0,0,0,0,0,0,0.499038696289062,0,0,0.979278564453125,0,0,0,0,0,0,0,0,0,0.533340454101562,0,0,0,0,0,0,0.943023681640625,0,0,0,0,0,0,-7.43659210205078,0,0,0,0,0,0,0,0,0,0,0,0,0.504364013671875,0,0,0,0,0,0,0,0,0,0.501899719238281,0,0,0,0,0,0,0,0,0,1.00580596923828,0,0,0,0,0,0,0,0,0.876716613769531,0,0,0,0,0.625404357910156,0,0,0,0,0,0,0,0,1.0048828125,0,0,0,0,0,0,0.639503479003906,0,0,0,0,0,0.509269714355469,0,0,0,0,0,0,0,0,0,0.465766906738281,0,0,0,0,0,0,0.481277465820312,0,0,0,0,0,0,0,0,0,0,0,0.595787048339844,0,0,0,0,0,0,-7.79576873779297,0,0,0,0.61871337890625,0,0.695228576660156,0,0,0,0,0,0,0,0.67010498046875,0,0,0,0,0,0.495025634765625,0,0,0,0,0,0.500823974609375,0,0,0,0,0,0.975753784179688,0,0,0,0,0,0.491081237792969,0,0,0,0,0,0,0,0,0,0.885238647460938,0,0,0,0,0,0.500999450683594,0,0,0,0.575271606445312,0,0,0,0,0,0,0,0.988410949707031,0,0,0,0,0,0,0,0,0,0,-7.44495391845703,0,0,1.00457000732422,0,0,0,0,0,0,1.00521850585938,0,0.562782287597656,0,0,0,0,0,0,0.484771728515625,0,0,0,0,0,0.501388549804688,0,0,0,0,0,0,0,0.570243835449219,0,0,0,0,0,0,0,0,0.716445922851562,0,0,0,0.5167236328125,0,0.520668029785156,0,0,0,0,0,0,0,0,0,0.668701171875,0,0,0,0,0,0,0,0,0.486808776855469,0,0.48529052734375,0,0,0,0,0,0,0,0,0,0.471343994140625,0,0,0,0,0,0,-7.66872406005859,0,0,0,0,0,0.690696716308594,0,0,0,0,0,0,0,0,0.497108459472656,0,0,0,0,0,0,0,0,0.592658996582031,0,0,0,0,0,0,0,0.612525939941406,0,0,0,0,0,0,0,0,0,0.662284851074219,0,0,0,0,0,0.496566772460938,0,0,0,0.981109619140625,0,0,0,0,0.987174987792969,0,0,0,0,0,0,0,0.98193359375,0,0,0,0,0,0,0.49407958984375,0,0,0,0,0,0,0,0.746353149414062,0,0,0,0,0,-7.81217956542969,0,0,0,0,0,0,0,0,0,0.884452819824219,0,0,0,0,0,0,0,1.01270294189453,0,0,0,0,0,0,0,1.01284027099609,0,0,0,0.956291198730469,0,0,0,0,0,0,0,0,0.48974609375,0,0,0,0,0,0,0,0,0,0,0,0,0.689155578613281,0,0,0,0,0,0,0,0,0,0.479644775390625,0,0,0,0.479736328125,0,0,0,0,0.679168701171875,0,0,0,0,0,0,0.484176635742188,0,0,0,0,0,0,-7.46804809570312,0,0,0,0,0,0,0,0,0,0.5623779296875,0,0,0,0,0.48455810546875,0,0,0,0,0,0,0.59320068359375,0,0,0,0,0,0,0,0,0,0,0,0,0.496696472167969,0,0,0,0,0.589027404785156,0,0,0,0,0,0,0.488616943359375,0,0,0,0.549087524414062,0,0.573104858398438,0,0,0,0,0,0,0,0.534568786621094,0,0.487648010253906,0,0,0,0,0.54144287109375,0,0,0,0,0,0.568817138671875,0,0,0,0,0,0,0,0,0,0.563278198242188,0,0,0,0,0.973724365234375,0,0,0,0,0,0,-7.43770599365234,0,0,0,0,0,0,0,0,0.998710632324219,0,0,0,0,0,0.991256713867188,0,0,0,0,0,0,0.619369506835938,0,0,0,0,0.48486328125,0,0,0,0,0,0,0,0,0,0.643112182617188,0,0,0,0,0,0,0,0,0,0.480552673339844,0,0.969680786132812,0,0,0,0,0,0,0,0.963310241699219,0,0,0,0,0,0,0,0,0,0,0.823432922363281,0,0,0,0,0,0,0.614906311035156,0,0,0,0,0,-7.72257995605469,0,0,0,0,0,0,0.988662719726562,0,0,0,0,0,0,0.989242553710938,0,0,0,0,0,0.4981689453125,0,0,0,0,0,0,0,0.487815856933594,0,0,0,0,0,0,0,0,0.907882690429688,0,0,0,0,0,0.987739562988281,0,0,0,0,0,0,0,0.497169494628906,0,0,0,0,0.978317260742188,0,0,0,0,0,0,0.494544982910156,0,0.897567749023438,0,0,0,0,0,0,-7.31955718994141,0,1.00617980957031,0,0,0,0,0,0,0.9228515625,0,0,0,0,0,0,0.501373291015625,0,0,0,0,0,0,0,0,0.990211486816406,0,0,0,0,0,0,0.641998291015625,0,0,0,0,0,0,0,0,0.698860168457031,0,0.483573913574219,0,0,0,0,0,0,0.988906860351562,0,0,0,0,0.978164672851562,0,-7.40611267089844,0,0,0,0,0.50274658203125,0,0,0,0,0.502151489257812,0,0,0,0,0,0,0,0.493583679199219,0,0,0,0,0,0,0,0,0,0,1.00010681152344,0,0,0,0,0,0,0.992904663085938,0,0,0,0,0,0,0,0,0,0,0,0.999076843261719,0,0,0,0,0,0,0.489456176757812,0,0,0,0,0,0,0,0,0,0,0.785560607910156,0,0,0,0,0,0,0.690322875976562,0,0,0,0,0,0,0,0,0.987716674804688,0,0,0,0,0,-7.90462493896484,0,0,0,0,0,0,0,0.998985290527344,0,0,0,0,0,0,1.00380706787109,0,0.794029235839844,0,0,0,0,0,0,0,0.701515197753906,0,0,0,0,0,0,0,0,0,0.498992919921875,0,0,0,0,0,0,0.979049682617188,0,0,0,0,0,0.984535217285156,0,0,0,0,0,0,0.988235473632812,0,0,0,0,0,0,0,0.498420715332031,0,0,0,0,0,0,0.479560852050781,0,0,-7.40314483642578,0,0.635002136230469,0,0,0,0,0,0,0.501014709472656,0,0,0,0,0.997146606445312,0,0,0,0,0,0,0,0,0.900428771972656,0,0,0,0,0,0,0,0,0,0.718490600585938,0,0,0,0,0.737640380859375,0,0,0,0.538261413574219,0,0,0,0,0.475028991699219,0,0,0,0,0,0.627761840820312,0,0,0,0,0,0,0.672859191894531,0,0.536933898925781,0,0,0,0,0,0,-7.88417053222656,0,0,0,0,0,0,0,0,0,0,0.603904724121094,0,0,0,0,0,0,0,0,0,0,0,0,0,0.568435668945312,0,0,0,0,0,0.687217712402344,0,0,0,0,0,0.586097717285156,0,0.498695373535156,0,0,0,0,0,0.546424865722656,0,0.494949340820312,0,0,0,0,0,0,0.4884033203125,0,0,0,0,0,0.581932067871094,0,0,0,0,0.555740356445312,0,0,0,0,0,0,0.797515869140625,0,0,0,0,0.993721008300781,0,0,0,0,0,0,0.537956237792969,0,0,-7.48560333251953,0,0,0,0,0,0,0.552383422851562,0,0,0,0,0,0.987388610839844,0,0,0,0,0,0,0.569183349609375,0,0,0,0,0.745857238769531,0,0,0,0,0,0,0.635414123535156,0,0,0,0,0,0,0,0,0.971450805664062,0,0,0,0,0,0,0,0,0,0.491508483886719,0,0,0,0,0,0.973297119140625,0,0,0,0,0,0,0.610816955566406,0,0,0,0,0.744613647460938,0,0,0,0,0.275276184082031,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-8.35098266601562,0,0,0,0,0,0,0,0.493759155273438,0,0,0,0,0,0.578094482421875,0,0,0,0,0,0,0,0,0,0,0.494087219238281,0,0,0,0,0,0,0,0.546112060546875,0,0,0,0,0,0,0.4920654296875,0,0,0,0,0,0,0,0.637443542480469,0,0,0,0,0,0,0,0.49957275390625,0,0,0,0,0,0,0.497909545898438,0,0,0,0,0,0,1.00664520263672,0,0,0.502578735351562,0,0,0,0.502044677734375,0,0,0,0,0,0,0,0,0.996627807617188,0,0,0,0,0,0,0,0,0,0,0.488128662109375,0,-7.40439605712891,0,0,0,0,0,0,0,0.484832763671875,0,0.691322326660156,0,0,0,0,0,0,0,0,0.497695922851562,0,0,0,0,0,0.487518310546875,0,0,0,0,0,0.656417846679688,0,0,0,0,0,0,0,0,0,0,0.50390625,0,0,0,0,0,0,0,0,0.603485107421875,0,0,0,0,0,0,0.534843444824219,0,0,0,0,0,0,0,0,0,0.499275207519531,0,0,0,0.588180541992188,0,0,0,0,0.551025390625,0,0,0,0,0,0.49664306640625,0,0,0,0,0,0,0.531105041503906,0,0,0,0,0,0,0,0,0.712791442871094,0,0,0,0,0,-7.79468536376953,0,0,0,1.02047729492188,0,0,0,0,0,0,0,0,0,0,0,0,0,0.493560791015625,0,0,0,0,0,0,0,0,0,0,0,0.501518249511719,0,0,0,0,0,0,0,0,0,0,0.492988586425781,0,0,0,0,0,0,0.910919189453125,0,0,0,0,0,0,0.502265930175781,0,0,0,0,0,0.835990905761719,0,0,0,0,0.662078857421875,0,0,0,0,0,0,0,0,0.67889404296875,0,0,0,0,0,0,0,0.8240966796875,0,0,0,0,0,0,0,0.988967895507812,0,0,0,0,0,0,0,-7.39442443847656,0,0,0,0,0,0,0,0.502952575683594,0,0,0,0,0,0,0,0.602615356445312,0,0,0,0,0,0.566307067871094,0,0,0,0,0.507270812988281,0,0,0,0,0,0,0.632675170898438,0,0,0,0,0,0,0.4891357421875,0,0.62030029296875,0,0,0,0,0,0,0,0.499649047851562,0,0,0,0,0.50567626953125,0,0,0,0,0,0,0,0.687599182128906,0,0,0,0,0,0,0.753204345703125,0,0,0,0,0,0,0.507865905761719,0,0,0,0,0,0,0.482139587402344,0,0,0,0,0,0,0,-7.83424377441406,0,0,0,0,0,0,0,0,0.498435974121094,0,0,0,0,0,0,0,0.892265319824219,0,0,0,0,0,0,0,0,0,0.502845764160156,0,0.643211364746094,0,0.487991333007812,0,0,0,0,0,0,0.5301513671875,0,0,0.661819458007812,0,0,0.492813110351562,0,0,0,0,0,0,0,0,0,0.512626647949219,0,0,0,0,0,0.467796325683594,0,0,0,0,0,0,0.605979919433594,0,0,0,0,0,0.470970153808594,0,0,0,0,0,0.533721923828125,0,0,0,0,-7.65797424316406,0,0,0,0,0.503364562988281,0,0,0,0,0,0,0,0.537033081054688,0,0,0,0,0.838241577148438,0,0,0.981025695800781,0,0,0,1.012451171875,0,0,0,0,0,0,0,0,0,0,0,0,0.502593994140625,0,0,0,0,0,0,0,0,0,0,0,0,0,0.740013122558594,0,0.490592956542969,0,0,0,0,0,0,0,0,0.500862121582031,0,0,0,0,0.760650634765625,0,0,0,0,0,0,0,0.744186401367188,0,0,0,0,0.493148803710938,0,0,0,0,0,0,0,0,-7.73180389404297,0,0,0,0,0,0,0.496444702148438,0,0.994598388671875,0,0,0,0,0,0,0,0,0.495719909667969,0,0,0,0,0,0,0,0,0,0.778007507324219,0,0,0,0,0,0,0,0.486610412597656,0,0,0,0,0,0,0,0.697135925292969,0,0,0,0,0,0,0,0,0,0.501022338867188,0,0,0,0,0,0,0,0.491165161132812,0,0,0,0,0,0.500953674316406,0,0,0,0,0,0,0,0,0,0.502410888671875,0,0.995887756347656,0,0,0,0,0,0,0.501960754394531,0,0,0,0,0,0,0,0,0,0,-7.78895568847656,0,0,0,0,0.905555725097656,0,0,0,0,0,0,0,0,0,0.502540588378906,0,0,0,0,0.502754211425781,0,0,0,0,1.00582122802734,0,0,0,1.00597381591797,0,0.502861022949219,0,0,0,0,0,0,0,0.603584289550781,0,0,0,0,0,0,0.90509033203125,0,0,0,0,0,0.503425598144531,0,0,0,0,0,0,0.479133605957031,0,0,0,0,0.991058349609375,0,0,0,0,0,0,-7.39778137207031,0,0,0,0,0,0,0.500289916992188,0,0,0,0,0,0,0,0,0,0,0,0.500801086425781,0,0,0,0,0,0,0.9892578125,0,0.495964050292969,0,0,0,0.495262145996094,0,0,0,0,0,0,0,0,0,0.999397277832031,0,0,0,0,0,0,0,0.488327026367188,0,0,0,0,0,0,0,0,0,0,0.491813659667969,0,0,0,0,0,0,0,0.996772766113281,0,0,0,0,0,0,0,0.981483459472656,0,0,0,0,0,0,0.501388549804688,0,0,0,0,0,0,0,0,0,0,-7.39166259765625,0,0,0,0,0,0.601333618164062,0,0,0,0,0,0,0.822639465332031,0,0,0,0,0,0,0.504638671875,0,0,0,0,0,0,0,0.666099548339844,0,0,0,0,0.840484619140625,0,0,0,0,0,0,0,0.50250244140625,0,0,0,1.00304412841797,0,0,0,0,0,0,0.837173461914062,0,0,0,0,0,0,0.670341491699219,0,0,0,0,0,0,0,0.575340270996094,0,0,0,0,0,0,0,0.500984191894531,0,0,0,0,0,-7.80654144287109,0,0,0.589790344238281,0,0,0,0,0,0,0.501457214355469,0,0,0,0,0,0,0,0,0,0.493118286132812,0,0,0.512466430664062,0,0,0,0,0,0,0,0,0,0.517295837402344,0,0,0,0,0,1.00984954833984,0,0,0,0,0,0,0.532791137695312,0,0,0,0,0.573387145996094,0,0.501358032226562,0,0,0,0,0.606559753417969,0,0,0,0,0,0,0,0,0,0,0,0,0.685539245605469,0,0,0,0,0,0.495933532714844,0,0,0,0,0.927238464355469,0,0,0,0,0,0,0,-7.29869079589844,0,0,0,0,1.00546264648438,0,0.843597412109375,0,0.527458190917969,0,0,0,0,0.554443359375,0,0,0,0,0,0,0.504463195800781,0,0,0,0,0,0,0,0.687049865722656,0,0,0,0,0,0,0.562477111816406,0,0,0,0,0,0.607437133789062,0,0,0,0,0,0,0,0,0.488479614257812,0,0,0,0,0,0,0.920196533203125,0,0,0,0,-7.39505004882812,0,0,0,0,0.499603271484375,0,0,0,0.490715026855469,0,0,0,0.493507385253906,0,0,0,0,0.596122741699219,0,0,0,0,0,0,0.499862670898438,0,1.00696563720703,0,0,0,0,0,0,1.00432586669922,0,0.999542236328125,0,0,0,0,0,0.998390197753906,0,0,0,0,0,0,0.95587158203125,0,-7.59417724609375,0,0,0,0,1.00569152832031,0,0,0,0,1.00564575195312,0,0.500518798828125,0,0,0,0,0,0,0.496818542480469,0,0,0,0,0,0,0,0.502769470214844,0,0,0,0,0,0,0,0.722602844238281,0,0,0.758354187011719,0,0,0,0,0,0,0.49298095703125,0,0,0,0,0,0,0,0.714080810546875,0,0,0,0,0,0,0.783134460449219,0,0,0,0,0,0,0,0.99908447265625,0,0,0,0,0,0,-7.35885620117188,0,0,0,0,1.00572967529297,0,0,0,0,1.00453948974609,0,0,0,0,0.62408447265625,0,0,0,0,0,0,0,0],\"filename\":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null]},\"interval\":10,\"files\":[],\"prof_output\":\"/var/folders/f3/t51slq3x2dlfgk3ksp7vddzm0000gq/T//Rtmpc50nMf/filea0268a8367a.prof\",\"highlight\":{\"output\":[\"^output\\\\$\"],\"gc\":[\"^$\"],\"stacktrace\":[\"^\\\\.\\\\.stacktraceo(n|ff)\\\\.\\\\.$\"]},\"split\":\"h\"}},\"evals\":[],\"jsHooks\":[]}  {\"x\":{\"message\":{\"prof\":{\"time\":[1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,9,9,10,10,10,10,10,10,11,11,11,11,11,11,11,11,11,12,12,12,12,12,12,12,12],\"depth\":[7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1],\"label\":[\"base::sample\",\"fastSimNullDistR_work\",\"fastSimNullDistRMean\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"getOption\",\"sum\",\"fastSimNullDistR_work\",\"fastSimNullDistRMean\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"sample.int\",\"base::sample\",\"fastSimNullDistR_work\",\"fastSimNullDistRMean\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"sample.int\",\"base::sample\",\"fastSimNullDistR_work\",\"fastSimNullDistRMean\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"sample.int\",\"base::sample\",\"fastSimNullDistR_work\",\"fastSimNullDistRMean\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"\",\"sample.int\",\"base::sample\",\"fastSimNullDistR_work\",\"fastSimNullDistRMean\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"sample.int\",\"base::sample\",\"fastSimNullDistR_work\",\"fastSimNullDistRMean\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"sample.int\",\"base::sample\",\"fastSimNullDistR_work\",\"fastSimNullDistRMean\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"is.null\",\"local\",\"fastSimNullDistR_work\",\"fastSimNullDistRMean\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"\",\"sample.int\",\"base::sample\",\"fastSimNullDistR_work\",\"fastSimNullDistRMean\",\"eval\",\"eval\",\"eval.parent\",\"local\",\"sample.int\",\"base::sample\",\"fastSimNullDistR_work\",\"fastSimNullDistRMean\",\"eval\",\"eval\",\"eval.parent\",\"local\"],\"filenum\":[null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null],\"linenum\":[null,null,3,null,null,null,null,null,null,null,3,null,null,null,null,null,null,null,3,null,null,null,null,null,null,null,3,null,null,null,null,null,null,null,3,null,null,null,null,null,null,null,null,3,null,null,null,null,null,null,null,3,null,null,null,null,null,null,null,3,null,null,null,null,null,null,null,3,null,null,null,null,null,null,null,null,3,null,null,null,null,null,null,null,3,null,null,null,null],\"memalloc\":[31.6913833618164,31.6913833618164,31.6913833618164,31.6913833618164,31.6913833618164,31.6913833618164,31.6913833618164,42.9852142333984,42.9852142333984,42.9852142333984,42.9852142333984,42.9852142333984,42.9852142333984,42.9852142333984,42.9852142333984,49.0664215087891,49.0664215087891,49.0664215087891,49.0664215087891,49.0664215087891,49.0664215087891,49.0664215087891,49.0664215087891,54.9366302490234,54.9366302490234,54.9366302490234,54.9366302490234,54.9366302490234,54.9366302490234,54.9366302490234,54.9366302490234,60.7808303833008,60.7808303833008,60.7808303833008,60.7808303833008,60.7808303833008,60.7808303833008,60.7808303833008,60.7808303833008,37.6629943847656,37.6629943847656,37.6629943847656,37.6629943847656,37.6629943847656,37.6629943847656,37.6629943847656,37.6629943847656,37.6629943847656,33.8526916503906,33.8526916503906,33.8526916503906,33.8526916503906,33.8526916503906,33.8526916503906,33.8526916503906,33.8526916503906,43.6068878173828,43.6068878173828,43.6068878173828,43.6068878173828,43.6068878173828,43.6068878173828,43.6068878173828,43.6068878173828,49.40283203125,49.40283203125,55.1845626831055,55.1845626831055,55.1845626831055,55.1845626831055,55.1845626831055,55.1845626831055,51.1763534545898,51.1763534545898,51.1763534545898,51.1763534545898,51.1763534545898,51.1763534545898,51.1763534545898,51.1763534545898,51.1763534545898,35.3730010986328,35.3730010986328,35.3730010986328,35.3730010986328,35.3730010986328,35.3730010986328,35.3730010986328,35.3730010986328],\"meminc\":[0,0,0,0,0,0,0,11.293830871582,0,0,0,0,0,0,0,6.08120727539062,0,0,0,0,0,0,0,5.87020874023438,0,0,0,0,0,0,0,5.84420013427734,0,0,0,0,0,0,0,-23.1178359985352,0,0,0,0,0,0,0,0,-3.810302734375,0,0,0,0,0,0,0,9.75419616699219,0,0,0,0,0,0,0,5.79594421386719,0,5.78173065185547,0,0,0,0,0,-4.00820922851562,0,0,0,0,0,0,0,0,-15.803352355957,0,0,0,0,0,0,0],\"filename\":[null,null,\"\",null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,null,\"\",null,null,null,null,null,null,null,\"\",null,null,null,null]},\"interval\":10,\"files\":[{\"filename\":\"\",\"content\":\"set.seed(2009)\\nprofvis({\\n NullDistFSNDR_mw \"}],\"prof_output\":\"/var/folders/f3/t51slq3x2dlfgk3ksp7vddzm0000gq/T//Rtmpc50nMf/filea025a731d15.prof\",\"highlight\":{\"output\":[\"^output\\\\$\"],\"gc\":[\"^$\"],\"stacktrace\":[\"^\\\\.\\\\.stacktraceo(n|ff)\\\\.\\\\.$\"]},\"split\":\"h\"}},\"evals\":[],\"jsHooks\":[]} Das mit den beiden Routinen aus FastSimNullDistR die gleichen Ergebnisse zu erwarten sind, sie also ein “(quasi-)drop-in-replacements” der Mosaic Routinen darstellen, kann man an den folgenden QQ-Plots erkennen:\ndf.diffprop \u0026lt;- data_frame(diffprop = c(NullDistFSNDR_aw$diffprop, NullDistMosaic_aw$diffprop), type = c(rep(\u0026quot;FSNDR\u0026quot;, 10000), rep(\u0026quot;mosaic\u0026quot;, 10000))) ## Warning: `data_frame()` was deprecated in tibble 1.1.0. ## Please use `tibble()` instead. gf_qq(~diffprop, color = ~type, data = df.diffprop) df.diffmean \u0026lt;- data_frame(diffmean = c(NullDistFSNDR_mw$diffmean, NullDistMosaic_mw$diffmean), type = c(rep(\u0026quot;FSNDR\u0026quot;, 10000), rep(\u0026quot;mosaic\u0026quot;, 10000))) gf_qq(~diffmean, color = ~type, data = df.diffmean) # qqplot(NullDistFSNDR_aw\\(diffprop, NullDistMosaic_aw\\)diffprop) gf_qq(FSNDR ~ Mosaic, data=df) # qqplot(NullDistFSNDR_mw\\(diffmean, NullDistMosaic_mw\\)diffmean) gf_qq(NullDistFSNDR_mw\\(diffmean ~ NullDistMosaic_mw\\)diffmean) ```\n Woher kommt die Geschwindigkeit? Schaut man sich den Quellcode von Mosaic an, wird einem schnell klar, dass es zwar didaktisch sinnvoll ist die unabhängige Variable mit shuffle() zu bearbeiten, nicht aber programmiertechnisch. Und wenn, dann nicht in dem man die ganze Datenzeile für die Berechnung kopiert. Statt also \\(10\\,000\\) mal die ganzen Daten im Speicher zu kopieren wäre es doch sinnvoller mit Hilfe eines Index auf die unveränderten Daten zuzugreifen. Und genau das machen die zwei Routinen. Es wird also nur dieser Zugriffsindex wird geshuffelt und das spart Speicherplatz und deutlich auch Rechenzeit.\n ","date":1525219200,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1525219200,"objectID":"c6869bacc3c45d64cb37b202808cd0ad","permalink":"https://sefiroth.net/nab/post/ein-wenig-schneller-zur-simulierten-nullverteilung/","publishdate":"2018-05-02T00:00:00Z","relpermalink":"/nab/post/ein-wenig-schneller-zur-simulierten-nullverteilung/","section":"post","summary":"Ein Nullhypothesentest ist schnell geschrieben. Will man den approximativen Weg gehen, so hilft R einem mit entsprechenden Tests mit einfachen Befehlen. Nimmt man MOSAIC dazu, so bekommt man u.","tags":["R","Lehre","Statistik"],"title":"Ein wenig schneller zur simulierten Nullverteilung","type":"post"},{"authors":null,"categories":["Statistisches"],"content":"  Der tipping Datensatz wird oft analysiert. Das Verhältnis von Trinkgeld (tip) und Rechnungsbetrag (total_bill) steht dabei im Vordergrund einer lineare Regressionsanalyse. So auch hier. Wir wollen die einzelnen Angaben von R dabei in den Fokus rücken und einmal Hinterfragen, was wir bei der Ausgabe von R eigentlich genau sehen, woher es kommt und wie man es interpretieren kann.\nZunächst laden wir dazu die tipping Daten mittels\nlibrary(mosaic) download.file(\u0026quot;https://goo.gl/whKjnl\u0026quot;, destfile = \u0026quot;tips.csv\u0026quot;) tips \u0026lt;- read.csv2(\u0026quot;tips.csv\u0026quot;) in den Arbeitsspeicher.\nEine lineares Modell wird schnell mit\nlinMod \u0026lt;- lm(tip ~ total_bill, data = tips) erstellt. Betrachten wir die Zusammenfassung:\nsummary(linMod) ## ## Call: ## lm(formula = tip ~ total_bill, data = tips) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1982 -0.5652 -0.0974 0.4863 3.7434 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.920270 0.159735 5.761 2.53e-08 *** ## total_bill 0.105025 0.007365 14.260 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.022 on 242 degrees of freedom ## Multiple R-squared: 0.4566, Adjusted R-squared: 0.4544 ## F-statistic: 203.4 on 1 and 242 DF, p-value: \u0026lt; 2.2e-16 Die zentrale Frage bei einer linearen Regression ist, finden wir einen linearen Zusammenhang in unserer Stichprobe, den wir auf die Population (als die Grundgesamtheit) übertragen können.\nDie Spalte Estimate im Abschnitt Coefficients liefert uns in unser Stichprobe einen möglichen linearen Zusammenhang gemäß\n\\[\\hat{y}_{\\text{tip}} = \\hat{\\beta}_{\\text{0}} + \\hat{\\beta}_{\\text{total_bill}} \\cdot x_{\\text{total_bill}},\\]\nmit den Regressionskoeffizienten \\(\\hat{\\beta}_0=0.9202696\\) und \\(\\hat{\\beta}_{\\text{total_bill}}=0.1050245\\).\nGraphisch ergibt sich damit das Modell wie folgt:\n# Statt plotModel(linMod) besser: mypanel \u0026lt;- function(x, y) { # Scatterplot: panel.xyplot(x, y, col = \u0026quot;darkgreen\u0026quot;) # Regressionsgerade: panel.abline(linMod, col = \u0026quot;red\u0026quot;, lwd = 1.2, lty = 2) } xyplot( tip ~ total_bill, data = tips, panel = mypanel, main = \u0026quot;Streudiagramm der Trinkgelder\u0026quot;, ylab = \u0026quot;Trinkgeld\u0026quot;, xlab = \u0026quot;Rechnungsbetrag\u0026quot;, key = list( space = \u0026quot;bottom\u0026quot;, padding.text = 8, lines = list(col = c(\u0026quot;red\u0026quot;), lty = c(2), lwd = 1.2), text = list(c(\u0026quot;Regressionsgerade\u0026quot;)) ) ) Was hat es mit dem y-Achsenabschnitt \\(\\hat{\\beta}_0\\) auf sich?\nIst es etwa eine Art Grundtrinkgeld, mit dem der Kellern rechnen kann, auch wenn der Kunde gar nichts bestellt?\nNun ja, es so etwas in der Art, aber eben ein rein fiktiver Wert, der durch die Konstruktion der Parameter entsteht. Eine (affin-)lineare Gerade geht nun einmal irgendwann durch die y-Achse (wenn sie nicht parallel dazu ist) und es kann passieren, dass eine sinnvolle Interpretation nicht so ohne weiteres möglich ist.\nWir können aber dieses Grundtrinkgeld heraus nehmen und den y-Achsenabschnitt auf Null setzen. Dazu ziehen wir \\(\\hat{\\beta}_0\\) einfach von alle Trinkgeldern ab. Wir erhalten quasi nur noch den Trinkgeldzuwach.\nbeta_0 \u0026lt;- coef(linMod)[\u0026quot;(Intercept)\u0026quot;] # Grundtrinkgeld tips$delta_tip \u0026lt;- tips$tip - beta_0 # wird abgezogen Vergleichen wir das alte lineare Modell mit dem neuen Modell (linModDelta):\nlinModDelta \u0026lt;- lm(delta_tip ~ total_bill, data = tips) summary(linModDelta) ## ## Call: ## lm(formula = delta_tip ~ total_bill, data = tips) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1982 -0.5652 -0.0974 0.4863 3.7434 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -4.549e-15 1.597e-01 0.00 1 ## total_bill 1.050e-01 7.365e-03 14.26 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.022 on 242 degrees of freedom ## Multiple R-squared: 0.4566, Adjusted R-squared: 0.4544 ## F-statistic: 203.4 on 1 and 242 DF, p-value: \u0026lt; 2.2e-16 In diesem Modell ist der Wert für den y-Achsenabschnitt numerisch gleich 0. – Ja, da mag zwar \\(-4.5487837\\times 10^{-15}\\) stehen, jedoch sind so kleine Werte der jedem Rechner inne wohnenden Ungenauigkeit in der Gleitkomma-Arithmetik geschuldet und ist faktisch gleich 0.\nDer Wert für die Steigung lautet weiterhin \\(0.1050245\\). Das war auch zu erwarten, denn wir haben unsere Regressionsgerade eigentlich nur um \\(\\hat{\\beta}_0\\) nach unten verschoben. (Der Fachmann spricht von einer Translation (Parallelverschiebung)1 um \\(-\\hat{\\beta}_0\\).\n# Statt plotModel(linModDelta) besser: mypanel \u0026lt;- function(x, y) { # Scatterplot: panel.xyplot(x, y, col = \u0026quot;darkgreen\u0026quot;) # Regressionsgerade: panel.abline(linModDelta, col = \u0026quot;red\u0026quot;, lwd = 1.2, lty = 2) } xyplot( delta_tip ~ total_bill, data=tips, panel = mypanel, main = \u0026quot;Streudiagramm der Delta Trinkgelder\u0026quot;, ylab = \u0026quot;Delta Trinkgeld\u0026quot;, xlab = \u0026quot;Rechnungsbetrag\u0026quot;, key = list( space=\u0026quot;bottom\u0026quot;, padding.text=8, lines=list(col=c(\u0026quot;red\u0026quot;), lty=c(2), lwd=1.2), text=list(c(\u0026quot;Regressionsgerade\u0026quot;))) ) Vergleichen wir die beiden Zusammenfassungen, so stellen wir fest das sich mit Ausnahme der [Intercept] Zeile praktisch nichts geändert hat. Das ist kein Wunder, sondern Absicht!\nDie Regressionsgerade stellt für unsere Stichprobe die Gerade mit dem geringsten Fehler an den Datenpunkten dar. Mathematisch heißt das folgendes:\nAn den \\(n=244\\) Datenpunkten unserer Stichprobe \\((x_i, y_i)=(tips\\$total\\_bill[i], tips\\$tip[i])\\) [für \\((i=1, \\dots, n)\\)] sind die Residuen, also die Fehlerterme,\n\\[ \\hat{e}_i =\\hat{y}_i - y_i = \\left[\\hat{\\beta}_{\\text{0}} + \\hat{\\beta}_{\\text{total_bill}} \\cdot x_i\\right] - y_i \\]\ndurch die verwendete Methode der kleinsten Quadrate2 quadratisch minimal. Kurz:\n\\[ \\sum_{i=1}^n (\\hat{e}_i)^2 \\text{ ist minimal!} \\]\nWir können diese Fehlerterme graphisch ansehen um die Varianz der Residuen zu sehen. Dazu ziehen wir von allen Datenpunkten \\(y_i\\) den geschätzten Wert \\(\\hat{y}_i\\) ab und erstellen ein neues lineares Modell:\nbeta_total_bill \u0026lt;- coef(linModDelta)[\u0026quot;total_bill\u0026quot;] tips$error_tip \u0026lt;- (tips$tip - beta_0 - beta_total_bill * tips$total_bill) linModError \u0026lt;- lm(error_tip ~ total_bill, data = tips) summary(linModError) ## ## Call: ## lm(formula = error_tip ~ total_bill, data = tips) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1982 -0.5652 -0.0974 0.4863 3.7434 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.900e-15 1.597e-01 0 1 ## total_bill -8.740e-17 7.365e-03 0 1 ## ## Residual standard error: 1.022 on 242 degrees of freedom ## Multiple R-squared: 6.665e-31, Adjusted R-squared: -0.004132 ## F-statistic: 1.613e-28 on 1 and 242 DF, p-value: 1 Also Diagramm sieht es dann so aus:\n# Statt plotModel(linModError) besser: mypanel \u0026lt;- function(x, y) { # Scatterplot: panel.xyplot(x, y, col = \u0026quot;darkgreen\u0026quot;) # Regressionsgerade: panel.abline(linModError, col = \u0026quot;red\u0026quot;, lwd = 1.2, lty = 2) } xyplot( error_tip ~ total_bill, data = tips, panel = mypanel, main = \u0026quot;Streudiagramm der Residuen\u0026quot;, ylab = \u0026quot;Residuen\u0026quot;, xlab = \u0026quot;Rechnungsbetrag\u0026quot;, key = list( space = \u0026quot;bottom\u0026quot;, rows = 3, padding.text = 8, lines = list(col=c(\u0026quot;red\u0026quot;), lty = c(2), lwd = 1.2), text = list(c(\u0026quot;Regressionsgerade / x-Achse\u0026quot;)) ) ) Wir können die Graphik im wesentlichen auch einfacher über den Befehl\nxyplot(residuals(linMod) ~ fitted(linMod)) erhalten.\nBetrachten wir kurz nur die Residuen:\nfavstats(~residuals(linMod)) ## min Q1 median Q3 max mean sd n ## -3.198225 -0.5651615 -0.09744499 0.4863111 3.743435 -2.022281e-17 1.019943 244 ## missing ## 0 Wir sehe, dass wir in der Zusammenfassung immer genau diese Werte unter dem Abschnitt Residuals gefunden haben. Minimum, das 1. Quantil, der Median, das 3. Quantil und das Maximum stimmen überein.\nDer erwartungstreue und unverzerrte Schätzer für den Standardfehler der Residuen, lautet\n\\[ \\begin{align*} SE_{\\text{Residuen}} \u0026amp;= \\sqrt{\\frac{1}{n-2} \\cdot \\sum_{i=1}^n (\\hat{e_i})^2} = \\sqrt{\\frac{n-1}{n-2} \\cdot \\frac{1}{n-1} \\cdot \\sum_{i=1}^n (\\hat{e_i})^2} \\\\ \u0026amp;= \\sqrt{\\frac{n-1}{n-2}} \\cdot \\sqrt{\\frac{1}{n-1} \\cdot \\sum_{i=1}^n (\\hat{e_i})^2} \\\\ \u0026amp;= \\sqrt{\\frac{n-1}{n-2}} \\cdot s_{\\text{Residuen}} \\end{align*} \\]\nAlso finden wir den Wert Residual standard error aus der Zeile\n## Residual standard error: 1.022 on 242 degrees of freedom in dem wir den in den favstats gefundenen Wert für die Standardabweichung entsprechen korrigieren:\n\\[ SE_{\\text{Residuen}} = \\sqrt{\\frac{n-1}{n-2}} \\cdot s_{\\text{Residuen}} = \\sqrt{\\frac{243}{242}} \\cdot 1.0199426 = 1.0220477 \\]\nDer Median der Residuen ist nicht gleich Null, wie der Mittelwert. (Welcher auch hier als numerisch Null interpretiert werden muss!) Es könnte also eine linkssteile, rechtsschiefe Verteilung der Residuen vorliegen. Betrachten wir dazu das Histogramm:\nhistogram(~residuals(linMod), nint = 19) Schon beim ersten Blick auf das Histogramm kann an eine Normalverteilung der Residuen nicht mehr so ganz geglaubt werden.\nEin Shapiro-Wilk-Test3 hat als Nullhypothese die Annahme, dass die Daten normalverteilt sind!\nshapiro.test(residuals(linMod)) ## ## Shapiro-Wilk normality test ## ## data: residuals(linMod) ## W = 0.96728, p-value = 2.171e-05 Davon ist nach dem Ergebnis eben sowenig auszugehen, wie nach einem Blick auf das QQ-Normal-Diagramm:\nqqnorm(residuals(linMod), col = \u0026quot;darkgreen\u0026quot;) Ein K.O.-Kriterium für gute Prognosen.\nWie gut aber beschreibt unsere Regressionsgerade die Daten?\nAls Maß dafür können wir das Bestimmtheitsmaß nehmen.\nEin kurzer Blick auf die Situation, der Mittelwert der Trinkgelder ist\n\\[ \\bar{y} = \\frac{1}{n} \\cdot \\sum_{i=1}^n y_i = 2.9982787. \\]\nWir erhalten so folgendes Diagramm:\nmypanel \u0026lt;- function(x, y) { panel.xyplot(x, y) panel.abline(h = mean(y), lwd = 1.2, lty = 2, col = \u0026quot;darkgreen\u0026quot;) panel.lmline(x, y, col = \u0026quot;red\u0026quot;, lwd = 1.2, lty = 2) } xyplot( tip ~ total_bill, data = tips, panel = mypanel, main = \u0026quot;Streudiagramm der Trinkgelder\u0026quot;, ylab = \u0026quot;Trinkgeld\u0026quot;, xlab = \u0026quot;Rechnungsbetrag\u0026quot;, key = list( space = \u0026quot;bottom\u0026quot;, padding.text = 8, columns = 2, just = c(\u0026quot;center\u0026quot;, \u0026quot;bottom\u0026quot;), lines = list(col = c(\u0026quot;darkgreen\u0026quot;, \u0026quot;red\u0026quot;), lty = c(2, 2), lwd = 1.2), text = list(c(expression(bar(y)), expression(hat(beta)[0]+hat(beta)[total_bill] * x[total_bill]))), text = list(c(\u0026quot;Mittelwert Trinkgeld\u0026quot;, \u0026quot;Regressionsgerade\u0026quot;)) ) ) Die Varianz \\(s^2_{y_i}=1.9144546\\) beschreibt die mittlere quadratische Abweichung der Datenpunkte \\(y_i\\) vom Mittelwert \\(\\bar{y}\\). Diese Varianz lässt sich Zerlegen in einen Anteil, der durch die Regressionsgerade erklärt wird und in einen Anteil, der durch die Regressionsgerade nicht erklärt wird.\n\\[ s^2_{y_i} = s^2_{\\hat{y}_i} + s^2_{\\hat{e}_i} \\] Dividiert man beider Seiten durch die Varianz \\(s^2_{y_i}\\), so normiert man den Ausdruck und kann den Faktor \\(\\frac{1}{n-1}\\) (bzw. \\(\\frac{1}{n}\\)) herauskürzen. Es bleibt dann:\n\\[ 1 = \\frac{\\sum_{i=1}^n (\\bar{y}- \\hat{y_i})^2}{\\sum_{i=1}^n (\\bar{y}-y_i)^2} + \\frac{\\sum_{i=1}^n (\\hat{e_i})^2}{\\sum_{i=1}^n (\\bar{y}-y_i)^2} \\]\nMultipliziert man beide Seiten mit \\(\\sum_{i=1}^n (y_i)^2\\), so erhält man:\n\\[ \\sum_{i=1}^n (\\bar{y}- y_i)^2 = \\sum_{i=1}^n (\\bar{y}- \\hat{y_i})^2+ \\sum_{i=1}^n (\\hat{e_i})^2 \\]\nZur Vereinfachung nennt man die einzelnen Summen in dem Ausdruck wie folgt:\n Der erste Ausdruck heißt Gesamtvarianz oder total sum of squares oder kurz \\(SS_T\\), (oder TSS) er ist die Summe der quadrierten Differenzen  \\[ SS_T = \\sum_{i=1}^n (\\bar{y}-y_i)^2 \\]\n Der zweite Ausdruck heißt Modellvarianz oder model sum of squares oder kurz \\(SS_M\\) (oder RSS), er ist die Summe der quadrierten Differenzen aus dem Mittelwert \\(\\bar{y}\\) und der Punkte auf der Regressionsgeraden \\(\\hat{y}_i\\):  \\[ SS_M = \\sum_{i=1}^n (\\bar{y}-\\hat{y}_i)^2 \\]\n Der dritte Ausdruck heißt Gesamt-Verhersage-Fehler, Fehlersteuung der Regression oder error sum of squares oder kurz \\(SS_E\\) (oder ESS), er ist die Summe der quadratischen Differenz aus den Datenpunkten \\(y_i\\) und den Punkten der Regressionsgeraden \\(\\hat{y}_i\\):  \\[ SS_E = \\sum_{i=1}^n (\\hat{y}_i-y_i)^2 = \\sum_{i=1}^n (\\hat{e}_i)^2 \\]\nWir können daher auch kurz\n\\[ SS_T = SS_M + SS_E \\] schreiben und sparen uns die ganzen Summenzeichen.\nDie Güte einer Regression wollen wir durch den Anteil der durch das Model erklärten Varianz (also der \\(SS_M\\)) ausdrücken und stellen daher nach \\(SS_M\\) um:\n\\[ SS_M = SS_T - SS_E \\] Teilen wir beide Seiten durch \\(SS_T\\) also der maximalen (weil totalen) Quadratsumme, so erhalten wir: \\[ \\frac{SS_M}{SS_T} = \\frac{SS_T}{SS_T} - \\frac{SS_E}{SS_T} = 1 - \\frac{SS_E}{SS_T} \\]\nDen Ausdruck \\(\\frac{SS_M}{SS_T}\\) nennen wir Bestimmtheitsmaß und schreiben dafür \\(R^2\\). Es ist ein Wert zwischen 0 und 1, der den Anteil der durch das Modell beschriebenen Varianz in Bezug auf die Gesamtvarianz angibt. Kraft Definition ist \\(R^2\\) im eindimensionalen Fall tatsächlich das Quadrat des (Pearson-)Korrelationskoeffizienten \\(r\\). (M.a.W.: \\(R^2= r^2\\).)\nIn unserer Zusammenfassung des linearen Models findet sich dieser Wert auch. Und zwar unter dem Begriff:\n## Multiple R-squared: 0.4566,  Es gilt ja:\n\\[ R^2 = 1 - \\frac{SS_E}{SS_T} = 1 - \\frac{s^2_{\\hat{e}_i}}{s^2_{y_i}} = 1 - \\frac{1.0402829}{1.9144546} = 0.4566166 \\]\nDer Wert\n## ..., Adjusted R-squared: 0.4544 erklärt sich daraus4, dass das Bestimmheitsmaß um so größer wird je größer die Zahl der unabhängigen Variablen wird. Und zwar unabhöngig davon, ob weitere unabhängige Variablen wirklich einen Beitrag zur Erklärungskraft liefern. Daher nutzt man besser das korrigierte Bestimmtheitsmaß (engl.: adjusted R-squared) \\(\\bar{R}^2\\):\n\\[ \\begin{align*} \\bar{R}^2 \u0026amp;= 1- (1-R^2) \\cdot \\frac{n-1}{n-p-1}\\\\ \u0026amp;= R^2 - (1-R^2) \\cdot \\frac{p}{n-p-1} \\end{align*} \\]\nWobei \\(p\\) die Anzahl der unabhängigen Variablen im Modell darstellt. In unserem Beispiel gilt daher:\n\\[ \\begin{align*} \\bar{R}^2 \u0026amp;= 1 - (1-R^2) \\cdot \\frac{n-1}{n-p-1} \\\\ \u0026amp;= 1 - (1- 0.4566166) \\cdot \\frac{244-1}{244- 1- 1} \\\\ \u0026amp;= 0.4543712 \\end{align*} \\]\nVorsicht: Das korrigierte Bestimmtheitsmaß ist nicht mehr an das Intervall \\([0; 1]\\) gebunden! Es kann negative Werte annehmen, ist in der Regel kleiner als das (unkorrigierte) Bestimmtheitsmaß und erreicht die obere Grenze (\\(\\bar{R}^2=1\\)) genau dann, wenn \\(R^2 = 1\\) ist.\nBei der Gesamtsignifikanz des Modells (auch Overall-F-Test genannt) wird geprüft, ob mindestens eine Variable einen Erklärungsgehalt für das Modell liefert.\nFalls diese Hypothese verworfen wird ist somit das Modell nutzlos. Dieser Test lässt sich so interpretieren als würde man die gesamte Güte des Modells, also das \\(R^2\\) des Modells, testen. Aus diesem Grund wird der F-Test der Gesamtsignifikanz des Modells auch als Anpassungsgüte-Test bezeichnet. Die Nullhypothese des F-Test der Gesamtsignifikanz des Modells sagt aus, dass alle erklärenden Variablen keinen Einfluss auf die abhängige Variable haben. Sowohl die abhängige Variable als auch die unabhängigen Variablen können binär (kategoriell) oder metrisch sein. Der Wald-Test kann dann die Hypothesen testen (ohne Einbezug des Achsenabschnittes):\n\\[ H_{0}\\colon \\beta _{1}=\\beta _{2}=\\ldots =\\beta _{k}\\;=\\;0\\Rightarrow R^{2}=0 \\] gegen\n\\[ H_{1}:\\beta _{j}\\;\\neq \\;0\\;\\mathrm {f{\\ddot {u}}r\\;mindestens\\;ein} \\;j\\in \\{1,\\ldots ,k\\}\\Rightarrow R^{2}\\neq 0 \\]\nDie Teststatistik dieses Tests lautet\n\\[ \\begin{aligned} F\\;\\;{\\stackrel {H_{0}}{=}}{\\frac {R^{2}}{1-R^{2}}} \\cdot {\\frac {n-p-1}{p}}\\;\\;{\\stackrel {H_{0}}{\\sim }}\\;\\;F(p,n-p) \\end{aligned}. \\]\nmit \\(p\\) und \\(\\displaystyle (n-p-1)\\) Freiheitsgraden. Überschreitet der empirische F-Wert einen kritischen F-Wert, der zu einem a priori festgelegten Signifikanzniveau \\(\\alpha\\), so verwirft man die Nullhypothese \\(H_{0}\\). Das \\(R^{2}\\) ist dann ausreichend groß und mindestens ein Regressor trägt also vermutlich genügend viel Information zur Erklärung von \\(y\\) bei. Es ist naheliegend bei hohen F-Werten die Nullhypothese zu verwerfen, da ein hohes Bestimmtheitsmaß zu einem hohen F-Wert führt. Wenn der Wald-Test für eine oder mehrere unabhängige Variablen die Nullhypothese ablehnt, dann kann man davon ausgehen, dass die zugehörigen Parameter ungleich Null sind, so dass die Variable(n) in das Modell mit einbezogen werden sollten.\nIn unserem Beispiel ist\n\\[ F={\\frac {R^{2}}{1-R^{2}}} \\cdot {\\frac {n-p-1}{p}} = \\frac{0.4566166}{1-0.4566166} \\cdot \\frac{244-1-1}{1} = 203.3577233 \\]\nder Wert in der Zeile\n## F-statistic: 203.4 on 1 and 242 DF, p-value: \u0026lt; 2.2e-16 mit Parametern \\(p=1\\) und \\(n-p-1=242\\) Freiheitsgraden.\nDer p-Wert von (numerisch) 0, liefert also ein hinreichendes Indiz dafür, dass der Rechnungsbetrag einen echten Beitrag liefert.\n vgl.: https://de.wikipedia.org/wiki/Parallelverschiebung↩︎\n vgl.: https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate↩︎\n vgl.: https://de.wikipedia.org/wiki/Shapiro-Wilk-Test↩︎\n vgl.: https://de.wikipedia.org/wiki/Bestimmtheitsmaß#Das_korrigierte_Bestimmtheitsmaß↩︎\n   ","date":1515369600,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1515369600,"objectID":"48e135650b75599cf307d91b5d94b8e0","permalink":"https://sefiroth.net/nab/post/nur-ein-wenig-lineare-regression/","publishdate":"2018-01-08T00:00:00Z","relpermalink":"/nab/post/nur-ein-wenig-lineare-regression/","section":"post","summary":"Der tipping Datensatz wird oft analysiert. Das Verhältnis von Trinkgeld (tip) und Rechnungsbetrag (total_bill) steht dabei im Vordergrund einer lineare Regressionsanalyse. So auch hier. Wir wollen die einzelnen Angaben von R dabei in den Fokus rücken und einmal Hinterfragen, was wir bei der Ausgabe von R eigentlich genau sehen, woher es kommt und wie man es interpretieren kann.","tags":["Lehre","R","Statistik","Lineare Regression","Regressionsanalyse"],"title":"Nur ein wenig lineare Regression","type":"post"},{"authors":null,"categories":["Statistisches"],"content":"  *WORK IN PROGRESS Dieser Eintrag ist noch nicht fertig und wird in der Zukunft erweitert!\nKonfidenzintervalle Definition von Konfidenzintervallen1 Für unabhängig identisch verteilte Zufallsvariablen \\(X_1,\\dotsc, X_n\\) mit unbekanntem reellen Verteilungsparameter \\(\\vartheta\\) kann unter bestimmten Umständen zwei Stichprobenfunktionen \\(U\\) und \\(V\\) angeben, so dass\n\\[P(U \u0026lt; \\vartheta \u0026lt; V) \\geq \\gamma\\]\ngilt, mit \\(\\gamma \\in (0,1)\\). Dann heißt das (stochastische) Intervall \\([U, V]\\) ein Konfidenzintervall für \\(\\vartheta\\) zum Konfidenzniveau \\(\\gamma\\) (auch: ein \\(\\gamma\\)-Konfidenzintervall).\nDie Realisationen \\(u\\) und \\(v\\) von \\(U\\) bzw. \\(V\\) bilden das Schätzintervall \\([u, v]\\).\nDa die Realisationen \\(u\\) und \\(v\\) der Grenzen \\(U\\) und \\(V\\) keine Zufallsvariablen sind und \\(\\vartheta\\) ein fixer Wert ist, kann man nicht sagen, dass das Schätzintervall \\([u, v]\\) mit einer Wahrscheinlichkeit von \\(\\gamma\\) den unbekannten Parameter \\(\\vartheta\\) enthält. Es bedeutet vielmehr, dass im Mittel ein Anteil von \\(\\gamma\\) aller so berechneten Schätzintervalle den unbekannten Parameter überdecken. Dem nicht widersprechend, kann –- wie bereits von Ronald Fisher festgestellt – in manchen Modellen die Qualität des Schätzintervalls von den Daten abhängen und sogar zu Antworten führen, die mit Blick auf die Daten unsinnig sind. Probleme mit solcher Post-Data-Inkohärenz führen zur Theorie der bedingten Inferenz. Ein weiteres Problem sind die Stichprobenfunktionen U und V an sich. Um diese zu finden werden oft Vereinfachungen getroffen, dadurch können systematische Fehler entstehen, oft es gibt mehrere Konfidenzintervalle (bei der Binomialverteilung z.B. nach Clopper-Pearson, Agresti-Coull oder Wald), welche oft unterschiedliche Werte liefern.\n Ein Beispiel Wir nehmen zunächst als Population \\(N=1000\\) normalverteilte Zufallszahlen mit dem Mittelwert \\(\\mu= 0\\) und der Standardabweichung \\(\\sigma=2.0088\\).\nDazu das Histogramm der Population:\nhistogram(pop, xlab=\u0026quot;Population\u0026quot;) Aus dieser Population ziehen wir eine Stichprobe \\(x\\) vom Umfang $n=$40 und erhalten die folgenden statistischen Daten:\nfavstats(x) ## min Q1 median Q3 max mean sd n missing ## -3.38 -0.9781 0.2042 1.546 4.002 0.1877 1.901 40 0 Wir wollen nun den wahren Mittelwert \\(\\vartheta=\\mu\\) mit Hilfe der Stichprobe \\(x\\) schätzen. So ist es ja in der Realität auch, denn normalerweise haben wir die Daten über die Population nicht.\nDie Schätzfunktion für den Mittelwert lautet nun \\[\\bar{X} = \\frac1n \\sum_{i=1}^n X_i\\], und damit die konkrete Punktschätzung \\[\\hat{\\mu}=\\bar{x}= \\sum_{i=1}^n x_i\\] liefert den Wert \\(\\hat{\\mu}=\\) 0.1877.\nIn unserem Beispiel unterscheiden sich die beiden Werte um \\(\\mu - \\hat{\\mu}=\\) -0.1877.\nEin 95%-Konfidenzintervall nimmt nun den geschätzen Wert \\(\\hat{\\mu}\\) als Grundlage und gibt liefert ein Intervall mit der Eigentschaft, ausgehend von den konkreten Stichproben in 95% der Fälle den tatsächlichen Wert \\(\\mu\\) zu umfassen. Es ist also \\[\\gamma = 0.95 = 1 - \\alpha = 1 - 0.05, \\quad \\alpha = 0.05\\]\nDazu werden die beiden Stichprobenfunktionen\n\\[U=U(X_1, \\dots, X_n)=\\bar{X}-z_{\\left(1-\\frac{\\alpha}{2}\\right)}\\cdot\\frac{\\sigma}{\\sqrt{n}}\\]\nund\n\\[V=V(X_1, \\dots, X_n)=\\bar{X}-z_{\\left(1-\\frac{\\alpha}{2}\\right)}\\cdot\\frac{\\sigma}{\\sqrt{n}}\\]\nmit der bekannten Standardabweichung \\(\\sigma\\) der Population und der Stichprobengröße \\(n\\) nun mit der konkreten Realisation \\(x_1, \\dots, x_n\\) der Stichprobe gefüttert und wir erhalten damit\n\\[u = \\bar{x}-z_{\\left(1-\\frac{\\alpha}{2}\\right)}\\cdot\\frac{\\sigma}{\\sqrt{n}} = 0.1877-z_{\\left(0.975\\right)}\\cdot\\frac{2.0088}{\\sqrt{40}}=-0.4348\\] und\n\\[v = \\bar{x}+z_{\\left(1-\\frac{\\alpha}{2}\\right)}\\cdot\\frac{\\sigma}{\\sqrt{n}} = 0.1877+z_{\\left(0.975\\right)}\\cdot\\frac{2.0088}{\\sqrt{40}}=0.8102.\\]\nDie Realisation unseres 95%-Konfidenzintervall lautet nun also:\n\\[[-0.4348; 0.8102]\\]\nWas hat es nun mit den ominösen 95% auf sich?\nDas Konfidenzintervall ist ein stochastisches Intervall, d.h. die hier angegebenen Werte für \\(u\\) und \\(v\\) sind abhängig von der Realisation \\(x_1, \\dots, x_n\\), also der konkreten Stichprobe.\nNehmen wir nun also einmal eine neue Stichprobe und berechnen erneut die Realisation unseres 95%-Konfidenzintervalls, so erhalten wir:\n\\[[-0.7033; 0.5418]\\]\n## Interval coverage: ## cover ## n Low Yes High ## 40 0.00 0.98 0.02 ## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please ## use `guide = \u0026quot;none\u0026quot;` instead.   Prognoseintervalle  Fuduzialintervalle Quellen:\n Logik in der Statistik; Andrea Wiencierz, 7.10.2007 Link: https://static.aminer.org/pdf/PDF/000/230/772/induktive_inferenz_und_mehrwertige_logik.pdf    vgl: https://de.wikipedia.org/wiki/Konfidenzintervall↩︎\n   ","date":1515024000,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1515024000,"objectID":"dc11f88cf1f1c3c8afcf39671770af10","permalink":"https://sefiroth.net/nab/post/prognose-konfidenz-und-fiduzialintervalle/","publishdate":"2018-01-04T00:00:00Z","relpermalink":"/nab/post/prognose-konfidenz-und-fiduzialintervalle/","section":"post","summary":"*WORK IN PROGRESS Dieser Eintrag ist noch nicht fertig und wird in der Zukunft erweitert!\nKonfidenzintervalle Definition von Konfidenzintervallen1 Für unabhängig identisch verteilte Zufallsvariablen \\(X_1,\\dotsc, X_n\\) mit unbekanntem reellen Verteilungsparameter \\(\\vartheta\\) kann unter bestimmten Umständen zwei Stichprobenfunktionen \\(U\\) und \\(V\\) angeben, so dass","tags":["Statistik","Lehre","Konfidenzintervalle"],"title":"Prognose-, Konfidenz- und Fiduzialintervalle","type":"post"},{"authors":null,"categories":["Statistisches"],"content":"  Stub!\n","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1514764800,"objectID":"d2da72d226f2874cb281cbe11f074661","permalink":"https://sefiroth.net/nab/post/konfidenzintervalle/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/nab/post/konfidenzintervalle/","section":"post","summary":"Stub!","tags":["Lehre","Statistik","Wahrscheinlichkeitstheorie"],"title":"Konfidenzintervalle","type":"post"},{"authors":null,"categories":["Statistisches"],"content":"  Das zentrale Schwankungsintervall sagt etwas über die Präzision der Lageschätzung eines Parameters (zum Beispiel eines Mittelwertes) aus. Das Schwankungsintervall schließt einen Bereich um den wahren Wert des Parameters in der Grundgesamtheit ein, der – vereinfacht gesprochen – mit einer zuvor festgelegten Sicherheitswahrscheinlichkeit den aus der Stichprobe geschätzten Parameter enthält.1\n vgl: https://de.wikipedia.org/wiki/Zentrales_Schwankungsintervall↩︎\n   ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1514764800,"objectID":"0cef821e93903fae276ebef421a3f5d4","permalink":"https://sefiroth.net/nab/post/zentrales-schwankungsintervall/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/nab/post/zentrales-schwankungsintervall/","section":"post","summary":"Das zentrale Schwankungsintervall sagt etwas über die Präzision der Lageschätzung eines Parameters (zum Beispiel eines Mittelwertes) aus. Das Schwankungsintervall schließt einen Bereich um den wahren Wert des Parameters in der Grundgesamtheit ein, der – vereinfacht gesprochen – mit einer zuvor festgelegten Sicherheitswahrscheinlichkeit den aus der Stichprobe geschätzten Parameter enthält.","tags":["Lehre","Statistik","Wahrscheinlichkeitstheorie"],"title":"Zentrales Schwankungsintervall","type":"post"},{"authors":null,"categories":["Allgemeines"],"content":"Im Laufe der Zeit sammeln sich bei mir mehr und mehr Links zu anderen Seiten an, die ich irgendwie speichern will aber nicht ernsthaft sortieren möchte. So ist diese Sammlung hier entstanden:\n  Blog von Prof. Dr. Timm Grams \u0026ndash; \u0026ldquo;Ein Weblogbuch über sonderbare Nachrichten und alltäglichen Statistikplunder\u0026rdquo;\n  Denkfallen und Paradoxa \u0026ndash; Prof. Dr. Timm Grams gibt einen Überblick\n  Signifikanztest mit der Vierfeldertafel \u0026ndash; Prof. Dr. Timm Grams\n  Querbeet \u0026ndash; Eine Problemsammlung \u0026ndash; Prof. Dr. Timm Grams\n  Blog von Prof. Dr. Sebastian Sauer \u0026ndash; Quelle der Erleuchtung und Intuition\n  Six Sigma Material \u0026ndash; Six Sigma Seite\n  AG Method(olgo)ische Grundlagen der Statistik und Ihre Anwendung \u0026ndash; LMU München \u0026hellip; WOW!\n  Leseprobe \u0026ldquo;Induktive Statistik und soziologische Theorie\u0026rdquo; \u0026ndash; Markus Ziegler - Eine Analyse des theoretischen Potenzials der Bayes-Statistik\n  Fiduzial \u0026ndash;\n  Vorsicht bei der σ-Regel \u0026ndash; Stefan Bartz\n  ","date":1514678400,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1514678400,"objectID":"434a6087f41ca5cdba40903421d1adf6","permalink":"https://sefiroth.net/nab/post/ein-paar-interessante-links/","publishdate":"2017-12-31T00:00:00Z","relpermalink":"/nab/post/ein-paar-interessante-links/","section":"post","summary":"Im Laufe der Zeit sammeln sich bei mir mehr und mehr Links zu anderen Seiten an, die ich irgendwie speichern will aber nicht ernsthaft sortieren möchte. So ist diese Sammlung hier entstanden:","tags":["Weblinks","Statistik"],"title":"Ein paar interessante Links","type":"post"},{"authors":null,"categories":["Statistisches"],"content":"  “Was hat das eigentlich mit den Quartilen, Quantilen und so weiter auf sich?” Diese Frage kommt ab und zu in Vorlesungen zur Statistik vor. Dabei ist die Antwort recht einfach.\nQuantile Definitorische Antwort Für eine gegebene reelle Zufallsvariable \\(X\\) heißt eine reelle Zahl \\(x_p\\) ein p-Quantil (von \\(X\\)), falls gilt:\n\\[P(X \\leq x_p) \\leq p \\quad \\text{ und }\\quad P(x_p \\leq X) \\geq 1-p.\\]\n Was bedeutet das denn nun konkret? Nun, ein Quantil ist ein Schwellenwert. Ein bestimmter Anteil der Werte ist kleiner als das Quantil, der Rest ist größer. Das 25-%-Quantil beispielsweise ist der Wert, für den gilt, dass 25 % aller Werte kleiner sind als dieser Wert. Quantile formalisieren praktische Aussagen wie „25 % aller Frauen sind kleiner als 1,62 m“ –- wobei 1,62 m hier das 25-%-Quantil ist.\nSpezielle Quantile sind der Median, die Quartile, die Quintile, die Dezile und die Perzentile:\nWir betrachten dazu in den Bespielen die Datenreihe dr an:\n# Die Zahlen von 0 bis 600 dr \u0026lt;- 0:600  Median Der Median (von lat. Medium für „Mitte, Mittelpunkt“ abgeleiteter Begriff mit der Bedeutung “in der Mitte gelegen”) die das 50-%-Quantil. Der Wert, welcher die Datenreihe (bestenfalls) in zwei (etwa) gleich große Abschnitte trennt. Sehr oft schreibt man \\(x_{med}\\), \\(x_{50\\%}\\), \\(x_{Med}\\) oder \\(Q_2\\) für den Median\nmedian(dr) ## [1] 300  Terzile Als Terile (von lat. tertius “der Dritte”) werden die beiden Quantile mit \\(p=1/3\\) und \\(p=2/3\\) bezeichnet. Sie teilen die Datenreihe in drei Abschnitte.\nquantile(dr, probs = seq(0, 1, 1/3)) ## 0% 33.33333% 66.66667% 100% ## 0 200 400 600  Quartile Die Quartile (von lat. quartus „der Vierte“) werden die Quantile mit \\(p=25\\%\\), \\(p=50\\%\\) und \\(p=75\\%\\) bezeichnet. Sie teilen die Datenreihe in vier Abschnitte. Dabei schreibt man oft: \\(Q_1 = x_{0{,}25}\\), \\(x_{Med} = Q_2 = x_{0{,}50}\\) und \\(Q_3 = x_{0{,}75}\\) für die drei Quantile.\nquantile(dr) # oder auch: quantile(dr, probs=seq(0, 1, 1/4)) ## 0% 25% 50% 75% 100% ## 0 150 300 450 600  Quintile Quintile (von lat. quintus “der Fünfte”) werden die Quantile mit \\(p=20\\%\\), \\(p=40\\%\\), \\(p=60\\%\\) und \\(p=80\\%\\) bezeichnet. Sie teilen die Datenreihe in fünf Abschnitte.\nquantile(dr, probs = seq(0, 1, 1/5)) ## 0% 20% 40% 60% 80% 100% ## 0 120 240 360 480 600  Dezile Die Quantile für vielfache von \\(0{,}1\\) also für \\(p=0{,}1;0{,}2;\\dots ;0{,}9\\) werden Dezile (von mittellateinisch decimalis, zu lat. decem „zehn“) genannt. Dabei heißt das \\(0{,}1\\)-Quantil das erste Dezil, das \\(0{,}2\\)-Quantil das zweite Dezil usw. Unterhalb des ersten Dezils liegen 10 % der Stichprobe, oberhalb entsprechend 90 % der Stichprobe. Ebenso liegen 40 % der Stichprobe unterhalb des vierten Dezils und 60 % oberhalb.\nquantile(dr, probs = seq(0, 1, 1/10)) ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## 0 60 120 180 240 300 360 420 480 540 600  Perzentile Als Perzentile (von lat.-ital. per centum “von Hundert, Hundertstel”) werden die Quantile von \\(\\displaystyle 0{,}01\\) bis $ 0{,}99$ in Schritten von \\(0{,}01\\) bezeichnet.\nquantile(dr, probs = seq(0, 1, 1/100)) ## 0% 1% 2% 3% 4% 5% 6% 7% 8% 9% 10% 11% 12% 13% 14% 15% ## 0 6 12 18 24 30 36 42 48 54 60 66 72 78 84 90 ## 16% 17% 18% 19% 20% 21% 22% 23% 24% 25% 26% 27% 28% 29% 30% 31% ## 96 102 108 114 120 126 132 138 144 150 156 162 168 174 180 186 ## 32% 33% 34% 35% 36% 37% 38% 39% 40% 41% 42% 43% 44% 45% 46% 47% ## 192 198 204 210 216 222 228 234 240 246 252 258 264 270 276 282 ## 48% 49% 50% 51% 52% 53% 54% 55% 56% 57% 58% 59% 60% 61% 62% 63% ## 288 294 300 306 312 318 324 330 336 342 348 354 360 366 372 378 ## 64% 65% 66% 67% 68% 69% 70% 71% 72% 73% 74% 75% 76% 77% 78% 79% ## 384 390 396 402 408 414 420 426 432 438 444 450 456 462 468 474 ## 80% 81% 82% 83% 84% 85% 86% 87% 88% 89% 90% 91% 92% 93% 94% 95% ## 480 486 492 498 504 510 516 522 528 534 540 546 552 558 564 570 ## 96% 97% 98% 99% 100% ## 576 582 588 594 600   ","date":1513641600,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1513641600,"objectID":"7ab582885ecb0b791d6e29a1c70a7966","permalink":"https://sefiroth.net/nab/post/quartile-quantile-perzentile-etc/","publishdate":"2017-12-19T00:00:00Z","relpermalink":"/nab/post/quartile-quantile-perzentile-etc/","section":"post","summary":"“Was hat das eigentlich mit den Quartilen, Quantilen und so weiter auf sich?” Diese Frage kommt ab und zu in Vorlesungen zur Statistik vor. Dabei ist die Antwort recht einfach.","tags":["Lehre","Statistik","Wahrscheinlichkeitstheorie","R"],"title":"Quartile, Quantile, Perzentile etc.","type":"post"},{"authors":null,"categories":["Fundstücke"],"content":"Gerade im Internet gefunden:\n10 Dinge die kein Talent benötigen!  Pünktlichkeit Arbeitsmoral Anstrengung Körpersprache Energie Haltung Leidenschaft Lernwillig sein Etwas mehr als das Minimum tun Vorbereitet sein  ","date":1499904000,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1499904000,"objectID":"b726e94f17c0c4914f37e85b232b4088","permalink":"https://sefiroth.net/nab/post/10-dinge-die-kein-talent-benotigen/","publishdate":"2017-07-13T00:00:00Z","relpermalink":"/nab/post/10-dinge-die-kein-talent-benotigen/","section":"post","summary":"Gerade im Internet gefunden:\n10 Dinge die kein Talent benötigen!  Pünktlichkeit Arbeitsmoral Anstrengung Körpersprache Energie Haltung Leidenschaft Lernwillig sein Etwas mehr als das Minimum tun Vorbereitet sein  ","tags":["Allgemein","Lehre"],"title":"10 Dinge die kein Talent benötigen!","type":"post"},{"authors":null,"categories":["Statitisches"],"content":"  Der Zentrale Grenzwertsatz der Statistik bei identischer Verteilung. Zentraler Grenzwertsatz Seien \\(X_1, X_2, ..., X_n\\) unabhängige und identisch verteilte Zufallsvariablen mit bekanntem Erwartungswert \\(E(X_i) = \\mu\\) und bekannter Varianz \\(Var(X_i)=\\sigma^2\\).\nFür die Summe \\(S_n = \\sum_{i=1}^n X_i\\) ist dann der Erwartungswert \\(E(S_n)= n \\cdot \\mu\\) und die Varianz \\(Var(S_n)= n \\cdot \\sigma^2\\).\nDann gilt für die standardisierte Zufallsvariable\n\\[ \\begin{align*} Z_n \u0026amp;= \\frac{\\left(\\sum\\limits_{i=1}^n X_i\\right) - n \\cdot \\mu}{\\sqrt{n\\cdot \\sigma^2}} = \\frac{S_n - n \\cdot \\mu}{\\sigma \\cdot \\sqrt{n}} = \\frac{n \\cdot \\bar{X}_n-n \\cdot \\mu}{\\sigma \\cdot n / \\sqrt{n}} \\\\ \u0026amp;= \\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}} = \\frac{\\bar{X}_n - \\mu}{\\sigma} \\cdot \\sqrt{n}, \\end{align*} \\]\ndass sie für wachsendes \\(n\\) immer besser durch die Standardnormalverteilung \\(N(0, 1)\\) approximiert werden kann.\nMit anderen Worten:\n\\[ P(Z_n \\leq x) \\longrightarrow \\Phi(x), \\quad \\text{ für }\\; n \\rightarrow \\infty \\]\n  Ein Beispiel: Nehmen wir drei Verteilungen mit Zufallsvariable \\(U\\), \\(X\\), \\(Y\\) und jeweils \\(n\\) Realisationen \\(u_1,\\dots, u_n\\), \\(x_1,\\dots, x_n\\), \\(y_1,\\dots, y_n\\).\nWählen wir zunächst \\(n=5\\):\nu ## [1] 19.726 69.683 60.790 0.955 42.901 x ## [1] 7.942 15.905 12.917 6.818 4.434 y ## [1] 59.961 56.552 51.094 75.288 47.985 Standardisieren wir die Werte:\nlibrary(mosaic) zscore(u) ## [1] -0.6695256 1.0830283 0.7710507 -1.3280357 0.1434823 zscore(x) ## [1] -0.3543069 1.3440714 0.7067796 -0.5940379 -1.1025063 zscore(y) ## [1] 0.1677971 -0.1526624 -0.6657361 1.6085958 -0.9579944 Die Behauptung des Zentralengrenzwertsatzes ist nun, dass mit steigender Anzahl an Werten \\(n\\) die standardisierten Werte in der empirischen Verteilungsfunktion sich immer mehr der Verteilungsfunktion der Standardnormalverteilung annähern:\nWeiterführende Literatur und Quellen dieses Eintrags:\n1. Schira, J.: Statistische Methoden der VWL und BWL. PEARSON Studion, München (2005)  2. Wikipedia: Zentraler Grenzwertsatz, https://de.wikipedia.org/w/index.php?title=Zentraler_Grenzwertsatz\u0026amp;oldid=162715036, (2017)  3. Weisstein, E.W.: Central limit theorem, http://mathworld.wolfram.com/CentralLimitTheorem.html, (2017)    ","date":1491350400,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1491350400,"objectID":"0743c5774fc79482034a47dd5c007a5e","permalink":"https://sefiroth.net/nab/post/der-zentrale-grenzwertsatz/","publishdate":"2017-04-05T00:00:00Z","relpermalink":"/nab/post/der-zentrale-grenzwertsatz/","section":"post","summary":"Der Zentrale Grenzwertsatz der Statistik bei identischer Verteilung. Zentraler Grenzwertsatz Seien \\(X_1, X_2, ..., X_n\\) unabhängige und identisch verteilte Zufallsvariablen mit bekanntem Erwartungswert \\(E(X_i) = \\mu\\) und bekannter Varianz \\(Var(X_i)=\\sigma^2\\).","tags":["Statistik","Wahrscheinlichkeitstheorie"],"title":"Der Zentrale Grenzwertsatz","type":"post"},{"authors":null,"categories":null,"content":"","date":1486684800,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1486684800,"objectID":"e85532955e8a35e9d46348bcbf16c22b","permalink":"https://sefiroth.net/nab/project/etwas-r-am-abend/","publishdate":"2017-02-10T00:00:00Z","relpermalink":"/nab/project/etwas-r-am-abend/","section":"project","summary":"Vortrag zur Einführung in R für Studierende","tags":["R","Statistik","RStudio","Data Science","Vortrag","Lehre"],"title":"Etwas R am Abend","type":"project"},{"authors":null,"categories":null,"content":"","date":1486684800,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1486684800,"objectID":"0631a1f00323140342d513a8ef2700c3","permalink":"https://sefiroth.net/nab/project/fastsimnulldistr/","publishdate":"2017-02-10T00:00:00Z","relpermalink":"/nab/project/fastsimnulldistr/","section":"project","summary":"Wenn das `do(oft)*` in Mosaic einfach zu langsam ist, dann hilft vielleicht diese (quasi)-drop-in-replacements um mal schnell die Nullverteilungen zu simulieren.","tags":["R","R markdown","Statistik","SBI"],"title":"FastSimNullDistR","type":"project"},{"authors":["Norman Markgraf"],"categories":["Typographie","Vorlesungen schreiben"],"content":"","date":1486684800,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1486684800,"objectID":"1acf504e26e2b9062b3965f2a5585300","permalink":"https://sefiroth.net/nab/project/style.py/","publishdate":"2017-02-10T00:00:00Z","relpermalink":"/nab/project/style.py/","section":"project","summary":"Ein Pandoc Filter (in Python 3.6+ auf Basis von 'panflute') um Stilelemente in Rmarkdown Dokumenten mittels geeigneter SPAN- bzw. DIV-Blöcke in entsprechende HTML-Stukturen bzw. LaTeX-Kommandos umzuwandelt.","tags":["R","R markdown","Python","pandoc-filter"],"title":"Pandoc filter: style.py","type":"project"},{"authors":null,"categories":null,"content":"","date":1486684800,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1486684800,"objectID":"12af9b91315b5ba286d091faa36bf3d6","permalink":"https://sefiroth.net/nab/project/typography.py/","publishdate":"2017-02-10T00:00:00Z","relpermalink":"/nab/project/typography.py/","section":"project","summary":"Ein Pandoc Filter (in Python 3.5+ auf Basis von panflute) um typographische Änderungen an aus  R markdown Dokumenten erzeugten HTML- bzw. LaTeX-Dokumenten vorzunehmen.","tags":["R","R markdown","pandoc-filter","Python","Pandoc","panflute","LaTeX"],"title":"Pandoc filter: typography.py","type":"project"},{"authors":null,"categories":null,"content":"Jede Sprache hat Regeln, auch Programmiersprachen und R markdown ist eine Programmiersprache. Wieso also nicht ein Tool schreiben, welches SStilregeln (engl. style guides) für R markdown kontrolliert um auch im kolaborativen Einsatz ein einheitliches \u0026ldquo;Schriftbild\u0026rdquo; des Quelltextes zu erhalten.\n\u0026hellip;\nEinen ersten Schritt habe ich mit dem Blog-Eintrag gemacht und dazu gleich noch ein Tool in Python geschrieben um Verstöße dagegen schneller zu finden.\n","date":1486684800,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1486684800,"objectID":"e87e9dab008fda3b107138c4d47ee933","permalink":"https://sefiroth.net/nab/project/rmdstylechecker/","publishdate":"2017-02-10T00:00:00Z","relpermalink":"/nab/project/rmdstylechecker/","section":"project","summary":"Python Script zum Überprüfen von Style Guidelines von R markdown Dokumenten","tags":["R","R markdown","Python"],"title":"RmdStyleChecker","type":"project"},{"authors":["Andreas Liefeld","Marten Völker"," Manuel Remelhe","Kai Dadhe","Sebastian Engell","Carsten Fritsch","Norman Markgaf"],"categories":null,"content":"","date":1096063200,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":1096063200,"objectID":"2e4ec66ecaba6ac511237aa249264d4d","permalink":"https://sefiroth.net/nab/publication/learn2control/","publishdate":"2004-09-25T00:00:00+02:00","relpermalink":"/nab/publication/learn2control/","section":"publication","summary":"LEARN2CONTROL ist eine computergestützte Lernumgebung, die es ermöglicht, vorhandenes Grundlagenwissen im Bereich der Regelungstechnik durch selbstständiges Lernen in einem projektorientierten Umfeld zu vertiefen. Der Lernschwerpunkt liegt dabei besonders auf der Vermittlung der inneren Abhängigkeiten und Wechselwirkungen der unterschiedlichen Verfahren und Methoden der Regelungstechnik.","tags":["Projektorientiertes Lernen","Regelungstechnik"],"title":"LEARN2CONTROL – Projektorientiertes Lernen für ein besseres Gesamtverständnis in der regelungstechnischen Ausbildung","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"de","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://sefiroth.net/nab/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nab/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]